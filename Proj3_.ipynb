{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMq48Q/eczY1OKBwXopV0OR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mafaldasalomao/binary_classification_breast_cancer/blob/main/Proj3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvucp4PhZpM5",
        "outputId": "45a008a3-2b49-4c53-f60a-73d198f44312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting skorch\n",
            "  Downloading skorch-0.12.0-py3-none-any.whl (185 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 40 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 185 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.64.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.10)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->skorch) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->skorch) (3.1.0)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install skorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skorch import NeuralNetBinaryClassifier\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Sqy_ocmdZuls",
        "outputId": "9ee6cc20-d45d-41d4-a38d-2bbe6fb1a8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2TwFUvraMwO",
        "outputId": "1c6c1ef4-0d03-4427-deb7-5cb2886a0e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f037b43d5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsores = pd.read_csv('/content/entradas_breast.csv')\n",
        "classe = pd.read_csv('/content/saidas_breast.csv')"
      ],
      "metadata": {
        "id": "K2GX9ONcaUQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previsores = np.array(previsores, dtype='float32')\n",
        "classe = np.array(classe, dtype='float32').squeeze(1)"
      ],
      "metadata": {
        "id": "KK427BxLaqZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class model(nn.Module):\n",
        "  def __init__(self, activation, neurons, initializer):\n",
        "    super().__init__()\n",
        "    self.dense0 = nn.Linear(30, neurons)\n",
        "    initializer(self.dense0.weight)\n",
        "    self.activation0 = activation\n",
        "    self.dense1 = nn.Linear(neurons, neurons)\n",
        "    initializer(self.dense1.weight)\n",
        "    self.activation1 = activation\n",
        "    self.dense2 = nn.Linear(neurons, 1)\n",
        "    initializer(self.dense2.weight)\n",
        "    self.output = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dense0(x)\n",
        "    x = self.activation0(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.activation1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = self.output(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "J51HDPrTa_Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sk = NeuralNetBinaryClassifier(module=model,\n",
        "                                     lr=0.001,\n",
        "                                     optimizer__weight_decay=0.0001,\n",
        "                                     train_split=False)"
      ],
      "metadata": {
        "id": "ens3bsq0cC3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params={'batch_size': [10, 32],\n",
        "        'max_epochs': [50, 100],\n",
        "        'optimizer': [torch.optim.Adam, torch.optim.SGD],\n",
        "        'criterion': [torch.nn.BCELoss, torch.nn.HingeEmbeddingLoss],\n",
        "        'module__activation': [F.relu, F.tanh],\n",
        "        'module__neurons': [8, 16],\n",
        "        'module__initializer': [nn.init.uniform_, nn.init.normal_]}"
      ],
      "metadata": {
        "id": "GX3IqHawcrOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt8KYcbdePKL",
        "outputId": "4c89d74a-d272-4440-b970-6b670186d3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': [10, 32],\n",
              " 'max_epochs': [50, 100],\n",
              " 'optimizer': [torch.optim.adam.Adam, torch.optim.sgd.SGD],\n",
              " 'criterion': [torch.nn.modules.loss.BCELoss,\n",
              "  torch.nn.modules.loss.HingeEmbeddingLoss],\n",
              " 'module__activation': [<function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor>,\n",
              "  <function torch.nn.functional.tanh(input)>],\n",
              " 'module__neurons': [8, 16],\n",
              " 'module__initializer': [<function torch.nn.init.uniform_(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0) -> torch.Tensor>,\n",
              "  <function torch.nn.init.normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0) -> torch.Tensor>]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(estimator=model_sk, param_grid=params, scoring='accuracy', cv=2)\n",
        "grid_search = grid_search.fit(previsores, classe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVPkCEbreQ5i",
        "outputId": "568cfe22-6091-4ee1-f1ab-e07bdafe43cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.2926\n",
            "      2       37.3239  0.0442\n",
            "      3       37.3239  0.0422\n",
            "      4       37.3239  0.0456\n",
            "      5       37.3239  0.0439\n",
            "      6       37.3239  0.0426\n",
            "      7       37.3239  0.0455\n",
            "      8       37.3239  0.0428\n",
            "      9       37.3239  0.0416\n",
            "     10       37.3239  0.0418\n",
            "     11       37.3239  0.0397\n",
            "     12       37.3239  0.0414\n",
            "     13       37.3239  0.0409\n",
            "     14       37.3239  0.0453\n",
            "     15       37.3239  0.0444\n",
            "     16       37.3239  0.0499\n",
            "     17       37.3239  0.0432\n",
            "     18       37.3239  0.0408\n",
            "     19       37.3239  0.0432\n",
            "     20       37.3239  0.0450\n",
            "     21       37.3239  0.0463\n",
            "     22       37.3239  0.0428\n",
            "     23       37.3239  0.0466\n",
            "     24       37.3239  0.0511\n",
            "     25       37.3239  0.0459\n",
            "     26       37.3239  0.0406\n",
            "     27       37.3239  0.0421\n",
            "     28       37.3239  0.0422\n",
            "     29       37.3239  0.0423\n",
            "     30       37.3239  0.0446\n",
            "     31       37.3239  0.0409\n",
            "     32       37.3239  0.0431\n",
            "     33       37.3239  0.0436\n",
            "     34       37.3239  0.0481\n",
            "     35       37.3239  0.0424\n",
            "     36       37.3239  0.0438\n",
            "     37       37.3239  0.0474\n",
            "     38       37.3239  0.0479\n",
            "     39       37.3239  0.0439\n",
            "     40       37.3239  0.0472\n",
            "     41       37.3239  0.0437\n",
            "     42       37.3239  0.0447\n",
            "     43       37.3239  0.0407\n",
            "     44       37.3239  0.0458\n",
            "     45       37.3239  0.0511\n",
            "     46       37.3239  0.0420\n",
            "     47       37.3239  0.0411\n",
            "     48       37.3239  0.0409\n",
            "     49       37.3239  0.0530\n",
            "     50       37.3239  0.0427\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0390\n",
            "      2       37.1930  0.0471\n",
            "      3       37.1930  0.0456\n",
            "      4       37.1930  0.0418\n",
            "      5       37.1930  0.0431\n",
            "      6       37.1930  0.0490\n",
            "      7       37.1930  0.0403\n",
            "      8       37.1930  0.0445\n",
            "      9       37.1930  0.0454\n",
            "     10       37.1930  0.0400\n",
            "     11       37.1930  0.0437\n",
            "     12       37.1930  0.0440\n",
            "     13       37.1930  0.0496\n",
            "     14       37.1930  0.0473\n",
            "     15       37.1930  0.0473\n",
            "     16       37.1930  0.0434\n",
            "     17       37.1930  0.0381\n",
            "     18       37.1930  0.0415\n",
            "     19       37.1930  0.0409\n",
            "     20       37.1930  0.0433\n",
            "     21       37.1930  0.0463\n",
            "     22       37.1930  0.0415\n",
            "     23       37.1930  0.0414\n",
            "     24       37.1930  0.0430\n",
            "     25       37.1930  0.0431\n",
            "     26       37.1930  0.0466\n",
            "     27       37.1930  0.0490\n",
            "     28       37.1930  0.0417\n",
            "     29       37.1930  0.0465\n",
            "     30       37.1930  0.0457\n",
            "     31       37.1930  0.0532\n",
            "     32       37.1930  0.0462\n",
            "     33       37.1930  0.0500\n",
            "     34       37.1930  0.0507\n",
            "     35       37.1930  0.0445\n",
            "     36       37.1930  0.0496\n",
            "     37       37.1930  0.0424\n",
            "     38       37.1930  0.0506\n",
            "     39       37.1930  0.0479\n",
            "     40       \u001b[36m31.4318\u001b[0m  0.0409\n",
            "     41        \u001b[36m0.7308\u001b[0m  0.0425\n",
            "     42        \u001b[36m0.6143\u001b[0m  0.0431\n",
            "     43        0.6174  0.0430\n",
            "     44        0.6165  0.0520\n",
            "     45        0.6159  0.0441\n",
            "     46        0.6153  0.0401\n",
            "     47        \u001b[36m0.6092\u001b[0m  0.0437\n",
            "     48        0.6167  0.0449\n",
            "     49        \u001b[36m0.6006\u001b[0m  0.0427\n",
            "     50        \u001b[36m0.5919\u001b[0m  0.0451\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0279\n",
            "      2       37.3239  0.0368\n",
            "      3       37.3239  0.0333\n",
            "      4       37.3239  0.0407\n",
            "      5       37.3239  0.0359\n",
            "      6       37.3239  0.0358\n",
            "      7       37.3239  0.0351\n",
            "      8       37.3239  0.0335\n",
            "      9       37.3239  0.0325\n",
            "     10       37.3239  0.0318\n",
            "     11       37.3239  0.0324\n",
            "     12       37.3239  0.0312\n",
            "     13       37.3239  0.0305\n",
            "     14       37.3239  0.0331\n",
            "     15       37.3239  0.0434\n",
            "     16       37.3239  0.0314\n",
            "     17       37.3239  0.0343\n",
            "     18       37.3239  0.0372\n",
            "     19       37.3239  0.0340\n",
            "     20       37.3239  0.0314\n",
            "     21       37.3239  0.0318\n",
            "     22       37.3239  0.0313\n",
            "     23       37.3239  0.0341\n",
            "     24       37.3239  0.0374\n",
            "     25       37.3239  0.0326\n",
            "     26       37.3239  0.0364\n",
            "     27       37.3239  0.0371\n",
            "     28       37.3239  0.0361\n",
            "     29       37.3239  0.0502\n",
            "     30       37.3239  0.0387\n",
            "     31       37.3239  0.0411\n",
            "     32       37.3239  0.0325\n",
            "     33       37.3239  0.0353\n",
            "     34       37.3239  0.0356\n",
            "     35       37.3239  0.0426\n",
            "     36       37.3239  0.0323\n",
            "     37       37.3239  0.0304\n",
            "     38       37.3239  0.0382\n",
            "     39       37.3239  0.0318\n",
            "     40       37.3239  0.0331\n",
            "     41       37.3239  0.0328\n",
            "     42       37.3239  0.0312\n",
            "     43       37.3239  0.0306\n",
            "     44       37.3239  0.0315\n",
            "     45       37.3239  0.0330\n",
            "     46       37.3239  0.0348\n",
            "     47       37.3239  0.0281\n",
            "     48       37.3239  0.0329\n",
            "     49       37.3239  0.0339\n",
            "     50       37.3239  0.0340\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0379\n",
            "      2       37.1930  0.0363\n",
            "      3       37.1930  0.0359\n",
            "      4       37.1930  0.0363\n",
            "      5       37.1930  0.0376\n",
            "      6       37.1930  0.0444\n",
            "      7       37.1930  0.0334\n",
            "      8       37.1930  0.0345\n",
            "      9       37.1930  0.0334\n",
            "     10       37.1930  0.0471\n",
            "     11       37.1930  0.0355\n",
            "     12       37.1930  0.0377\n",
            "     13       37.1930  0.0339\n",
            "     14       37.1930  0.0288\n",
            "     15       37.1930  0.0313\n",
            "     16       37.1930  0.0301\n",
            "     17       37.1930  0.0310\n",
            "     18       37.1930  0.0319\n",
            "     19       37.1930  0.0313\n",
            "     20       37.1930  0.0311\n",
            "     21       37.1930  0.0336\n",
            "     22       37.1930  0.0336\n",
            "     23       37.1930  0.0325\n",
            "     24       37.1930  0.0327\n",
            "     25       37.1930  0.0337\n",
            "     26       37.1930  0.0354\n",
            "     27       37.1930  0.0342\n",
            "     28       37.1930  0.0315\n",
            "     29       37.1930  0.0314\n",
            "     30       37.1930  0.0331\n",
            "     31       37.1930  0.0335\n",
            "     32       37.1930  0.0403\n",
            "     33       37.1930  0.0348\n",
            "     34       37.1930  0.0363\n",
            "     35       37.1930  0.0313\n",
            "     36       37.1930  0.0360\n",
            "     37       37.1930  0.0343\n",
            "     38       37.1930  0.0313\n",
            "     39       37.1930  0.0350\n",
            "     40       37.1930  0.0318\n",
            "     41       37.1930  0.0310\n",
            "     42       37.1930  0.0322\n",
            "     43       37.1930  0.0326\n",
            "     44       37.1930  0.0321\n",
            "     45       37.1930  0.0341\n",
            "     46       37.1930  0.0298\n",
            "     47       37.1930  0.0318\n",
            "     48       37.1930  0.0340\n",
            "     49       37.1930  0.0307\n",
            "     50       37.1930  0.0328\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0395\n",
            "      2       37.3239  0.0495\n",
            "      3       37.3239  0.0447\n",
            "      4       37.3239  0.0427\n",
            "      5       37.3239  0.0388\n",
            "      6       37.3239  0.0423\n",
            "      7       37.3239  0.0433\n",
            "      8       37.3239  0.0421\n",
            "      9       37.3239  0.0625\n",
            "     10       37.3239  0.0551\n",
            "     11       37.3239  0.0436\n",
            "     12       37.3239  0.0432\n",
            "     13       37.3239  0.0462\n",
            "     14       37.3239  0.0467\n",
            "     15       37.3239  0.0495\n",
            "     16       37.3239  0.0408\n",
            "     17       37.3239  0.0421\n",
            "     18       37.3239  0.0429\n",
            "     19       37.3239  0.0426\n",
            "     20       37.3239  0.0423\n",
            "     21       37.3239  0.0445\n",
            "     22       37.3239  0.0500\n",
            "     23       37.3239  0.0454\n",
            "     24       37.3239  0.0454\n",
            "     25       37.3239  0.0409\n",
            "     26       37.3239  0.0428\n",
            "     27       37.3239  0.0478\n",
            "     28       37.3239  0.0450\n",
            "     29       37.3239  0.0482\n",
            "     30       37.3239  0.0564\n",
            "     31       37.3239  0.0465\n",
            "     32       37.3239  0.0543\n",
            "     33       37.3239  0.0475\n",
            "     34       37.3239  0.0453\n",
            "     35       37.3239  0.0424\n",
            "     36       37.3239  0.0427\n",
            "     37       37.3239  0.0420\n",
            "     38       37.3239  0.0441\n",
            "     39       37.3239  0.0427\n",
            "     40       37.3239  0.0427\n",
            "     41       37.3239  0.0419\n",
            "     42       37.3239  0.0413\n",
            "     43       37.3239  0.0418\n",
            "     44       37.3239  0.0471\n",
            "     45       37.3239  0.0441\n",
            "     46       37.3239  0.0412\n",
            "     47       37.3239  0.0457\n",
            "     48       37.3239  0.0445\n",
            "     49       37.3239  0.0499\n",
            "     50       37.3239  0.0502\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0569\n",
            "      2       37.1930  0.0484\n",
            "      3       37.1930  0.0499\n",
            "      4       37.1930  0.0458\n",
            "      5       37.1930  0.0462\n",
            "      6       37.1930  0.0435\n",
            "      7       37.1930  0.0436\n",
            "      8       37.1930  0.0507\n",
            "      9       37.1930  0.0428\n",
            "     10       37.1930  0.0416\n",
            "     11       37.1930  0.0425\n",
            "     12       37.1930  0.0408\n",
            "     13       37.1930  0.0431\n",
            "     14       37.1930  0.0440\n",
            "     15       37.1930  0.0452\n",
            "     16       37.1930  0.0393\n",
            "     17       37.1930  0.0442\n",
            "     18       37.1930  0.0445\n",
            "     19       37.1930  0.0461\n",
            "     20       37.1930  0.0509\n",
            "     21       37.1930  0.0454\n",
            "     22       37.1930  0.0572\n",
            "     23       37.1930  0.0443\n",
            "     24       37.1930  0.0402\n",
            "     25       37.1930  0.0404\n",
            "     26       37.1930  0.0420\n",
            "     27       37.1930  0.0546\n",
            "     28       37.1930  0.0456\n",
            "     29       37.1930  0.0423\n",
            "     30       37.1930  0.0420\n",
            "     31       37.1930  0.0402\n",
            "     32       37.1930  0.0403\n",
            "     33       37.1930  0.0414\n",
            "     34       37.1930  0.0426\n",
            "     35       37.1930  0.0446\n",
            "     36       37.1930  0.0460\n",
            "     37       37.1930  0.0456\n",
            "     38       37.1930  0.0438\n",
            "     39       37.1930  0.0459\n",
            "     40       37.1930  0.0444\n",
            "     41       37.1930  0.0499\n",
            "     42       37.1930  0.0515\n",
            "     43       \u001b[36m30.9935\u001b[0m  0.0454\n",
            "     44        \u001b[36m1.1818\u001b[0m  0.0506\n",
            "     45        \u001b[36m0.6662\u001b[0m  0.0443\n",
            "     46        \u001b[36m0.5724\u001b[0m  0.0474\n",
            "     47        0.5885  0.0397\n",
            "     48        \u001b[36m0.5540\u001b[0m  0.0402\n",
            "     49        0.5634  0.0417\n",
            "     50        \u001b[36m0.5311\u001b[0m  0.0402\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0285\n",
            "      2       37.3239  0.0334\n",
            "      3       37.3239  0.0336\n",
            "      4       37.3239  0.0311\n",
            "      5       37.3239  0.0301\n",
            "      6       37.3239  0.0312\n",
            "      7       37.3239  0.0305\n",
            "      8       37.3239  0.0312\n",
            "      9       37.3239  0.0343\n",
            "     10       37.3239  0.0335\n",
            "     11       37.3239  0.0352\n",
            "     12       37.3239  0.0331\n",
            "     13       37.3239  0.0323\n",
            "     14       37.3239  0.0343\n",
            "     15       37.3239  0.0440\n",
            "     16       37.3239  0.0352\n",
            "     17       37.3239  0.0347\n",
            "     18       37.3239  0.0429\n",
            "     19       37.3239  0.0432\n",
            "     20       37.3239  0.0310\n",
            "     21       37.3239  0.0318\n",
            "     22       37.3239  0.0315\n",
            "     23       37.3239  0.0308\n",
            "     24       37.3239  0.0290\n",
            "     25       37.3239  0.0344\n",
            "     26       37.3239  0.0306\n",
            "     27       37.3239  0.0291\n",
            "     28       37.3239  0.0383\n",
            "     29       37.3239  0.0297\n",
            "     30       37.3239  0.0348\n",
            "     31       37.3239  0.0326\n",
            "     32       37.3239  0.0329\n",
            "     33       37.3239  0.0328\n",
            "     34       37.3239  0.0322\n",
            "     35       37.3239  0.0328\n",
            "     36       37.3239  0.0357\n",
            "     37       37.3239  0.0330\n",
            "     38       37.3239  0.0334\n",
            "     39       37.3239  0.0333\n",
            "     40       37.3239  0.0332\n",
            "     41       37.3239  0.0300\n",
            "     42       37.3239  0.0308\n",
            "     43       37.3239  0.0414\n",
            "     44       37.3239  0.0334\n",
            "     45       37.3239  0.0319\n",
            "     46       37.3239  0.0333\n",
            "     47       37.3239  0.0346\n",
            "     48       37.3239  0.0411\n",
            "     49       37.3239  0.0313\n",
            "     50       37.3239  0.0308\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0280\n",
            "      2       37.1930  0.0293\n",
            "      3       37.1930  0.0368\n",
            "      4       37.1930  0.0320\n",
            "      5       37.1930  0.0304\n",
            "      6       37.1930  0.0321\n",
            "      7       37.1930  0.0322\n",
            "      8       37.1930  0.0302\n",
            "      9       37.1930  0.0296\n",
            "     10       37.1930  0.0315\n",
            "     11       37.1930  0.0313\n",
            "     12       37.1930  0.0342\n",
            "     13       37.1930  0.0320\n",
            "     14       37.1930  0.0331\n",
            "     15       37.1930  0.0323\n",
            "     16       37.1930  0.0316\n",
            "     17       37.1930  0.0328\n",
            "     18       37.1930  0.0308\n",
            "     19       37.1930  0.0353\n",
            "     20       37.1930  0.0357\n",
            "     21       37.1930  0.0341\n",
            "     22       37.1930  0.0360\n",
            "     23       37.1930  0.0306\n",
            "     24       37.1930  0.0374\n",
            "     25       37.1930  0.0335\n",
            "     26       37.1930  0.0453\n",
            "     27       37.1930  0.0315\n",
            "     28       37.1930  0.0367\n",
            "     29       37.1930  0.0321\n",
            "     30       37.1930  0.0325\n",
            "     31       37.1930  0.0314\n",
            "     32       37.1930  0.0325\n",
            "     33       37.1930  0.0382\n",
            "     34       37.1930  0.0310\n",
            "     35       37.1930  0.0329\n",
            "     36       37.1930  0.0354\n",
            "     37       37.1930  0.0359\n",
            "     38       37.1930  0.0313\n",
            "     39       37.1930  0.0307\n",
            "     40       37.1930  0.0383\n",
            "     41       37.1930  0.0326\n",
            "     42       37.1930  0.0292\n",
            "     43       37.1930  0.0313\n",
            "     44       37.1930  0.0334\n",
            "     45       37.1930  0.0355\n",
            "     46       37.1930  0.0329\n",
            "     47       37.1930  0.0318\n",
            "     48       37.1930  0.0361\n",
            "     49       37.1930  0.0357\n",
            "     50       37.1930  0.0363\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0472\n",
            "      2       37.3239  0.0455\n",
            "      3       37.3239  0.0523\n",
            "      4       37.3239  0.0421\n",
            "      5       37.3239  0.0413\n",
            "      6       37.3239  0.0426\n",
            "      7       37.3239  0.0412\n",
            "      8       37.3239  0.0464\n",
            "      9       37.3239  0.0453\n",
            "     10       37.3239  0.0420\n",
            "     11       37.3239  0.0400\n",
            "     12       37.3239  0.0424\n",
            "     13       37.3239  0.0484\n",
            "     14       37.3239  0.0529\n",
            "     15       37.3239  0.0626\n",
            "     16       37.3239  0.0474\n",
            "     17       37.3239  0.0490\n",
            "     18       37.3239  0.0732\n",
            "     19       37.3239  0.0954\n",
            "     20       37.3239  0.0848\n",
            "     21       37.3239  0.0800\n",
            "     22       37.3239  0.0434\n",
            "     23       37.3239  0.0471\n",
            "     24       37.3239  0.0408\n",
            "     25       37.3239  0.0421\n",
            "     26       37.3239  0.0442\n",
            "     27       37.3239  0.0417\n",
            "     28       37.3239  0.0422\n",
            "     29       37.3239  0.0459\n",
            "     30       37.3239  0.0463\n",
            "     31       37.3239  0.0494\n",
            "     32       37.3239  0.0472\n",
            "     33       37.3239  0.0425\n",
            "     34       37.3239  0.0461\n",
            "     35       37.3239  0.0476\n",
            "     36       37.3239  0.0454\n",
            "     37       37.3239  0.0469\n",
            "     38       37.3239  0.0497\n",
            "     39       37.3239  0.0432\n",
            "     40       37.3239  0.0424\n",
            "     41       37.3239  0.0429\n",
            "     42       37.3239  0.0564\n",
            "     43       37.3239  0.0412\n",
            "     44       37.3239  0.0408\n",
            "     45       37.3239  0.0417\n",
            "     46       37.3239  0.0419\n",
            "     47       37.3239  0.0424\n",
            "     48       37.3239  0.0430\n",
            "     49       37.3239  0.0426\n",
            "     50       37.3239  0.0484\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m59.5277\u001b[0m  0.0407\n",
            "      2       \u001b[36m59.2982\u001b[0m  0.0588\n",
            "      3       59.3351  0.0531\n",
            "      4       \u001b[36m58.7926\u001b[0m  0.0474\n",
            "      5       \u001b[36m57.8770\u001b[0m  0.0450\n",
            "      6       \u001b[36m56.8358\u001b[0m  0.0457\n",
            "      7       \u001b[36m55.7838\u001b[0m  0.0638\n",
            "      8       \u001b[36m51.7646\u001b[0m  0.0921\n",
            "      9       \u001b[36m45.0580\u001b[0m  0.0687\n",
            "     10       47.1425  0.1094\n",
            "     11       \u001b[36m44.3828\u001b[0m  0.0773\n",
            "     12       44.9273  0.1752\n",
            "     13       45.5598  0.0452\n",
            "     14       45.5539  0.0443\n",
            "     15       45.5487  0.0411\n",
            "     16       45.5423  0.0404\n",
            "     17       45.5293  0.0625\n",
            "     18       45.4798  0.0650\n",
            "     19       45.1592  0.0807\n",
            "     20       \u001b[36m44.0070\u001b[0m  0.0789\n",
            "     21       \u001b[36m43.9472\u001b[0m  0.1198\n",
            "     22       \u001b[36m43.9444\u001b[0m  0.0598\n",
            "     23       \u001b[36m43.9443\u001b[0m  0.1124\n",
            "     24       43.9443  0.0982\n",
            "     25       43.9443  0.1423\n",
            "     26       \u001b[36m43.9436\u001b[0m  0.0736\n",
            "     27       \u001b[36m43.9430\u001b[0m  0.0688\n",
            "     28       \u001b[36m43.9427\u001b[0m  0.0821\n",
            "     29       \u001b[36m43.9426\u001b[0m  0.0788\n",
            "     30       43.9426  0.1039\n",
            "     31       43.9427  0.0767\n",
            "     32       43.9429  0.1057\n",
            "     33       43.9431  0.0521\n",
            "     34       43.9434  0.0435\n",
            "     35       \u001b[36m43.9395\u001b[0m  0.0975\n",
            "     36       \u001b[36m43.9265\u001b[0m  0.1406\n",
            "     37       \u001b[36m43.9162\u001b[0m  0.0755\n",
            "     38       \u001b[36m43.9091\u001b[0m  0.0780\n",
            "     39       \u001b[36m43.9081\u001b[0m  0.0795\n",
            "     40       \u001b[36m43.9079\u001b[0m  0.0783\n",
            "     41       43.9082  0.0723\n",
            "     42       43.9083  0.0494\n",
            "     43       43.9088  0.0431\n",
            "     44       43.9091  0.0422\n",
            "     45       43.9096  0.0418\n",
            "     46       43.9102  0.0538\n",
            "     47       43.9109  0.1120\n",
            "     48       43.9118  0.1834\n",
            "     49       43.9128  0.0481\n",
            "     50       43.9140  0.0451\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m54.0402\u001b[0m  0.0288\n",
            "      2       \u001b[36m52.1127\u001b[0m  0.0339\n",
            "      3       52.1127  0.0342\n",
            "      4       52.1127  0.0585\n",
            "      5       52.1127  0.0472\n",
            "      6       52.1127  0.0608\n",
            "      7       52.1127  0.0419\n",
            "      8       52.1127  0.0450\n",
            "      9       52.1127  0.0328\n",
            "     10       52.1127  0.0310\n",
            "     11       52.1127  0.0524\n",
            "     12       52.1127  0.0632\n",
            "     13       52.1127  0.0579\n",
            "     14       52.1127  0.0578\n",
            "     15       52.1127  0.0487\n",
            "     16       52.1127  0.1363\n",
            "     17       52.1127  0.0966\n",
            "     18       52.1127  0.0458\n",
            "     19       52.1127  0.0758\n",
            "     20       52.1127  0.0582\n",
            "     21       52.1127  0.0775\n",
            "     22       52.1127  0.0765\n",
            "     23       52.1127  0.0749\n",
            "     24       52.1127  0.0772\n",
            "     25       52.1127  0.0945\n",
            "     26       52.1127  0.0530\n",
            "     27       52.1127  0.0560\n",
            "     28       52.1127  0.0443\n",
            "     29       52.1127  0.0529\n",
            "     30       52.1127  0.0648\n",
            "     31       52.1127  0.1108\n",
            "     32       52.1127  0.0806\n",
            "     33       52.1127  0.0884\n",
            "     34       52.1127  0.0686\n",
            "     35       52.1127  0.0450\n",
            "     36       52.1127  0.0419\n",
            "     37       52.1127  0.0505\n",
            "     38       52.1127  0.0418\n",
            "     39       52.1127  0.1014\n",
            "     40       52.1127  0.1951\n",
            "     41       52.1127  0.0504\n",
            "     42       52.1127  0.0615\n",
            "     43       52.1127  0.0479\n",
            "     44       52.1127  0.0456\n",
            "     45       52.1127  0.0491\n",
            "     46       52.1127  0.0351\n",
            "     47       52.1127  0.0463\n",
            "     48       52.1127  0.0527\n",
            "     49       52.1127  0.0480\n",
            "     50       52.1127  0.0474\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m65.2638\u001b[0m  0.0859\n",
            "      2       \u001b[36m63.1579\u001b[0m  0.0450\n",
            "      3       63.1579  0.0727\n",
            "      4       63.1579  0.1089\n",
            "      5       63.1579  0.0472\n",
            "      6       63.1579  0.0483\n",
            "      7       63.1579  0.0704\n",
            "      8       63.1579  0.0519\n",
            "      9       63.1579  0.0727\n",
            "     10       63.1579  0.0451\n",
            "     11       63.1579  0.0346\n",
            "     12       63.1579  0.0337\n",
            "     13       63.1579  0.0327\n",
            "     14       63.1579  0.0319\n",
            "     15       63.1579  0.0370\n",
            "     16       63.1579  0.0338\n",
            "     17       63.1579  0.0485\n",
            "     18       63.1579  0.0503\n",
            "     19       63.1579  0.0305\n",
            "     20       63.1579  0.0335\n",
            "     21       63.1579  0.0322\n",
            "     22       63.1579  0.0316\n",
            "     23       63.1579  0.0316\n",
            "     24       63.1579  0.0399\n",
            "     25       63.1579  0.0711\n",
            "     26       63.1579  0.0902\n",
            "     27       63.1579  0.0347\n",
            "     28       63.1579  0.0308\n",
            "     29       63.1579  0.0340\n",
            "     30       63.1579  0.0336\n",
            "     31       63.1579  0.0327\n",
            "     32       63.1579  0.0339\n",
            "     33       63.1579  0.0548\n",
            "     34       63.1579  0.0820\n",
            "     35       63.1579  0.0839\n",
            "     36       63.1579  0.0326\n",
            "     37       63.1579  0.0345\n",
            "     38       63.1579  0.0334\n",
            "     39       63.1579  0.0319\n",
            "     40       63.1579  0.0317\n",
            "     41       63.1579  0.0322\n",
            "     42       63.1579  0.0596\n",
            "     43       63.1579  0.0787\n",
            "     44       63.1579  0.1047\n",
            "     45       63.1579  0.1139\n",
            "     46       63.1579  0.1227\n",
            "     47       63.1579  0.0459\n",
            "     48       63.1579  0.0456\n",
            "     49       63.1579  0.0482\n",
            "     50       63.1579  0.1157\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m59.8592\u001b[0m  0.0395\n",
            "      2       60.5634  0.0432\n",
            "      3       61.2676  0.0439\n",
            "      4       61.2676  0.0507\n",
            "      5       61.2676  0.0705\n",
            "      6       61.2876  0.0826\n",
            "      7       60.5939  0.0760\n",
            "      8       60.5634  0.0669\n",
            "      9       60.5634  0.1695\n",
            "     10       60.5634  0.1345\n",
            "     11       60.5634  0.0824\n",
            "     12       60.5634  0.0428\n",
            "     13       60.5634  0.0411\n",
            "     14       60.5634  0.0463\n",
            "     15       60.5634  0.0440\n",
            "     16       60.5634  0.0451\n",
            "     17       60.5634  0.0435\n",
            "     18       60.5634  0.0446\n",
            "     19       60.2113  0.0460\n",
            "     20       \u001b[36m59.6519\u001b[0m  0.0443\n",
            "     21       \u001b[36m59.1549\u001b[0m  0.0570\n",
            "     22       59.1549  0.0469\n",
            "     23       59.1549  0.0469\n",
            "     24       59.1549  0.0473\n",
            "     25       59.1549  0.0443\n",
            "     26       59.1549  0.0505\n",
            "     27       59.1549  0.0546\n",
            "     28       59.1549  0.0463\n",
            "     29       59.1549  0.0463\n",
            "     30       59.1549  0.0485\n",
            "     31       59.1549  0.0569\n",
            "     32       59.1549  0.0605\n",
            "     33       59.1549  0.0697\n",
            "     34       59.1549  0.0543\n",
            "     35       \u001b[36m58.8028\u001b[0m  0.0433\n",
            "     36       58.8028  0.0490\n",
            "     37       58.8028  0.0446\n",
            "     38       58.8028  0.0449\n",
            "     39       58.8028  0.1091\n",
            "     40       58.8028  0.1058\n",
            "     41       58.8028  0.0927\n",
            "     42       58.8028  0.1573\n",
            "     43       58.8028  0.1072\n",
            "     44       58.8028  0.0700\n",
            "     45       58.8028  0.0422\n",
            "     46       58.8028  0.0438\n",
            "     47       58.8028  0.0428\n",
            "     48       58.8028  0.0528\n",
            "     49       58.8028  0.0402\n",
            "     50       58.8028  0.0427\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m45.4536\u001b[0m  0.0384\n",
            "      2       47.3364  0.0456\n",
            "      3       45.5869  0.0486\n",
            "      4       45.4670  0.0426\n",
            "      5       \u001b[36m43.4547\u001b[0m  0.1042\n",
            "      6       44.5614  0.2145\n",
            "      7       44.9123  0.2356\n",
            "      8       44.6447  0.1103\n",
            "      9       \u001b[36m41.6037\u001b[0m  0.1505\n",
            "     10       43.5961  0.0540\n",
            "     11       \u001b[36m41.0526\u001b[0m  0.0495\n",
            "     12       41.1122  0.0493\n",
            "     13       \u001b[36m40.4563\u001b[0m  0.0493\n",
            "     14       \u001b[36m39.4962\u001b[0m  0.0438\n",
            "     15       \u001b[36m39.3053\u001b[0m  0.0460\n",
            "     16       39.6491  0.0499\n",
            "     17       39.6491  0.0485\n",
            "     18       39.6491  0.0459\n",
            "     19       39.6491  0.0518\n",
            "     20       39.6491  0.0458\n",
            "     21       39.6491  0.0562\n",
            "     22       39.6491  0.0454\n",
            "     23       39.6491  0.0534\n",
            "     24       40.0000  0.0417\n",
            "     25       40.0000  0.0416\n",
            "     26       40.0000  0.0397\n",
            "     27       40.0000  0.0419\n",
            "     28       40.0000  0.0437\n",
            "     29       40.3509  0.0416\n",
            "     30       40.3509  0.0453\n",
            "     31       40.3509  0.0437\n",
            "     32       41.0526  0.0462\n",
            "     33       41.4035  0.0448\n",
            "     34       41.7544  0.0563\n",
            "     35       41.7544  0.0566\n",
            "     36       41.7544  0.0479\n",
            "     37       41.7544  0.0496\n",
            "     38       41.7544  0.0526\n",
            "     39       41.7544  0.0628\n",
            "     40       41.7544  0.0540\n",
            "     41       41.7544  0.0441\n",
            "     42       41.7544  0.0494\n",
            "     43       41.7544  0.0457\n",
            "     44       41.7544  0.0473\n",
            "     45       41.7544  0.0496\n",
            "     46       41.7544  0.0449\n",
            "     47       41.7544  0.0502\n",
            "     48       41.7544  0.0498\n",
            "     49       41.7544  0.0447\n",
            "     50       41.7544  0.0411\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m62.3239\u001b[0m  0.0474\n",
            "      2       62.3239  0.0480\n",
            "      3       62.3239  0.0504\n",
            "      4       62.3239  0.0473\n",
            "      5       62.3239  0.0881\n",
            "      6       62.3239  0.0660\n",
            "      7       62.3239  0.0525\n",
            "      8       62.3239  0.0554\n",
            "      9       62.3239  0.0928\n",
            "     10       62.3239  0.1078\n",
            "     11       62.3239  0.1256\n",
            "     12       62.3239  0.0529\n",
            "     13       62.3239  0.0329\n",
            "     14       62.3239  0.0435\n",
            "     15       62.3239  0.0309\n",
            "     16       62.3239  0.0325\n",
            "     17       62.3239  0.0353\n",
            "     18       62.3239  0.0358\n",
            "     19       62.3239  0.0808\n",
            "     20       62.3239  0.0390\n",
            "     21       62.3239  0.0329\n",
            "     22       62.3239  0.0344\n",
            "     23       62.3239  0.0326\n",
            "     24       62.3239  0.0321\n",
            "     25       62.3239  0.0398\n",
            "     26       62.3239  0.0495\n",
            "     27       62.3239  0.0555\n",
            "     28       62.3239  0.0500\n",
            "     29       62.3239  0.0540\n",
            "     30       62.3239  0.0461\n",
            "     31       62.3239  0.0612\n",
            "     32       62.3239  0.1014\n",
            "     33       62.3239  0.0695\n",
            "     34       62.3239  0.0624\n",
            "     35       62.3239  0.0315\n",
            "     36       62.3239  0.0363\n",
            "     37       62.3239  0.0350\n",
            "     38       62.3239  0.0343\n",
            "     39       62.3239  0.0330\n",
            "     40       62.3239  0.0301\n",
            "     41       62.3239  0.0361\n",
            "     42       62.3239  0.0324\n",
            "     43       62.3239  0.0285\n",
            "     44       62.3239  0.0299\n",
            "     45       62.3239  0.0332\n",
            "     46       62.3239  0.0338\n",
            "     47       62.3239  0.0335\n",
            "     48       62.3239  0.0339\n",
            "     49       62.3239  0.0302\n",
            "     50       62.3239  0.0315\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0295\n",
            "      2       37.1930  0.0370\n",
            "      3       37.1930  0.0332\n",
            "      4       37.1930  0.0344\n",
            "      5       37.1930  0.0319\n",
            "      6       37.1930  0.0328\n",
            "      7       37.1930  0.0322\n",
            "      8       37.1930  0.0379\n",
            "      9       37.1930  0.0374\n",
            "     10       37.1930  0.0348\n",
            "     11       37.1930  0.0354\n",
            "     12       37.1930  0.0352\n",
            "     13       37.1930  0.0359\n",
            "     14       37.1930  0.0298\n",
            "     15       37.1930  0.0337\n",
            "     16       37.1930  0.0333\n",
            "     17       37.1930  0.0415\n",
            "     18       37.1930  0.0369\n",
            "     19       37.1930  0.0442\n",
            "     20       37.1930  0.0322\n",
            "     21       37.1930  0.0339\n",
            "     22       37.1930  0.0323\n",
            "     23       37.1930  0.0393\n",
            "     24       37.1930  0.0297\n",
            "     25       37.1930  0.0324\n",
            "     26       37.1930  0.0390\n",
            "     27       37.1930  0.0342\n",
            "     28       37.1930  0.0332\n",
            "     29       37.1930  0.0332\n",
            "     30       37.1930  0.0405\n",
            "     31       37.1930  0.0356\n",
            "     32       37.1930  0.0331\n",
            "     33       37.1930  0.0337\n",
            "     34       37.1930  0.0335\n",
            "     35       37.1930  0.0320\n",
            "     36       37.1930  0.0453\n",
            "     37       37.1930  0.0520\n",
            "     38       37.1930  0.0319\n",
            "     39       37.1930  0.0394\n",
            "     40       37.1930  0.0368\n",
            "     41       37.1930  0.0362\n",
            "     42       37.1930  0.0361\n",
            "     43       37.1930  0.0332\n",
            "     44       37.1930  0.0325\n",
            "     45       37.1930  0.0330\n",
            "     46       37.1930  0.0334\n",
            "     47       37.1930  0.0343\n",
            "     48       37.1930  0.0375\n",
            "     49       37.1930  0.0299\n",
            "     50       37.1930  0.0331\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.4783\u001b[0m  0.0517\n",
            "      2        \u001b[36m1.4157\u001b[0m  0.0430\n",
            "      3        \u001b[36m1.3514\u001b[0m  0.0397\n",
            "      4        \u001b[36m1.2859\u001b[0m  0.0458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m1.2192\u001b[0m  0.0490\n",
            "      6        \u001b[36m1.1492\u001b[0m  0.0408\n",
            "      7        \u001b[36m1.0711\u001b[0m  0.0455\n",
            "      8        \u001b[36m0.9783\u001b[0m  0.0473\n",
            "      9        \u001b[36m0.8721\u001b[0m  0.0458\n",
            "     10        \u001b[36m0.7754\u001b[0m  0.0655\n",
            "     11        \u001b[36m0.7139\u001b[0m  0.0574\n",
            "     12        \u001b[36m0.6859\u001b[0m  0.0477\n",
            "     13        \u001b[36m0.6751\u001b[0m  0.0477\n",
            "     14        \u001b[36m0.6709\u001b[0m  0.0436\n",
            "     15        \u001b[36m0.6691\u001b[0m  0.0490\n",
            "     16        \u001b[36m0.6682\u001b[0m  0.0435\n",
            "     17        \u001b[36m0.6677\u001b[0m  0.0428\n",
            "     18        \u001b[36m0.6673\u001b[0m  0.0426\n",
            "     19        \u001b[36m0.6671\u001b[0m  0.0466\n",
            "     20        \u001b[36m0.6669\u001b[0m  0.0474\n",
            "     21        \u001b[36m0.6668\u001b[0m  0.0420\n",
            "     22        \u001b[36m0.6667\u001b[0m  0.0435\n",
            "     23        \u001b[36m0.6666\u001b[0m  0.0418\n",
            "     24        \u001b[36m0.6665\u001b[0m  0.0417\n",
            "     25        \u001b[36m0.6665\u001b[0m  0.0458\n",
            "     26        \u001b[36m0.6664\u001b[0m  0.0422\n",
            "     27        \u001b[36m0.6664\u001b[0m  0.0459\n",
            "     28        \u001b[36m0.6663\u001b[0m  0.0489\n",
            "     29        \u001b[36m0.6663\u001b[0m  0.0503\n",
            "     30        \u001b[36m0.6662\u001b[0m  0.0584\n",
            "     31        \u001b[36m0.6662\u001b[0m  0.0487\n",
            "     32        \u001b[36m0.6662\u001b[0m  0.0519\n",
            "     33        \u001b[36m0.6662\u001b[0m  0.0429\n",
            "     34        \u001b[36m0.6661\u001b[0m  0.0416\n",
            "     35        \u001b[36m0.6661\u001b[0m  0.0461\n",
            "     36        \u001b[36m0.6661\u001b[0m  0.0397\n",
            "     37        \u001b[36m0.6661\u001b[0m  0.0595\n",
            "     38        \u001b[36m0.6660\u001b[0m  0.1033\n",
            "     39        \u001b[36m0.6533\u001b[0m  0.0685\n",
            "     40        0.6746  0.0589\n",
            "     41        0.6719  0.1073\n",
            "     42        0.6703  0.0789\n",
            "     43        0.6693  0.0469\n",
            "     44        0.6688  0.0444\n",
            "     45        \u001b[36m0.6503\u001b[0m  0.0458\n",
            "     46        0.7031  0.0558\n",
            "     47        0.6873  0.0447\n",
            "     48        0.6794  0.0469\n",
            "     49        0.6757  0.0446\n",
            "     50        0.6737  0.0426\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.8482\u001b[0m  0.0416\n",
            "      2        \u001b[36m1.7786\u001b[0m  0.0438\n",
            "      3        \u001b[36m1.7088\u001b[0m  0.0515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m1.6350\u001b[0m  0.0942\n",
            "      5        \u001b[36m1.5546\u001b[0m  0.0667\n",
            "      6        \u001b[36m1.4599\u001b[0m  0.0703\n",
            "      7        \u001b[36m1.3339\u001b[0m  0.0440\n",
            "      8        \u001b[36m1.1542\u001b[0m  0.0446\n",
            "      9        \u001b[36m0.9320\u001b[0m  0.0601\n",
            "     10        \u001b[36m0.7608\u001b[0m  0.0504\n",
            "     11        \u001b[36m0.6911\u001b[0m  0.0455\n",
            "     12        \u001b[36m0.6737\u001b[0m  0.0566\n",
            "     13        \u001b[36m0.6704\u001b[0m  0.0476\n",
            "     14        \u001b[36m0.6697\u001b[0m  0.1341\n",
            "     15        \u001b[36m0.6692\u001b[0m  0.0676\n",
            "     16        \u001b[36m0.6689\u001b[0m  0.0491\n",
            "     17        \u001b[36m0.6686\u001b[0m  0.0498\n",
            "     18        \u001b[36m0.6684\u001b[0m  0.0624\n",
            "     19        \u001b[36m0.6682\u001b[0m  0.0769\n",
            "     20        \u001b[36m0.6681\u001b[0m  0.0674\n",
            "     21        \u001b[36m0.6679\u001b[0m  0.0843\n",
            "     22        \u001b[36m0.6678\u001b[0m  0.1241\n",
            "     23        \u001b[36m0.6677\u001b[0m  0.0604\n",
            "     24        \u001b[36m0.6676\u001b[0m  0.0502\n",
            "     25        \u001b[36m0.6676\u001b[0m  0.0461\n",
            "     26        \u001b[36m0.6675\u001b[0m  0.0464\n",
            "     27        \u001b[36m0.6674\u001b[0m  0.0494\n",
            "     28        \u001b[36m0.6674\u001b[0m  0.0576\n",
            "     29        \u001b[36m0.6572\u001b[0m  0.0514\n",
            "     30        0.6876  0.0558\n",
            "     31        0.6828  0.0626\n",
            "     32        0.6793  0.0436\n",
            "     33        0.6751  0.0425\n",
            "     34        0.6731  0.0451\n",
            "     35        0.6697  0.0472\n",
            "     36        0.6672  0.0453\n",
            "     37        0.6667  0.0675\n",
            "     38        0.6663  0.0598\n",
            "     39        0.6661  0.0451\n",
            "     40        0.6659  0.0491\n",
            "     41        0.6658  0.0517\n",
            "     42        0.6657  0.0502\n",
            "     43        0.6657  0.0800\n",
            "     44        0.6656  0.0753\n",
            "     45        0.6656  0.0975\n",
            "     46        0.6655  0.0550\n",
            "     47        0.6655  0.0429\n",
            "     48        0.6655  0.0394\n",
            "     49        0.6655  0.0508\n",
            "     50        0.6654  0.0661\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9916\u001b[0m  0.0462\n",
            "      2        \u001b[36m0.9694\u001b[0m  0.0454\n",
            "      3        \u001b[36m0.9482\u001b[0m  0.0463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9278\u001b[0m  0.0927\n",
            "      5        \u001b[36m0.9084\u001b[0m  0.0494\n",
            "      6        \u001b[36m0.8900\u001b[0m  0.0429\n",
            "      7        \u001b[36m0.8725\u001b[0m  0.0306\n",
            "      8        \u001b[36m0.8559\u001b[0m  0.0371\n",
            "      9        \u001b[36m0.8403\u001b[0m  0.0370\n",
            "     10        \u001b[36m0.8256\u001b[0m  0.0391\n",
            "     11        \u001b[36m0.8118\u001b[0m  0.0399\n",
            "     12        \u001b[36m0.7989\u001b[0m  0.0342\n",
            "     13        \u001b[36m0.7869\u001b[0m  0.0390\n",
            "     14        \u001b[36m0.7757\u001b[0m  0.0438\n",
            "     15        \u001b[36m0.7653\u001b[0m  0.0319\n",
            "     16        \u001b[36m0.7557\u001b[0m  0.0298\n",
            "     17        \u001b[36m0.7469\u001b[0m  0.0345\n",
            "     18        \u001b[36m0.7387\u001b[0m  0.0378\n",
            "     19        \u001b[36m0.7312\u001b[0m  0.0340\n",
            "     20        \u001b[36m0.7243\u001b[0m  0.0308\n",
            "     21        \u001b[36m0.7180\u001b[0m  0.0314\n",
            "     22        \u001b[36m0.7123\u001b[0m  0.0323\n",
            "     23        \u001b[36m0.7071\u001b[0m  0.0311\n",
            "     24        \u001b[36m0.7024\u001b[0m  0.0328\n",
            "     25        \u001b[36m0.6981\u001b[0m  0.0391\n",
            "     26        \u001b[36m0.6942\u001b[0m  0.0456\n",
            "     27        \u001b[36m0.6907\u001b[0m  0.0495\n",
            "     28        \u001b[36m0.6875\u001b[0m  0.0504\n",
            "     29        \u001b[36m0.6846\u001b[0m  0.0441\n",
            "     30        \u001b[36m0.6820\u001b[0m  0.1094\n",
            "     31        \u001b[36m0.6797\u001b[0m  0.0603\n",
            "     32        \u001b[36m0.6777\u001b[0m  0.1100\n",
            "     33        \u001b[36m0.6758\u001b[0m  0.0486\n",
            "     34        \u001b[36m0.6741\u001b[0m  0.1423\n",
            "     35        \u001b[36m0.6726\u001b[0m  0.0953\n",
            "     36        \u001b[36m0.6713\u001b[0m  0.0626\n",
            "     37        \u001b[36m0.6701\u001b[0m  0.0563\n",
            "     38        \u001b[36m0.6691\u001b[0m  0.0413\n",
            "     39        \u001b[36m0.6681\u001b[0m  0.0305\n",
            "     40        \u001b[36m0.6673\u001b[0m  0.0297\n",
            "     41        \u001b[36m0.6666\u001b[0m  0.0331\n",
            "     42        \u001b[36m0.6659\u001b[0m  0.0358\n",
            "     43        \u001b[36m0.6653\u001b[0m  0.0384\n",
            "     44        \u001b[36m0.6648\u001b[0m  0.0498\n",
            "     45        \u001b[36m0.6643\u001b[0m  0.0475\n",
            "     46        \u001b[36m0.6639\u001b[0m  0.0621\n",
            "     47        \u001b[36m0.6636\u001b[0m  0.0527\n",
            "     48        \u001b[36m0.6633\u001b[0m  0.0643\n",
            "     49        \u001b[36m0.6630\u001b[0m  0.0539\n",
            "     50        \u001b[36m0.6627\u001b[0m  0.0682\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.2106\u001b[0m  0.0301\n",
            "      2        \u001b[36m1.1831\u001b[0m  0.0377\n",
            "      3        \u001b[36m1.1561\u001b[0m  0.0410\n",
            "      4        \u001b[36m1.1299\u001b[0m  0.0397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m1.1042\u001b[0m  0.0317\n",
            "      6        \u001b[36m1.0793\u001b[0m  0.0350\n",
            "      7        \u001b[36m1.0552\u001b[0m  0.0310\n",
            "      8        \u001b[36m1.0317\u001b[0m  0.0318\n",
            "      9        \u001b[36m1.0091\u001b[0m  0.0304\n",
            "     10        \u001b[36m0.9873\u001b[0m  0.0332\n",
            "     11        \u001b[36m0.9663\u001b[0m  0.0309\n",
            "     12        \u001b[36m0.9461\u001b[0m  0.0325\n",
            "     13        \u001b[36m0.9268\u001b[0m  0.0354\n",
            "     14        \u001b[36m0.9084\u001b[0m  0.0329\n",
            "     15        \u001b[36m0.8908\u001b[0m  0.0298\n",
            "     16        \u001b[36m0.8741\u001b[0m  0.0340\n",
            "     17        \u001b[36m0.8583\u001b[0m  0.0324\n",
            "     18        \u001b[36m0.8434\u001b[0m  0.0350\n",
            "     19        \u001b[36m0.8293\u001b[0m  0.0376\n",
            "     20        \u001b[36m0.8160\u001b[0m  0.0367\n",
            "     21        \u001b[36m0.8036\u001b[0m  0.0354\n",
            "     22        \u001b[36m0.7919\u001b[0m  0.0321\n",
            "     23        \u001b[36m0.7811\u001b[0m  0.0366\n",
            "     24        \u001b[36m0.7710\u001b[0m  0.0337\n",
            "     25        \u001b[36m0.7616\u001b[0m  0.0339\n",
            "     26        \u001b[36m0.7529\u001b[0m  0.0342\n",
            "     27        \u001b[36m0.7448\u001b[0m  0.0396\n",
            "     28        \u001b[36m0.7374\u001b[0m  0.0318\n",
            "     29        \u001b[36m0.7305\u001b[0m  0.0410\n",
            "     30        \u001b[36m0.7242\u001b[0m  0.0303\n",
            "     31        \u001b[36m0.7184\u001b[0m  0.0321\n",
            "     32        \u001b[36m0.7131\u001b[0m  0.0359\n",
            "     33        \u001b[36m0.7083\u001b[0m  0.0353\n",
            "     34        \u001b[36m0.7039\u001b[0m  0.0348\n",
            "     35        \u001b[36m0.6998\u001b[0m  0.0358\n",
            "     36        \u001b[36m0.6961\u001b[0m  0.0342\n",
            "     37        \u001b[36m0.6928\u001b[0m  0.0331\n",
            "     38        \u001b[36m0.6897\u001b[0m  0.0320\n",
            "     39        \u001b[36m0.6870\u001b[0m  0.0339\n",
            "     40        \u001b[36m0.6845\u001b[0m  0.0380\n",
            "     41        \u001b[36m0.6822\u001b[0m  0.0315\n",
            "     42        \u001b[36m0.6801\u001b[0m  0.0327\n",
            "     43        \u001b[36m0.6783\u001b[0m  0.0331\n",
            "     44        \u001b[36m0.6766\u001b[0m  0.0316\n",
            "     45        \u001b[36m0.6751\u001b[0m  0.0331\n",
            "     46        \u001b[36m0.6737\u001b[0m  0.0359\n",
            "     47        \u001b[36m0.6724\u001b[0m  0.0354\n",
            "     48        \u001b[36m0.6713\u001b[0m  0.0350\n",
            "     49        \u001b[36m0.6703\u001b[0m  0.0388\n",
            "     50        \u001b[36m0.6694\u001b[0m  0.0334\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.8402\u001b[0m  0.0418\n",
            "      2        \u001b[36m2.7134\u001b[0m  0.0532\n",
            "      3        \u001b[36m2.5815\u001b[0m  0.0484\n",
            "      4        \u001b[36m2.4465\u001b[0m  0.0460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m2.3098\u001b[0m  0.0475\n",
            "      6        \u001b[36m2.1700\u001b[0m  0.0441\n",
            "      7        \u001b[36m2.0088\u001b[0m  0.0437\n",
            "      8        \u001b[36m1.7379\u001b[0m  0.0452\n",
            "      9        \u001b[36m1.2521\u001b[0m  0.0531\n",
            "     10        \u001b[36m0.7972\u001b[0m  0.0422\n",
            "     11        \u001b[36m0.6901\u001b[0m  0.0458\n",
            "     12        \u001b[36m0.6814\u001b[0m  0.0572\n",
            "     13        \u001b[36m0.6809\u001b[0m  0.0445\n",
            "     14        \u001b[36m0.6791\u001b[0m  0.0512\n",
            "     15        \u001b[36m0.6779\u001b[0m  0.0451\n",
            "     16        \u001b[36m0.6770\u001b[0m  0.0448\n",
            "     17        \u001b[36m0.6764\u001b[0m  0.0490\n",
            "     18        \u001b[36m0.6759\u001b[0m  0.0597\n",
            "     19        \u001b[36m0.6755\u001b[0m  0.0598\n",
            "     20        \u001b[36m0.6751\u001b[0m  0.0447\n",
            "     21        \u001b[36m0.6748\u001b[0m  0.0412\n",
            "     22        \u001b[36m0.6745\u001b[0m  0.0419\n",
            "     23        \u001b[36m0.6743\u001b[0m  0.0524\n",
            "     24        \u001b[36m0.6741\u001b[0m  0.0428\n",
            "     25        \u001b[36m0.6739\u001b[0m  0.0515\n",
            "     26        \u001b[36m0.6737\u001b[0m  0.0530\n",
            "     27        \u001b[36m0.6735\u001b[0m  0.0430\n",
            "     28        \u001b[36m0.6734\u001b[0m  0.0442\n",
            "     29        \u001b[36m0.6732\u001b[0m  0.0424\n",
            "     30        \u001b[36m0.6731\u001b[0m  0.0419\n",
            "     31        \u001b[36m0.6729\u001b[0m  0.0455\n",
            "     32        \u001b[36m0.6728\u001b[0m  0.0457\n",
            "     33        \u001b[36m0.6727\u001b[0m  0.0447\n",
            "     34        \u001b[36m0.6726\u001b[0m  0.0425\n",
            "     35        \u001b[36m0.6725\u001b[0m  0.0457\n",
            "     36        \u001b[36m0.6724\u001b[0m  0.0441\n",
            "     37        \u001b[36m0.6723\u001b[0m  0.0454\n",
            "     38        \u001b[36m0.6722\u001b[0m  0.0462\n",
            "     39        \u001b[36m0.6721\u001b[0m  0.0467\n",
            "     40        \u001b[36m0.6720\u001b[0m  0.0448\n",
            "     41        \u001b[36m0.6719\u001b[0m  0.0412\n",
            "     42        \u001b[36m0.6719\u001b[0m  0.0447\n",
            "     43        \u001b[36m0.6718\u001b[0m  0.0438\n",
            "     44        \u001b[36m0.6717\u001b[0m  0.0640\n",
            "     45        \u001b[36m0.6716\u001b[0m  0.0443\n",
            "     46        \u001b[36m0.6715\u001b[0m  0.0432\n",
            "     47        \u001b[36m0.6715\u001b[0m  0.0431\n",
            "     48        \u001b[36m0.6715\u001b[0m  0.0423\n",
            "     49        \u001b[36m0.6710\u001b[0m  0.0435\n",
            "     50        \u001b[36m0.6708\u001b[0m  0.0428\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m3.8125\u001b[0m  0.0393\n",
            "      2        \u001b[36m3.6811\u001b[0m  0.0478\n",
            "      3        \u001b[36m3.5504\u001b[0m  0.0420\n",
            "      4        \u001b[36m3.4166\u001b[0m  0.0436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m3.2806\u001b[0m  0.0511\n",
            "      6        \u001b[36m3.1406\u001b[0m  0.0402\n",
            "      7        \u001b[36m2.9742\u001b[0m  0.0423\n",
            "      8        \u001b[36m2.6498\u001b[0m  0.0526\n",
            "      9        \u001b[36m1.9621\u001b[0m  0.0526\n",
            "     10        \u001b[36m1.1885\u001b[0m  0.0479\n",
            "     11        \u001b[36m0.8220\u001b[0m  0.0474\n",
            "     12        \u001b[36m0.7035\u001b[0m  0.0436\n",
            "     13        \u001b[36m0.6762\u001b[0m  0.0454\n",
            "     14        \u001b[36m0.6733\u001b[0m  0.0427\n",
            "     15        \u001b[36m0.6723\u001b[0m  0.0528\n",
            "     16        \u001b[36m0.6715\u001b[0m  0.0432\n",
            "     17        \u001b[36m0.6709\u001b[0m  0.0430\n",
            "     18        \u001b[36m0.6705\u001b[0m  0.0543\n",
            "     19        \u001b[36m0.6701\u001b[0m  0.0420\n",
            "     20        \u001b[36m0.6699\u001b[0m  0.0430\n",
            "     21        \u001b[36m0.6697\u001b[0m  0.0430\n",
            "     22        \u001b[36m0.6695\u001b[0m  0.0457\n",
            "     23        \u001b[36m0.6484\u001b[0m  0.0438\n",
            "     24        0.7136  0.0448\n",
            "     25        0.6920  0.0505\n",
            "     26        0.6795  0.0463\n",
            "     27        0.6760  0.0502\n",
            "     28        0.6733  0.0477\n",
            "     29        0.6708  0.0471\n",
            "     30        0.6687  0.0446\n",
            "     31        0.6667  0.0415\n",
            "     32        0.6649  0.0471\n",
            "     33        0.6632  0.0444\n",
            "     34        0.6615  0.0439\n",
            "     35        0.6600  0.0416\n",
            "     36        0.6585  0.0522\n",
            "     37        0.6570  0.0521\n",
            "     38        0.6556  0.0433\n",
            "     39        0.6543  0.0443\n",
            "     40        0.6530  0.0438\n",
            "     41        0.6517  0.0431\n",
            "     42        0.6505  0.0466\n",
            "     43        0.6493  0.0459\n",
            "     44        \u001b[36m0.6482\u001b[0m  0.0453\n",
            "     45        0.6522  0.0552\n",
            "     46        0.6682  0.0450\n",
            "     47        0.6624  0.0495\n",
            "     48        0.6583  0.0546\n",
            "     49        0.6561  0.0472\n",
            "     50        0.6541  0.0492\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.8602\u001b[0m  0.0300\n",
            "      2        \u001b[36m2.7903\u001b[0m  0.0339\n",
            "      3        \u001b[36m2.7204\u001b[0m  0.0349\n",
            "      4        \u001b[36m2.6506\u001b[0m  0.0344\n",
            "      5        \u001b[36m2.5808\u001b[0m  0.0358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m2.5111\u001b[0m  0.0386\n",
            "      7        \u001b[36m2.4415\u001b[0m  0.0360\n",
            "      8        \u001b[36m2.3719\u001b[0m  0.0330\n",
            "      9        \u001b[36m2.3025\u001b[0m  0.0324\n",
            "     10        \u001b[36m2.2333\u001b[0m  0.0328\n",
            "     11        \u001b[36m2.1642\u001b[0m  0.0328\n",
            "     12        \u001b[36m2.0954\u001b[0m  0.0327\n",
            "     13        \u001b[36m2.0267\u001b[0m  0.0337\n",
            "     14        \u001b[36m1.9584\u001b[0m  0.0314\n",
            "     15        \u001b[36m1.8905\u001b[0m  0.0329\n",
            "     16        \u001b[36m1.8230\u001b[0m  0.0355\n",
            "     17        \u001b[36m1.7560\u001b[0m  0.0309\n",
            "     18        \u001b[36m1.6897\u001b[0m  0.0329\n",
            "     19        \u001b[36m1.6240\u001b[0m  0.0354\n",
            "     20        \u001b[36m1.5592\u001b[0m  0.0343\n",
            "     21        \u001b[36m1.4955\u001b[0m  0.0315\n",
            "     22        \u001b[36m1.4329\u001b[0m  0.0358\n",
            "     23        \u001b[36m1.3717\u001b[0m  0.0364\n",
            "     24        \u001b[36m1.3120\u001b[0m  0.0359\n",
            "     25        \u001b[36m1.2542\u001b[0m  0.0344\n",
            "     26        \u001b[36m1.1984\u001b[0m  0.0325\n",
            "     27        \u001b[36m1.1449\u001b[0m  0.0336\n",
            "     28        \u001b[36m1.0939\u001b[0m  0.0342\n",
            "     29        \u001b[36m1.0458\u001b[0m  0.0370\n",
            "     30        \u001b[36m1.0006\u001b[0m  0.0362\n",
            "     31        \u001b[36m0.9586\u001b[0m  0.0383\n",
            "     32        \u001b[36m0.9199\u001b[0m  0.0383\n",
            "     33        \u001b[36m0.8847\u001b[0m  0.0316\n",
            "     34        \u001b[36m0.8528\u001b[0m  0.0325\n",
            "     35        \u001b[36m0.8244\u001b[0m  0.0399\n",
            "     36        \u001b[36m0.7992\u001b[0m  0.0321\n",
            "     37        \u001b[36m0.7771\u001b[0m  0.0319\n",
            "     38        \u001b[36m0.7580\u001b[0m  0.0302\n",
            "     39        \u001b[36m0.7416\u001b[0m  0.0314\n",
            "     40        \u001b[36m0.7276\u001b[0m  0.0304\n",
            "     41        \u001b[36m0.7158\u001b[0m  0.0321\n",
            "     42        \u001b[36m0.7059\u001b[0m  0.0366\n",
            "     43        \u001b[36m0.6976\u001b[0m  0.0344\n",
            "     44        \u001b[36m0.6908\u001b[0m  0.0327\n",
            "     45        \u001b[36m0.6851\u001b[0m  0.0327\n",
            "     46        \u001b[36m0.6805\u001b[0m  0.0324\n",
            "     47        \u001b[36m0.6768\u001b[0m  0.0358\n",
            "     48        \u001b[36m0.6737\u001b[0m  0.0340\n",
            "     49        \u001b[36m0.6712\u001b[0m  0.0341\n",
            "     50        \u001b[36m0.6692\u001b[0m  0.0410\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m3.9666\u001b[0m  0.0345\n",
            "      2        \u001b[36m3.8994\u001b[0m  0.0326\n",
            "      3        \u001b[36m3.8324\u001b[0m  0.0349\n",
            "      4        \u001b[36m3.7654\u001b[0m  0.0358\n",
            "      5        \u001b[36m3.6986\u001b[0m  0.0340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m3.6314\u001b[0m  0.0393\n",
            "      7        \u001b[36m3.5643\u001b[0m  0.0334\n",
            "      8        \u001b[36m3.4975\u001b[0m  0.0318\n",
            "      9        \u001b[36m3.4304\u001b[0m  0.0396\n",
            "     10        \u001b[36m3.3634\u001b[0m  0.0329\n",
            "     11        \u001b[36m3.2964\u001b[0m  0.0331\n",
            "     12        \u001b[36m3.2295\u001b[0m  0.0424\n",
            "     13        \u001b[36m3.1626\u001b[0m  0.0312\n",
            "     14        \u001b[36m3.0956\u001b[0m  0.0332\n",
            "     15        \u001b[36m3.0286\u001b[0m  0.0314\n",
            "     16        \u001b[36m2.9617\u001b[0m  0.0306\n",
            "     17        \u001b[36m2.8948\u001b[0m  0.0323\n",
            "     18        \u001b[36m2.8280\u001b[0m  0.0354\n",
            "     19        \u001b[36m2.7612\u001b[0m  0.0384\n",
            "     20        \u001b[36m2.6944\u001b[0m  0.0352\n",
            "     21        \u001b[36m2.6276\u001b[0m  0.0331\n",
            "     22        \u001b[36m2.5610\u001b[0m  0.0336\n",
            "     23        \u001b[36m2.4943\u001b[0m  0.0356\n",
            "     24        \u001b[36m2.4278\u001b[0m  0.0352\n",
            "     25        \u001b[36m2.3614\u001b[0m  0.0356\n",
            "     26        \u001b[36m2.2951\u001b[0m  0.0339\n",
            "     27        \u001b[36m2.2289\u001b[0m  0.0358\n",
            "     28        \u001b[36m2.1629\u001b[0m  0.0314\n",
            "     29        \u001b[36m2.0970\u001b[0m  0.0343\n",
            "     30        \u001b[36m2.0315\u001b[0m  0.0339\n",
            "     31        \u001b[36m1.9662\u001b[0m  0.0307\n",
            "     32        \u001b[36m1.9012\u001b[0m  0.0339\n",
            "     33        \u001b[36m1.8366\u001b[0m  0.0334\n",
            "     34        \u001b[36m1.7725\u001b[0m  0.0342\n",
            "     35        \u001b[36m1.7089\u001b[0m  0.0337\n",
            "     36        \u001b[36m1.6460\u001b[0m  0.0328\n",
            "     37        \u001b[36m1.5838\u001b[0m  0.0378\n",
            "     38        \u001b[36m1.5225\u001b[0m  0.0332\n",
            "     39        \u001b[36m1.4622\u001b[0m  0.0470\n",
            "     40        \u001b[36m1.4031\u001b[0m  0.0331\n",
            "     41        \u001b[36m1.3454\u001b[0m  0.0329\n",
            "     42        \u001b[36m1.2892\u001b[0m  0.0320\n",
            "     43        \u001b[36m1.2348\u001b[0m  0.0308\n",
            "     44        \u001b[36m1.1823\u001b[0m  0.0452\n",
            "     45        \u001b[36m1.1321\u001b[0m  0.0387\n",
            "     46        \u001b[36m1.0842\u001b[0m  0.0332\n",
            "     47        \u001b[36m1.0390\u001b[0m  0.0380\n",
            "     48        \u001b[36m0.9965\u001b[0m  0.0335\n",
            "     49        \u001b[36m0.9570\u001b[0m  0.0317\n",
            "     50        \u001b[36m0.9205\u001b[0m  0.0352\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0112\u001b[0m  0.0477\n",
            "      2        \u001b[36m0.9358\u001b[0m  0.0464\n",
            "      3        \u001b[36m0.8740\u001b[0m  0.0478\n",
            "      4        \u001b[36m0.8177\u001b[0m  0.0435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.7928\u001b[0m  0.0491\n",
            "      6        \u001b[36m0.7414\u001b[0m  0.0441\n",
            "      7        \u001b[36m0.7239\u001b[0m  0.0485\n",
            "      8        \u001b[36m0.6913\u001b[0m  0.0479\n",
            "      9        \u001b[36m0.6709\u001b[0m  0.0442\n",
            "     10        \u001b[36m0.6495\u001b[0m  0.0416\n",
            "     11        \u001b[36m0.6159\u001b[0m  0.0437\n",
            "     12        \u001b[36m0.6100\u001b[0m  0.0543\n",
            "     13        \u001b[36m0.5975\u001b[0m  0.0433\n",
            "     14        \u001b[36m0.5870\u001b[0m  0.0423\n",
            "     15        \u001b[36m0.5625\u001b[0m  0.0440\n",
            "     16        0.5653  0.0495\n",
            "     17        0.5801  0.0542\n",
            "     18        \u001b[36m0.5595\u001b[0m  0.0459\n",
            "     19        \u001b[36m0.5588\u001b[0m  0.0469\n",
            "     20        \u001b[36m0.5536\u001b[0m  0.0440\n",
            "     21        \u001b[36m0.5444\u001b[0m  0.0450\n",
            "     22        0.5653  0.0451\n",
            "     23        0.5590  0.0447\n",
            "     24        0.5551  0.0448\n",
            "     25        0.5520  0.0428\n",
            "     26        0.5484  0.0466\n",
            "     27        0.5447  0.0436\n",
            "     28        \u001b[36m0.5413\u001b[0m  0.0420\n",
            "     29        \u001b[36m0.5381\u001b[0m  0.0409\n",
            "     30        \u001b[36m0.5351\u001b[0m  0.0422\n",
            "     31        \u001b[36m0.5323\u001b[0m  0.0423\n",
            "     32        \u001b[36m0.5297\u001b[0m  0.0420\n",
            "     33        \u001b[36m0.5271\u001b[0m  0.0464\n",
            "     34        \u001b[36m0.5247\u001b[0m  0.0581\n",
            "     35        \u001b[36m0.5225\u001b[0m  0.0440\n",
            "     36        \u001b[36m0.5203\u001b[0m  0.0469\n",
            "     37        \u001b[36m0.5181\u001b[0m  0.0515\n",
            "     38        \u001b[36m0.5153\u001b[0m  0.0478\n",
            "     39        \u001b[36m0.5134\u001b[0m  0.0499\n",
            "     40        \u001b[36m0.5117\u001b[0m  0.0454\n",
            "     41        \u001b[36m0.5111\u001b[0m  0.0485\n",
            "     42        \u001b[36m0.5090\u001b[0m  0.0492\n",
            "     43        \u001b[36m0.5066\u001b[0m  0.0517\n",
            "     44        \u001b[36m0.5046\u001b[0m  0.0496\n",
            "     45        \u001b[36m0.5014\u001b[0m  0.0463\n",
            "     46        \u001b[36m0.4821\u001b[0m  0.0429\n",
            "     47        \u001b[36m0.4801\u001b[0m  0.0487\n",
            "     48        \u001b[36m0.4785\u001b[0m  0.0394\n",
            "     49        \u001b[36m0.4770\u001b[0m  0.0423\n",
            "     50        \u001b[36m0.4757\u001b[0m  0.0431\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.7518\u001b[0m  0.0400\n",
            "      2        \u001b[36m1.5674\u001b[0m  0.0449\n",
            "      3        \u001b[36m1.4277\u001b[0m  0.0456\n",
            "      4        \u001b[36m1.2947\u001b[0m  0.0514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m1.1919\u001b[0m  0.0470\n",
            "      6        \u001b[36m1.1009\u001b[0m  0.0412\n",
            "      7        \u001b[36m1.0000\u001b[0m  0.0441\n",
            "      8        \u001b[36m0.9407\u001b[0m  0.0436\n",
            "      9        \u001b[36m0.8636\u001b[0m  0.0454\n",
            "     10        \u001b[36m0.8146\u001b[0m  0.0479\n",
            "     11        \u001b[36m0.7929\u001b[0m  0.0494\n",
            "     12        \u001b[36m0.7826\u001b[0m  0.0456\n",
            "     13        \u001b[36m0.7594\u001b[0m  0.0478\n",
            "     14        \u001b[36m0.7368\u001b[0m  0.0439\n",
            "     15        \u001b[36m0.7285\u001b[0m  0.0443\n",
            "     16        \u001b[36m0.7219\u001b[0m  0.0429\n",
            "     17        \u001b[36m0.6959\u001b[0m  0.0442\n",
            "     18        0.6983  0.0423\n",
            "     19        \u001b[36m0.6951\u001b[0m  0.0486\n",
            "     20        \u001b[36m0.6924\u001b[0m  0.0443\n",
            "     21        \u001b[36m0.6900\u001b[0m  0.0421\n",
            "     22        \u001b[36m0.6880\u001b[0m  0.0406\n",
            "     23        \u001b[36m0.6861\u001b[0m  0.0412\n",
            "     24        \u001b[36m0.6844\u001b[0m  0.0413\n",
            "     25        \u001b[36m0.6829\u001b[0m  0.0513\n",
            "     26        \u001b[36m0.6815\u001b[0m  0.0430\n",
            "     27        \u001b[36m0.6798\u001b[0m  0.0420\n",
            "     28        \u001b[36m0.6786\u001b[0m  0.0474\n",
            "     29        \u001b[36m0.6775\u001b[0m  0.0393\n",
            "     30        \u001b[36m0.6764\u001b[0m  0.0435\n",
            "     31        \u001b[36m0.6753\u001b[0m  0.0450\n",
            "     32        \u001b[36m0.6743\u001b[0m  0.0567\n",
            "     33        \u001b[36m0.6733\u001b[0m  0.0485\n",
            "     34        \u001b[36m0.6724\u001b[0m  0.0539\n",
            "     35        \u001b[36m0.6714\u001b[0m  0.0588\n",
            "     36        \u001b[36m0.6705\u001b[0m  0.0447\n",
            "     37        \u001b[36m0.6697\u001b[0m  0.0415\n",
            "     38        \u001b[36m0.6688\u001b[0m  0.0417\n",
            "     39        \u001b[36m0.6680\u001b[0m  0.0391\n",
            "     40        \u001b[36m0.6672\u001b[0m  0.0449\n",
            "     41        \u001b[36m0.6664\u001b[0m  0.0426\n",
            "     42        \u001b[36m0.6656\u001b[0m  0.0426\n",
            "     43        \u001b[36m0.6644\u001b[0m  0.0482\n",
            "     44        \u001b[36m0.6637\u001b[0m  0.0462\n",
            "     45        0.6670  0.0463\n",
            "     46        0.6728  0.0527\n",
            "     47        0.6717  0.0418\n",
            "     48        0.6710  0.0486\n",
            "     49        0.6701  0.0986\n",
            "     50        0.6692  0.0779\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9931\u001b[0m  0.0312\n",
            "      2        \u001b[36m0.9889\u001b[0m  0.0487\n",
            "      3        \u001b[36m0.9714\u001b[0m  0.0852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9583\u001b[0m  0.0618\n",
            "      5        \u001b[36m0.9456\u001b[0m  0.0549\n",
            "      6        \u001b[36m0.9335\u001b[0m  0.0677\n",
            "      7        \u001b[36m0.9205\u001b[0m  0.0418\n",
            "      8        \u001b[36m0.9108\u001b[0m  0.0333\n",
            "      9        \u001b[36m0.9002\u001b[0m  0.0351\n",
            "     10        \u001b[36m0.8901\u001b[0m  0.0360\n",
            "     11        \u001b[36m0.8806\u001b[0m  0.0324\n",
            "     12        \u001b[36m0.8715\u001b[0m  0.0303\n",
            "     13        \u001b[36m0.8629\u001b[0m  0.0409\n",
            "     14        \u001b[36m0.8547\u001b[0m  0.0314\n",
            "     15        \u001b[36m0.8471\u001b[0m  0.0425\n",
            "     16        \u001b[36m0.8398\u001b[0m  0.0358\n",
            "     17        \u001b[36m0.8330\u001b[0m  0.0322\n",
            "     18        \u001b[36m0.8265\u001b[0m  0.0290\n",
            "     19        \u001b[36m0.8204\u001b[0m  0.0368\n",
            "     20        \u001b[36m0.8147\u001b[0m  0.0371\n",
            "     21        \u001b[36m0.8093\u001b[0m  0.0333\n",
            "     22        \u001b[36m0.8043\u001b[0m  0.0556\n",
            "     23        \u001b[36m0.7995\u001b[0m  0.0518\n",
            "     24        \u001b[36m0.7950\u001b[0m  0.0388\n",
            "     25        \u001b[36m0.7908\u001b[0m  0.0407\n",
            "     26        \u001b[36m0.7867\u001b[0m  0.0382\n",
            "     27        \u001b[36m0.7830\u001b[0m  0.0338\n",
            "     28        \u001b[36m0.7794\u001b[0m  0.0323\n",
            "     29        \u001b[36m0.7760\u001b[0m  0.0319\n",
            "     30        \u001b[36m0.7728\u001b[0m  0.0433\n",
            "     31        \u001b[36m0.7697\u001b[0m  0.0388\n",
            "     32        \u001b[36m0.7668\u001b[0m  0.0317\n",
            "     33        \u001b[36m0.7640\u001b[0m  0.0314\n",
            "     34        \u001b[36m0.7614\u001b[0m  0.0310\n",
            "     35        \u001b[36m0.7589\u001b[0m  0.0316\n",
            "     36        \u001b[36m0.7565\u001b[0m  0.0369\n",
            "     37        \u001b[36m0.7542\u001b[0m  0.0331\n",
            "     38        \u001b[36m0.7519\u001b[0m  0.0507\n",
            "     39        \u001b[36m0.7498\u001b[0m  0.0525\n",
            "     40        \u001b[36m0.7478\u001b[0m  0.0568\n",
            "     41        \u001b[36m0.7458\u001b[0m  0.0415\n",
            "     42        \u001b[36m0.7439\u001b[0m  0.0321\n",
            "     43        \u001b[36m0.7421\u001b[0m  0.0358\n",
            "     44        \u001b[36m0.7403\u001b[0m  0.0313\n",
            "     45        \u001b[36m0.7386\u001b[0m  0.0331\n",
            "     46        \u001b[36m0.7370\u001b[0m  0.0352\n",
            "     47        \u001b[36m0.7353\u001b[0m  0.0349\n",
            "     48        \u001b[36m0.7338\u001b[0m  0.0703\n",
            "     49        \u001b[36m0.7323\u001b[0m  0.0477\n",
            "     50        \u001b[36m0.7308\u001b[0m  0.0625\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0266\u001b[0m  0.0290\n",
            "      2        \u001b[36m1.0020\u001b[0m  0.0361\n",
            "      3        \u001b[36m0.9767\u001b[0m  0.0312\n",
            "      4        \u001b[36m0.9530\u001b[0m  0.0321\n",
            "      5        \u001b[36m0.9309\u001b[0m  0.0323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m0.9105\u001b[0m  0.0349\n",
            "      7        \u001b[36m0.8917\u001b[0m  0.0480\n",
            "      8        \u001b[36m0.8744\u001b[0m  0.0454\n",
            "      9        \u001b[36m0.8586\u001b[0m  0.0498\n",
            "     10        \u001b[36m0.8442\u001b[0m  0.0449\n",
            "     11        \u001b[36m0.8311\u001b[0m  0.0515\n",
            "     12        \u001b[36m0.8193\u001b[0m  0.0474\n",
            "     13        \u001b[36m0.8086\u001b[0m  0.0503\n",
            "     14        \u001b[36m0.7988\u001b[0m  0.0360\n",
            "     15        \u001b[36m0.7900\u001b[0m  0.0330\n",
            "     16        \u001b[36m0.7821\u001b[0m  0.0339\n",
            "     17        \u001b[36m0.7748\u001b[0m  0.0330\n",
            "     18        \u001b[36m0.7682\u001b[0m  0.0444\n",
            "     19        \u001b[36m0.7623\u001b[0m  0.0521\n",
            "     20        \u001b[36m0.7568\u001b[0m  0.0758\n",
            "     21        \u001b[36m0.7518\u001b[0m  0.0402\n",
            "     22        \u001b[36m0.7472\u001b[0m  0.0318\n",
            "     23        \u001b[36m0.7429\u001b[0m  0.0303\n",
            "     24        \u001b[36m0.7390\u001b[0m  0.0356\n",
            "     25        \u001b[36m0.7354\u001b[0m  0.0325\n",
            "     26        \u001b[36m0.7320\u001b[0m  0.0312\n",
            "     27        \u001b[36m0.7289\u001b[0m  0.0356\n",
            "     28        \u001b[36m0.7259\u001b[0m  0.0337\n",
            "     29        \u001b[36m0.7232\u001b[0m  0.0334\n",
            "     30        \u001b[36m0.7206\u001b[0m  0.0313\n",
            "     31        \u001b[36m0.7181\u001b[0m  0.0408\n",
            "     32        \u001b[36m0.7158\u001b[0m  0.0492\n",
            "     33        \u001b[36m0.7136\u001b[0m  0.0348\n",
            "     34        \u001b[36m0.7115\u001b[0m  0.0400\n",
            "     35        \u001b[36m0.7095\u001b[0m  0.0308\n",
            "     36        \u001b[36m0.7076\u001b[0m  0.0337\n",
            "     37        \u001b[36m0.7058\u001b[0m  0.0329\n",
            "     38        \u001b[36m0.7040\u001b[0m  0.0339\n",
            "     39        \u001b[36m0.7023\u001b[0m  0.0472\n",
            "     40        \u001b[36m0.7007\u001b[0m  0.0498\n",
            "     41        \u001b[36m0.6991\u001b[0m  0.0513\n",
            "     42        \u001b[36m0.6976\u001b[0m  0.0513\n",
            "     43        \u001b[36m0.6961\u001b[0m  0.0540\n",
            "     44        \u001b[36m0.6947\u001b[0m  0.0502\n",
            "     45        \u001b[36m0.6933\u001b[0m  0.0450\n",
            "     46        \u001b[36m0.6919\u001b[0m  0.1022\n",
            "     47        \u001b[36m0.6906\u001b[0m  0.0672\n",
            "     48        \u001b[36m0.6893\u001b[0m  0.0509\n",
            "     49        \u001b[36m0.6881\u001b[0m  0.0532\n",
            "     50        \u001b[36m0.6868\u001b[0m  0.0895\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8309\u001b[0m  0.1358\n",
            "      2        \u001b[36m0.7708\u001b[0m  0.0496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      3        \u001b[36m0.7263\u001b[0m  0.0504\n",
            "      4        0.7297  0.0724\n",
            "      5        \u001b[36m0.7188\u001b[0m  0.0449\n",
            "      6        \u001b[36m0.6951\u001b[0m  0.0532\n",
            "      7        \u001b[36m0.6629\u001b[0m  0.0436\n",
            "      8        \u001b[36m0.6543\u001b[0m  0.0599\n",
            "      9        \u001b[36m0.6338\u001b[0m  0.0422\n",
            "     10        \u001b[36m0.6240\u001b[0m  0.0417\n",
            "     11        \u001b[36m0.6128\u001b[0m  0.0430\n",
            "     12        0.6146  0.0658\n",
            "     13        \u001b[36m0.5978\u001b[0m  0.0646\n",
            "     14        \u001b[36m0.5766\u001b[0m  0.0418\n",
            "     15        \u001b[36m0.5687\u001b[0m  0.0453\n",
            "     16        \u001b[36m0.5643\u001b[0m  0.0413\n",
            "     17        \u001b[36m0.5553\u001b[0m  0.0455\n",
            "     18        \u001b[36m0.5523\u001b[0m  0.0558\n",
            "     19        \u001b[36m0.5501\u001b[0m  0.0553\n",
            "     20        \u001b[36m0.5440\u001b[0m  0.0460\n",
            "     21        0.5484  0.0499\n",
            "     22        0.5455  0.0493\n",
            "     23        \u001b[36m0.5429\u001b[0m  0.0518\n",
            "     24        \u001b[36m0.5390\u001b[0m  0.0688\n",
            "     25        \u001b[36m0.5365\u001b[0m  0.0497\n",
            "     26        \u001b[36m0.5344\u001b[0m  0.0542\n",
            "     27        \u001b[36m0.5323\u001b[0m  0.0420\n",
            "     28        \u001b[36m0.5304\u001b[0m  0.0438\n",
            "     29        \u001b[36m0.5284\u001b[0m  0.0876\n",
            "     30        \u001b[36m0.5267\u001b[0m  0.0718\n",
            "     31        \u001b[36m0.5249\u001b[0m  0.0789\n",
            "     32        \u001b[36m0.5238\u001b[0m  0.0704\n",
            "     33        \u001b[36m0.5214\u001b[0m  0.0918\n",
            "     34        \u001b[36m0.5206\u001b[0m  0.0581\n",
            "     35        0.5238  0.0474\n",
            "     36        0.5219  0.0429\n",
            "     37        \u001b[36m0.5200\u001b[0m  0.0493\n",
            "     38        0.5308  0.0598\n",
            "     39        0.5228  0.0573\n",
            "     40        \u001b[36m0.5161\u001b[0m  0.0533\n",
            "     41        \u001b[36m0.5135\u001b[0m  0.0476\n",
            "     42        \u001b[36m0.5124\u001b[0m  0.0806\n",
            "     43        \u001b[36m0.5104\u001b[0m  0.0857\n",
            "     44        \u001b[36m0.5092\u001b[0m  0.0482\n",
            "     45        \u001b[36m0.5076\u001b[0m  0.0510\n",
            "     46        \u001b[36m0.5061\u001b[0m  0.0436\n",
            "     47        \u001b[36m0.5003\u001b[0m  0.0491\n",
            "     48        0.5019  0.0471\n",
            "     49        0.5020  0.0705\n",
            "     50        0.5004  0.0886\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0372\u001b[0m  0.0406\n",
            "      2        \u001b[36m0.8812\u001b[0m  0.0491\n",
            "      3        \u001b[36m0.8345\u001b[0m  0.0479\n",
            "      4        \u001b[36m0.7812\u001b[0m  0.0490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.7176\u001b[0m  0.0739\n",
            "      6        \u001b[36m0.6721\u001b[0m  0.0809\n",
            "      7        \u001b[36m0.6337\u001b[0m  0.0794\n",
            "      8        \u001b[36m0.6078\u001b[0m  0.0693\n",
            "      9        \u001b[36m0.5916\u001b[0m  0.0694\n",
            "     10        \u001b[36m0.5788\u001b[0m  0.0413\n",
            "     11        \u001b[36m0.5688\u001b[0m  0.0657\n",
            "     12        \u001b[36m0.5599\u001b[0m  0.0443\n",
            "     13        \u001b[36m0.5517\u001b[0m  0.0432\n",
            "     14        \u001b[36m0.5425\u001b[0m  0.0436\n",
            "     15        \u001b[36m0.5351\u001b[0m  0.0425\n",
            "     16        \u001b[36m0.5276\u001b[0m  0.0464\n",
            "     17        \u001b[36m0.5271\u001b[0m  0.0497\n",
            "     18        0.5387  0.0437\n",
            "     19        0.5424  0.0516\n",
            "     20        0.5329  0.0482\n",
            "     21        0.5307  0.0543\n",
            "     22        \u001b[36m0.5249\u001b[0m  0.0443\n",
            "     23        0.5289  0.0422\n",
            "     24        0.5285  0.0434\n",
            "     25        \u001b[36m0.5101\u001b[0m  0.0427\n",
            "     26        \u001b[36m0.5035\u001b[0m  0.0671\n",
            "     27        \u001b[36m0.4986\u001b[0m  0.0416\n",
            "     28        \u001b[36m0.4956\u001b[0m  0.0431\n",
            "     29        \u001b[36m0.4783\u001b[0m  0.0620\n",
            "     30        \u001b[36m0.4682\u001b[0m  0.0501\n",
            "     31        \u001b[36m0.4629\u001b[0m  0.0462\n",
            "     32        0.4647  0.0737\n",
            "     33        0.4693  0.0780\n",
            "     34        \u001b[36m0.4485\u001b[0m  0.0969\n",
            "     35        \u001b[36m0.4449\u001b[0m  0.1228\n",
            "     36        \u001b[36m0.4402\u001b[0m  0.0519\n",
            "     37        \u001b[36m0.4385\u001b[0m  0.0590\n",
            "     38        \u001b[36m0.4361\u001b[0m  0.0630\n",
            "     39        \u001b[36m0.4356\u001b[0m  0.0427\n",
            "     40        \u001b[36m0.4333\u001b[0m  0.0417\n",
            "     41        0.4339  0.0421\n",
            "     42        \u001b[36m0.4319\u001b[0m  0.0554\n",
            "     43        \u001b[36m0.4302\u001b[0m  0.0498\n",
            "     44        0.4308  0.0711\n",
            "     45        \u001b[36m0.4297\u001b[0m  0.0683\n",
            "     46        0.4302  0.0678\n",
            "     47        \u001b[36m0.4293\u001b[0m  0.0922\n",
            "     48        \u001b[36m0.4286\u001b[0m  0.0445\n",
            "     49        \u001b[36m0.4273\u001b[0m  0.0500\n",
            "     50        \u001b[36m0.4265\u001b[0m  0.0633\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0913\u001b[0m  0.0331\n",
            "      2        1.1629  0.0376\n",
            "      3        \u001b[36m1.0088\u001b[0m  0.0385\n",
            "      4        \u001b[36m0.9363\u001b[0m  0.0398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.8813\u001b[0m  0.0431\n",
            "      6        \u001b[36m0.8444\u001b[0m  0.0398\n",
            "      7        \u001b[36m0.8211\u001b[0m  0.0360\n",
            "      8        \u001b[36m0.8063\u001b[0m  0.0415\n",
            "      9        \u001b[36m0.7966\u001b[0m  0.0330\n",
            "     10        \u001b[36m0.7897\u001b[0m  0.0335\n",
            "     11        \u001b[36m0.7847\u001b[0m  0.0584\n",
            "     12        \u001b[36m0.7808\u001b[0m  0.0467\n",
            "     13        \u001b[36m0.7720\u001b[0m  0.0899\n",
            "     14        \u001b[36m0.7671\u001b[0m  0.1192\n",
            "     15        0.7698  0.1635\n",
            "     16        0.7678  0.1205\n",
            "     17        \u001b[36m0.7659\u001b[0m  0.1109\n",
            "     18        \u001b[36m0.7641\u001b[0m  0.0542\n",
            "     19        \u001b[36m0.7625\u001b[0m  0.0757\n",
            "     20        \u001b[36m0.7609\u001b[0m  0.0752\n",
            "     21        \u001b[36m0.7594\u001b[0m  0.0343\n",
            "     22        \u001b[36m0.7580\u001b[0m  0.0321\n",
            "     23        \u001b[36m0.7565\u001b[0m  0.0357\n",
            "     24        \u001b[36m0.7552\u001b[0m  0.0377\n",
            "     25        \u001b[36m0.7538\u001b[0m  0.0377\n",
            "     26        \u001b[36m0.7525\u001b[0m  0.0340\n",
            "     27        \u001b[36m0.7512\u001b[0m  0.0391\n",
            "     28        \u001b[36m0.7499\u001b[0m  0.0511\n",
            "     29        \u001b[36m0.7487\u001b[0m  0.0528\n",
            "     30        \u001b[36m0.7475\u001b[0m  0.0553\n",
            "     31        \u001b[36m0.7462\u001b[0m  0.0547\n",
            "     32        \u001b[36m0.7450\u001b[0m  0.0519\n",
            "     33        \u001b[36m0.7439\u001b[0m  0.0639\n",
            "     34        \u001b[36m0.7427\u001b[0m  0.0497\n",
            "     35        \u001b[36m0.7415\u001b[0m  0.0350\n",
            "     36        \u001b[36m0.7404\u001b[0m  0.0339\n",
            "     37        \u001b[36m0.7393\u001b[0m  0.0342\n",
            "     38        \u001b[36m0.7381\u001b[0m  0.0345\n",
            "     39        \u001b[36m0.7370\u001b[0m  0.0379\n",
            "     40        \u001b[36m0.7359\u001b[0m  0.0331\n",
            "     41        \u001b[36m0.7348\u001b[0m  0.0349\n",
            "     42        \u001b[36m0.7338\u001b[0m  0.0388\n",
            "     43        \u001b[36m0.7327\u001b[0m  0.0340\n",
            "     44        \u001b[36m0.7316\u001b[0m  0.0346\n",
            "     45        \u001b[36m0.7306\u001b[0m  0.0402\n",
            "     46        \u001b[36m0.7296\u001b[0m  0.0517\n",
            "     47        \u001b[36m0.7285\u001b[0m  0.0575\n",
            "     48        \u001b[36m0.7275\u001b[0m  0.0813\n",
            "     49        \u001b[36m0.7265\u001b[0m  0.0342\n",
            "     50        \u001b[36m0.7255\u001b[0m  0.0360\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.6059\u001b[0m  0.0334\n",
            "      2        \u001b[36m1.5305\u001b[0m  0.0327\n",
            "      3        \u001b[36m1.4587\u001b[0m  0.0318\n",
            "      4        \u001b[36m1.3903\u001b[0m  0.0531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m1.3254\u001b[0m  0.0739\n",
            "      6        \u001b[36m1.2638\u001b[0m  0.0687\n",
            "      7        \u001b[36m1.2056\u001b[0m  0.0533\n",
            "      8        \u001b[36m1.1509\u001b[0m  0.0698\n",
            "      9        \u001b[36m1.0998\u001b[0m  0.0609\n",
            "     10        \u001b[36m1.0522\u001b[0m  0.0335\n",
            "     11        \u001b[36m1.0082\u001b[0m  0.0334\n",
            "     12        \u001b[36m0.9677\u001b[0m  0.0336\n",
            "     13        \u001b[36m0.9306\u001b[0m  0.0355\n",
            "     14        \u001b[36m0.8967\u001b[0m  0.0334\n",
            "     15        \u001b[36m0.8658\u001b[0m  0.0332\n",
            "     16        \u001b[36m0.8376\u001b[0m  0.0289\n",
            "     17        \u001b[36m0.8119\u001b[0m  0.0325\n",
            "     18        \u001b[36m0.7884\u001b[0m  0.0312\n",
            "     19        \u001b[36m0.7670\u001b[0m  0.0363\n",
            "     20        \u001b[36m0.7474\u001b[0m  0.0358\n",
            "     21        \u001b[36m0.7295\u001b[0m  0.0334\n",
            "     22        \u001b[36m0.7131\u001b[0m  0.0321\n",
            "     23        \u001b[36m0.6982\u001b[0m  0.0327\n",
            "     24        \u001b[36m0.6846\u001b[0m  0.0346\n",
            "     25        \u001b[36m0.6724\u001b[0m  0.0317\n",
            "     26        \u001b[36m0.6615\u001b[0m  0.0330\n",
            "     27        \u001b[36m0.6518\u001b[0m  0.0341\n",
            "     28        \u001b[36m0.6433\u001b[0m  0.0309\n",
            "     29        \u001b[36m0.6359\u001b[0m  0.0443\n",
            "     30        \u001b[36m0.6294\u001b[0m  0.0436\n",
            "     31        \u001b[36m0.6239\u001b[0m  0.0321\n",
            "     32        \u001b[36m0.6191\u001b[0m  0.0307\n",
            "     33        \u001b[36m0.6150\u001b[0m  0.0416\n",
            "     34        \u001b[36m0.6115\u001b[0m  0.0365\n",
            "     35        \u001b[36m0.6085\u001b[0m  0.0535\n",
            "     36        \u001b[36m0.6059\u001b[0m  0.0323\n",
            "     37        \u001b[36m0.6037\u001b[0m  0.0558\n",
            "     38        \u001b[36m0.6018\u001b[0m  0.0553\n",
            "     39        \u001b[36m0.6001\u001b[0m  0.0510\n",
            "     40        \u001b[36m0.5986\u001b[0m  0.0539\n",
            "     41        \u001b[36m0.5973\u001b[0m  0.0521\n",
            "     42        \u001b[36m0.5961\u001b[0m  0.0479\n",
            "     43        \u001b[36m0.5950\u001b[0m  0.0434\n",
            "     44        \u001b[36m0.5940\u001b[0m  0.0359\n",
            "     45        \u001b[36m0.5930\u001b[0m  0.0387\n",
            "     46        \u001b[36m0.5851\u001b[0m  0.0374\n",
            "     47        0.5867  0.0357\n",
            "     48        0.5856  0.0411\n",
            "     49        \u001b[36m0.5846\u001b[0m  0.0428\n",
            "     50        \u001b[36m0.5837\u001b[0m  0.0446\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0493\n",
            "      2       37.3239  0.0538\n",
            "      3       37.3239  0.0453\n",
            "      4       37.3239  0.0433\n",
            "      5       37.3239  0.0422\n",
            "      6       37.3239  0.0461\n",
            "      7       37.3239  0.0429\n",
            "      8       37.3239  0.0437\n",
            "      9       37.3239  0.0449\n",
            "     10       37.3239  0.0440\n",
            "     11       37.3239  0.0425\n",
            "     12       37.3239  0.0434\n",
            "     13       37.3239  0.0464\n",
            "     14       37.3239  0.0499\n",
            "     15       37.3239  0.0471\n",
            "     16       37.3239  0.0433\n",
            "     17       37.3239  0.0579\n",
            "     18       37.3239  0.0494\n",
            "     19       37.3239  0.0477\n",
            "     20       37.3239  0.0462\n",
            "     21       37.3239  0.0637\n",
            "     22       37.3239  0.0455\n",
            "     23       37.3239  0.0473\n",
            "     24       37.3239  0.0616\n",
            "     25       37.3239  0.0511\n",
            "     26       37.3239  0.0420\n",
            "     27       37.3239  0.0419\n",
            "     28       37.3239  0.0408\n",
            "     29       37.3239  0.0412\n",
            "     30       37.3239  0.0494\n",
            "     31       37.3239  0.0632\n",
            "     32       37.3239  0.1055\n",
            "     33       37.3239  0.0847\n",
            "     34       37.3239  0.0820\n",
            "     35       37.3239  0.0701\n",
            "     36       37.3239  0.0495\n",
            "     37       37.3239  0.0554\n",
            "     38       37.3239  0.0517\n",
            "     39       37.3239  0.0536\n",
            "     40       37.3239  0.0432\n",
            "     41       37.3239  0.0424\n",
            "     42       37.3239  0.0417\n",
            "     43       37.3239  0.0423\n",
            "     44       37.3239  0.0451\n",
            "     45       37.3239  0.0394\n",
            "     46       37.3239  0.0435\n",
            "     47       37.3239  0.0428\n",
            "     48       37.3239  0.0432\n",
            "     49       37.3239  0.0452\n",
            "     50       37.3239  0.0456\n",
            "     51       37.3239  0.0439\n",
            "     52       37.3239  0.0458\n",
            "     53       37.3239  0.0491\n",
            "     54       37.3239  0.0479\n",
            "     55       37.3239  0.0465\n",
            "     56       37.3239  0.0636\n",
            "     57       37.3239  0.0450\n",
            "     58       37.3239  0.0520\n",
            "     59       37.3239  0.0427\n",
            "     60       37.3239  0.0427\n",
            "     61       37.3239  0.0414\n",
            "     62       37.3239  0.0508\n",
            "     63       37.3239  0.0624\n",
            "     64       37.3239  0.0502\n",
            "     65       37.3239  0.2163\n",
            "     66       \u001b[36m24.9064\u001b[0m  0.2290\n",
            "     67        \u001b[36m0.6312\u001b[0m  0.0872\n",
            "     68        \u001b[36m0.6221\u001b[0m  0.1770\n",
            "     69        \u001b[36m0.6204\u001b[0m  0.0759\n",
            "     70        \u001b[36m0.6193\u001b[0m  0.0865\n",
            "     71        \u001b[36m0.6185\u001b[0m  0.0684\n",
            "     72        \u001b[36m0.6178\u001b[0m  0.1277\n",
            "     73        \u001b[36m0.6173\u001b[0m  0.1742\n",
            "     74        \u001b[36m0.6168\u001b[0m  0.0454\n",
            "     75        \u001b[36m0.6164\u001b[0m  0.0454\n",
            "     76        \u001b[36m0.6161\u001b[0m  0.0494\n",
            "     77        \u001b[36m0.6157\u001b[0m  0.0598\n",
            "     78        \u001b[36m0.6155\u001b[0m  0.0474\n",
            "     79        \u001b[36m0.6152\u001b[0m  0.0555\n",
            "     80        \u001b[36m0.6150\u001b[0m  0.0607\n",
            "     81        \u001b[36m0.6148\u001b[0m  0.0438\n",
            "     82        \u001b[36m0.6146\u001b[0m  0.0433\n",
            "     83        \u001b[36m0.6145\u001b[0m  0.0437\n",
            "     84        \u001b[36m0.6143\u001b[0m  0.0412\n",
            "     85        \u001b[36m0.6142\u001b[0m  0.0463\n",
            "     86        \u001b[36m0.6141\u001b[0m  0.0496\n",
            "     87        \u001b[36m0.6139\u001b[0m  0.0483\n",
            "     88        \u001b[36m0.6138\u001b[0m  0.0471\n",
            "     89        \u001b[36m0.6137\u001b[0m  0.0463\n",
            "     90        \u001b[36m0.6136\u001b[0m  0.0509\n",
            "     91        \u001b[36m0.6136\u001b[0m  0.0464\n",
            "     92        \u001b[36m0.6135\u001b[0m  0.0518\n",
            "     93        \u001b[36m0.6134\u001b[0m  0.0530\n",
            "     94        \u001b[36m0.6133\u001b[0m  0.0549\n",
            "     95        \u001b[36m0.6133\u001b[0m  0.0608\n",
            "     96        \u001b[36m0.6132\u001b[0m  0.0604\n",
            "     97        \u001b[36m0.6131\u001b[0m  0.0463\n",
            "     98        \u001b[36m0.6131\u001b[0m  0.0521\n",
            "     99        \u001b[36m0.6130\u001b[0m  0.0454\n",
            "    100        \u001b[36m0.6130\u001b[0m  0.0546\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0383\n",
            "      2       37.1930  0.0501\n",
            "      3       37.1930  0.0533\n",
            "      4       37.1930  0.0493\n",
            "      5       37.1930  0.0448\n",
            "      6       37.1930  0.0444\n",
            "      7       37.1930  0.0439\n",
            "      8       37.1930  0.0473\n",
            "      9       37.1930  0.0431\n",
            "     10       37.1930  0.0426\n",
            "     11       37.1930  0.0482\n",
            "     12       37.1930  0.0481\n",
            "     13       37.1930  0.0489\n",
            "     14       37.1930  0.0493\n",
            "     15       37.1930  0.0483\n",
            "     16       37.1930  0.0448\n",
            "     17       37.1930  0.0450\n",
            "     18       37.1930  0.0441\n",
            "     19       37.1930  0.0430\n",
            "     20       37.1930  0.0496\n",
            "     21       37.1930  0.0509\n",
            "     22       37.1930  0.0448\n",
            "     23       37.1930  0.0559\n",
            "     24       37.1930  0.0437\n",
            "     25       37.1930  0.0483\n",
            "     26       37.1930  0.0482\n",
            "     27       37.1930  0.0429\n",
            "     28       37.1930  0.0463\n",
            "     29       37.1930  0.0447\n",
            "     30       37.1930  0.0448\n",
            "     31       37.1930  0.0646\n",
            "     32       37.1930  0.0673\n",
            "     33       37.1930  0.0442\n",
            "     34       37.1930  0.0474\n",
            "     35       \u001b[36m36.5959\u001b[0m  0.0464\n",
            "     36       \u001b[36m13.0734\u001b[0m  0.0544\n",
            "     37        \u001b[36m0.7128\u001b[0m  0.0791\n",
            "     38        \u001b[36m0.6971\u001b[0m  0.1191\n",
            "     39        \u001b[36m0.5340\u001b[0m  0.0697\n",
            "     40        \u001b[36m0.5280\u001b[0m  0.0734\n",
            "     41        \u001b[36m0.5100\u001b[0m  0.0792\n",
            "     42        \u001b[36m0.4849\u001b[0m  0.1303\n",
            "     43        \u001b[36m0.4700\u001b[0m  0.0821\n",
            "     44        \u001b[36m0.4552\u001b[0m  0.0840\n",
            "     45        \u001b[36m0.4465\u001b[0m  0.0540\n",
            "     46        \u001b[36m0.4356\u001b[0m  0.0509\n",
            "     47        \u001b[36m0.4262\u001b[0m  0.0507\n",
            "     48        \u001b[36m0.4173\u001b[0m  0.0488\n",
            "     49        \u001b[36m0.4081\u001b[0m  0.0449\n",
            "     50        \u001b[36m0.3999\u001b[0m  0.0437\n",
            "     51        \u001b[36m0.3926\u001b[0m  0.0440\n",
            "     52        \u001b[36m0.3887\u001b[0m  0.0439\n",
            "     53        \u001b[36m0.3820\u001b[0m  0.0517\n",
            "     54        \u001b[36m0.3764\u001b[0m  0.0416\n",
            "     55        \u001b[36m0.3720\u001b[0m  0.0473\n",
            "     56        \u001b[36m0.3660\u001b[0m  0.0416\n",
            "     57        \u001b[36m0.3612\u001b[0m  0.0418\n",
            "     58        \u001b[36m0.3568\u001b[0m  0.0430\n",
            "     59        \u001b[36m0.3523\u001b[0m  0.0414\n",
            "     60        \u001b[36m0.3476\u001b[0m  0.0469\n",
            "     61        \u001b[36m0.3434\u001b[0m  0.0716\n",
            "     62        \u001b[36m0.3396\u001b[0m  0.0443\n",
            "     63        \u001b[36m0.3348\u001b[0m  0.0499\n",
            "     64        \u001b[36m0.3303\u001b[0m  0.0567\n",
            "     65        \u001b[36m0.3248\u001b[0m  0.0505\n",
            "     66        \u001b[36m0.3222\u001b[0m  0.0492\n",
            "     67        \u001b[36m0.3190\u001b[0m  0.0547\n",
            "     68        \u001b[36m0.3131\u001b[0m  0.0525\n",
            "     69        \u001b[36m0.3114\u001b[0m  0.0536\n",
            "     70        \u001b[36m0.3088\u001b[0m  0.0510\n",
            "     71        \u001b[36m0.3014\u001b[0m  0.0417\n",
            "     72        \u001b[36m0.2982\u001b[0m  0.0515\n",
            "     73        \u001b[36m0.2947\u001b[0m  0.0488\n",
            "     74        \u001b[36m0.2941\u001b[0m  0.0426\n",
            "     75        \u001b[36m0.2904\u001b[0m  0.0431\n",
            "     76        \u001b[36m0.2802\u001b[0m  0.0512\n",
            "     77        \u001b[36m0.2802\u001b[0m  0.0457\n",
            "     78        \u001b[36m0.2779\u001b[0m  0.0426\n",
            "     79        \u001b[36m0.2735\u001b[0m  0.0462\n",
            "     80        \u001b[36m0.2680\u001b[0m  0.0452\n",
            "     81        \u001b[36m0.2656\u001b[0m  0.0444\n",
            "     82        \u001b[36m0.2606\u001b[0m  0.0411\n",
            "     83        \u001b[36m0.2588\u001b[0m  0.0453\n",
            "     84        \u001b[36m0.2538\u001b[0m  0.0475\n",
            "     85        0.2545  0.0504\n",
            "     86        \u001b[36m0.2483\u001b[0m  0.0544\n",
            "     87        \u001b[36m0.2480\u001b[0m  0.0480\n",
            "     88        \u001b[36m0.2455\u001b[0m  0.0482\n",
            "     89        \u001b[36m0.2434\u001b[0m  0.0440\n",
            "     90        \u001b[36m0.2403\u001b[0m  0.0425\n",
            "     91        \u001b[36m0.2384\u001b[0m  0.0516\n",
            "     92        \u001b[36m0.2345\u001b[0m  0.0413\n",
            "     93        0.2356  0.0431\n",
            "     94        \u001b[36m0.2301\u001b[0m  0.0508\n",
            "     95        0.2308  0.0406\n",
            "     96        \u001b[36m0.2236\u001b[0m  0.0421\n",
            "     97        0.2290  0.0437\n",
            "     98        \u001b[36m0.2219\u001b[0m  0.0433\n",
            "     99        0.2249  0.0498\n",
            "    100        \u001b[36m0.2189\u001b[0m  0.0454\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0279\n",
            "      2       37.3239  0.0358\n",
            "      3       37.3239  0.0322\n",
            "      4       37.3239  0.0335\n",
            "      5       37.3239  0.0358\n",
            "      6       37.3239  0.0352\n",
            "      7       37.3239  0.0368\n",
            "      8       37.3239  0.0359\n",
            "      9       37.3239  0.0354\n",
            "     10       37.3239  0.0348\n",
            "     11       37.3239  0.0341\n",
            "     12       37.3239  0.0330\n",
            "     13       37.3239  0.0298\n",
            "     14       37.3239  0.0320\n",
            "     15       37.3239  0.0327\n",
            "     16       37.3239  0.0381\n",
            "     17       37.3239  0.0298\n",
            "     18       37.3239  0.0330\n",
            "     19       37.3239  0.0362\n",
            "     20       37.3239  0.0378\n",
            "     21       37.3239  0.0346\n",
            "     22       37.3239  0.0321\n",
            "     23       37.3239  0.0328\n",
            "     24       37.3239  0.0291\n",
            "     25       37.3239  0.0306\n",
            "     26       37.3239  0.0374\n",
            "     27       37.3239  0.0369\n",
            "     28       37.3239  0.0334\n",
            "     29       37.3239  0.0340\n",
            "     30       37.3239  0.0327\n",
            "     31       37.3239  0.0322\n",
            "     32       37.3239  0.0349\n",
            "     33       37.3239  0.0426\n",
            "     34       37.3239  0.0339\n",
            "     35       37.3239  0.0342\n",
            "     36       37.3239  0.0389\n",
            "     37       37.3239  0.0339\n",
            "     38       37.3239  0.0330\n",
            "     39       37.3239  0.0331\n",
            "     40       37.3239  0.0345\n",
            "     41       37.3239  0.0346\n",
            "     42       37.3239  0.0328\n",
            "     43       37.3239  0.0304\n",
            "     44       37.3239  0.0325\n",
            "     45       37.3239  0.0439\n",
            "     46       37.3239  0.0377\n",
            "     47       37.3239  0.0413\n",
            "     48       37.3239  0.0365\n",
            "     49       37.3239  0.0310\n",
            "     50       37.3239  0.0328\n",
            "     51       37.3239  0.0332\n",
            "     52       37.3239  0.0308\n",
            "     53       37.3239  0.0365\n",
            "     54       37.3239  0.0362\n",
            "     55       37.3239  0.0331\n",
            "     56       37.3239  0.0354\n",
            "     57       37.3239  0.0406\n",
            "     58       37.3239  0.0365\n",
            "     59       37.3239  0.0373\n",
            "     60       37.3239  0.0414\n",
            "     61       37.3239  0.0382\n",
            "     62       37.3239  0.0341\n",
            "     63       37.3239  0.0341\n",
            "     64       37.3239  0.0425\n",
            "     65       37.3239  0.0323\n",
            "     66       37.3239  0.0327\n",
            "     67       37.3239  0.0324\n",
            "     68       37.3239  0.0320\n",
            "     69       37.3239  0.0355\n",
            "     70       37.3239  0.0322\n",
            "     71       37.3239  0.0322\n",
            "     72       37.3239  0.0342\n",
            "     73       37.3239  0.0332\n",
            "     74       37.3239  0.0394\n",
            "     75       37.3239  0.0346\n",
            "     76       37.3239  0.0341\n",
            "     77       37.3239  0.0340\n",
            "     78       37.3239  0.0324\n",
            "     79       37.3239  0.0370\n",
            "     80       37.3239  0.0367\n",
            "     81       37.3239  0.0326\n",
            "     82       37.3239  0.0315\n",
            "     83       37.3239  0.0319\n",
            "     84       37.3239  0.0351\n",
            "     85       37.3239  0.0379\n",
            "     86       37.3239  0.0371\n",
            "     87       37.3239  0.0345\n",
            "     88       37.3239  0.0376\n",
            "     89       37.3239  0.0349\n",
            "     90       37.3239  0.0394\n",
            "     91       37.3239  0.0348\n",
            "     92       37.3239  0.0325\n",
            "     93       37.3239  0.0327\n",
            "     94       37.3239  0.0303\n",
            "     95       37.3239  0.0297\n",
            "     96       37.3239  0.0358\n",
            "     97       37.3239  0.0332\n",
            "     98       37.3239  0.0315\n",
            "     99       37.3239  0.0344\n",
            "    100       37.3239  0.0329\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0448\n",
            "      2       37.1930  0.0326\n",
            "      3       37.1930  0.0354\n",
            "      4       37.1930  0.0340\n",
            "      5       37.1930  0.0335\n",
            "      6       37.1930  0.0343\n",
            "      7       37.1930  0.0347\n",
            "      8       37.1930  0.0331\n",
            "      9       37.1930  0.0333\n",
            "     10       37.1930  0.0325\n",
            "     11       37.1930  0.0321\n",
            "     12       37.1930  0.0326\n",
            "     13       37.1930  0.0354\n",
            "     14       37.1930  0.0415\n",
            "     15       37.1930  0.0364\n",
            "     16       37.1930  0.0333\n",
            "     17       37.1930  0.0380\n",
            "     18       37.1930  0.0357\n",
            "     19       37.1930  0.0337\n",
            "     20       37.1930  0.0351\n",
            "     21       37.1930  0.0340\n",
            "     22       37.1930  0.0329\n",
            "     23       37.1930  0.0291\n",
            "     24       37.1930  0.0371\n",
            "     25       37.1930  0.0388\n",
            "     26       37.1930  0.0357\n",
            "     27       37.1930  0.0337\n",
            "     28       37.1930  0.0332\n",
            "     29       37.1930  0.0380\n",
            "     30       37.1930  0.0327\n",
            "     31       37.1930  0.0370\n",
            "     32       37.1930  0.0323\n",
            "     33       37.1930  0.0349\n",
            "     34       37.1930  0.0311\n",
            "     35       37.1930  0.0324\n",
            "     36       37.1930  0.0325\n",
            "     37       37.1930  0.0339\n",
            "     38       37.1930  0.0372\n",
            "     39       37.1930  0.0353\n",
            "     40       37.1930  0.0342\n",
            "     41       37.1930  0.0422\n",
            "     42       37.1930  0.0390\n",
            "     43       37.1930  0.0415\n",
            "     44       37.1930  0.0338\n",
            "     45       37.1930  0.0356\n",
            "     46       37.1930  0.0395\n",
            "     47       37.1930  0.0370\n",
            "     48       37.1930  0.0330\n",
            "     49       37.1930  0.0436\n",
            "     50       37.1930  0.0354\n",
            "     51       37.1930  0.0374\n",
            "     52       37.1930  0.0346\n",
            "     53       37.1930  0.0311\n",
            "     54       37.1930  0.0355\n",
            "     55       37.1930  0.0412\n",
            "     56       37.1930  0.0341\n",
            "     57       37.1930  0.0318\n",
            "     58       37.1930  0.0328\n",
            "     59       37.1930  0.0408\n",
            "     60       37.1930  0.0354\n",
            "     61       37.1930  0.0324\n",
            "     62       37.1930  0.0372\n",
            "     63       37.1930  0.0347\n",
            "     64       37.1930  0.0321\n",
            "     65       37.1930  0.0370\n",
            "     66       37.1930  0.0352\n",
            "     67       37.1930  0.0350\n",
            "     68       37.1930  0.0313\n",
            "     69       37.1930  0.0370\n",
            "     70       37.1930  0.0371\n",
            "     71       37.1930  0.0342\n",
            "     72       37.1930  0.0426\n",
            "     73       37.1930  0.0344\n",
            "     74       37.1930  0.0317\n",
            "     75       37.1930  0.0337\n",
            "     76       37.1930  0.0337\n",
            "     77       37.1930  0.0331\n",
            "     78       37.1930  0.0361\n",
            "     79       37.1930  0.0357\n",
            "     80       37.1930  0.0310\n",
            "     81       37.1930  0.0320\n",
            "     82       37.1930  0.0364\n",
            "     83       37.1930  0.0346\n",
            "     84       37.1930  0.0389\n",
            "     85       37.1930  0.0350\n",
            "     86       37.1930  0.0356\n",
            "     87       37.1930  0.0341\n",
            "     88       37.1930  0.0374\n",
            "     89       37.1930  0.0304\n",
            "     90       37.1930  0.0333\n",
            "     91       37.1930  0.0390\n",
            "     92       37.1930  0.0390\n",
            "     93       37.1930  0.0317\n",
            "     94       37.1930  0.0359\n",
            "     95       37.1930  0.0369\n",
            "     96       37.1930  0.0335\n",
            "     97       37.1930  0.0377\n",
            "     98       37.1930  0.0342\n",
            "     99       37.1930  0.0354\n",
            "    100       37.1930  0.0333\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0417\n",
            "      2       37.3239  0.0445\n",
            "      3       37.3239  0.0492\n",
            "      4       37.3239  0.0502\n",
            "      5       37.3239  0.0456\n",
            "      6       37.3239  0.0443\n",
            "      7       37.3239  0.0544\n",
            "      8       37.3239  0.0463\n",
            "      9       37.3239  0.0506\n",
            "     10       37.3239  0.0504\n",
            "     11       37.3239  0.0471\n",
            "     12       37.3239  0.0436\n",
            "     13       37.3239  0.0431\n",
            "     14       37.3239  0.0422\n",
            "     15       37.3239  0.0464\n",
            "     16       37.3239  0.0508\n",
            "     17       37.3239  0.0548\n",
            "     18       37.3239  0.0605\n",
            "     19       37.3239  0.0660\n",
            "     20       37.3239  0.0540\n",
            "     21       37.3239  0.0438\n",
            "     22       37.3239  0.0431\n",
            "     23       37.3239  0.0454\n",
            "     24       37.3239  0.0420\n",
            "     25       37.3239  0.0415\n",
            "     26       37.3239  0.0463\n",
            "     27       37.3239  0.0514\n",
            "     28       37.3239  0.0464\n",
            "     29       37.3239  0.0494\n",
            "     30       37.3239  0.0753\n",
            "     31       37.3239  0.0553\n",
            "     32       37.3239  0.0553\n",
            "     33       37.3239  0.0540\n",
            "     34       37.3239  0.0470\n",
            "     35       37.3239  0.0466\n",
            "     36       37.3239  0.0436\n",
            "     37       37.3239  0.0430\n",
            "     38       37.3239  0.0470\n",
            "     39       37.3239  0.0436\n",
            "     40       37.3239  0.0440\n",
            "     41       37.3239  0.0427\n",
            "     42       37.3239  0.0441\n",
            "     43       37.3239  0.0438\n",
            "     44       37.3239  0.0433\n",
            "     45       37.3239  0.0466\n",
            "     46       37.3239  0.0440\n",
            "     47       37.3239  0.0552\n",
            "     48       37.3239  0.0437\n",
            "     49       37.3239  0.0441\n",
            "     50       37.3239  0.0496\n",
            "     51       37.3239  0.0560\n",
            "     52       37.3239  0.0532\n",
            "     53       37.3239  0.0461\n",
            "     54       37.3239  0.0480\n",
            "     55       37.3239  0.0467\n",
            "     56       37.3239  0.0469\n",
            "     57       37.3239  0.0482\n",
            "     58       37.3239  0.0419\n",
            "     59       37.3239  0.0431\n",
            "     60       37.3239  0.0422\n",
            "     61       37.3239  0.0417\n",
            "     62       37.3239  0.0474\n",
            "     63       37.3239  0.0483\n",
            "     64       37.3239  0.0453\n",
            "     65       37.3239  0.0444\n",
            "     66       37.3239  0.0479\n",
            "     67       37.3239  0.0476\n",
            "     68       37.3239  0.0533\n",
            "     69       37.3239  0.0464\n",
            "     70       37.3239  0.0479\n",
            "     71       37.3239  0.0517\n",
            "     72       37.3239  0.0451\n",
            "     73       37.3239  0.0494\n",
            "     74       37.3239  0.0494\n",
            "     75       37.3239  0.0524\n",
            "     76       37.3239  0.0499\n",
            "     77       37.3239  0.0561\n",
            "     78       37.3239  0.0441\n",
            "     79       37.3239  0.0430\n",
            "     80       37.3239  0.0523\n",
            "     81       37.3239  0.0402\n",
            "     82       37.3239  0.0461\n",
            "     83       37.3239  0.0432\n",
            "     84       37.3239  0.0425\n",
            "     85       37.3239  0.0431\n",
            "     86       37.3239  0.0457\n",
            "     87       37.3239  0.0446\n",
            "     88       37.3239  0.0544\n",
            "     89       37.3239  0.0443\n",
            "     90       37.3239  0.0412\n",
            "     91       37.3239  0.0586\n",
            "     92       37.3239  0.0503\n",
            "     93       37.3239  0.0564\n",
            "     94       37.3239  0.0484\n",
            "     95       37.3239  0.0499\n",
            "     96       37.3239  0.0494\n",
            "     97       37.3239  0.0502\n",
            "     98       37.3239  0.0510\n",
            "     99       37.3239  0.0447\n",
            "    100       37.3239  0.0454\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0391\n",
            "      2       37.1930  0.0459\n",
            "      3       37.1930  0.0437\n",
            "      4       37.1930  0.0408\n",
            "      5       37.1930  0.0416\n",
            "      6       37.1930  0.0515\n",
            "      7       37.1930  0.0429\n",
            "      8       37.1930  0.0519\n",
            "      9       37.1930  0.0474\n",
            "     10       37.1930  0.0456\n",
            "     11       37.1930  0.0492\n",
            "     12       37.1930  0.0472\n",
            "     13       37.1930  0.0485\n",
            "     14       37.1930  0.0504\n",
            "     15       37.1930  0.0462\n",
            "     16       37.1930  0.0576\n",
            "     17       37.1930  0.0470\n",
            "     18       37.1930  0.0401\n",
            "     19       37.1930  0.0401\n",
            "     20       37.1930  0.0403\n",
            "     21       37.1930  0.0432\n",
            "     22       37.1930  0.0455\n",
            "     23       37.1930  0.0462\n",
            "     24       37.1930  0.0456\n",
            "     25       37.1930  0.0438\n",
            "     26       37.1930  0.0459\n",
            "     27       37.1930  0.0454\n",
            "     28       37.1930  0.0420\n",
            "     29       37.1930  0.0506\n",
            "     30       37.1930  0.0436\n",
            "     31       37.1930  0.0437\n",
            "     32       37.1930  0.0467\n",
            "     33       37.1930  0.0476\n",
            "     34       37.1930  0.0470\n",
            "     35       37.1930  0.0449\n",
            "     36       37.1930  0.0485\n",
            "     37       37.1930  0.0446\n",
            "     38       37.1930  0.0430\n",
            "     39       37.1930  0.0497\n",
            "     40       37.1930  0.0409\n",
            "     41       37.1930  0.0484\n",
            "     42       37.1930  0.0427\n",
            "     43       \u001b[36m27.9454\u001b[0m  0.0421\n",
            "     44        \u001b[36m1.3476\u001b[0m  0.0398\n",
            "     45        \u001b[36m0.6329\u001b[0m  0.0443\n",
            "     46        \u001b[36m0.6004\u001b[0m  0.0438\n",
            "     47        \u001b[36m0.5912\u001b[0m  0.0438\n",
            "     48        \u001b[36m0.5793\u001b[0m  0.0493\n",
            "     49        \u001b[36m0.5666\u001b[0m  0.0437\n",
            "     50        0.5775  0.0539\n",
            "     51        \u001b[36m0.5545\u001b[0m  0.0506\n",
            "     52        \u001b[36m0.5385\u001b[0m  0.0471\n",
            "     53        0.5714  0.0471\n",
            "     54        \u001b[36m0.5245\u001b[0m  0.0483\n",
            "     55        0.5355  0.0477\n",
            "     56        \u001b[36m0.5089\u001b[0m  0.0439\n",
            "     57        \u001b[36m0.5063\u001b[0m  0.0468\n",
            "     58        \u001b[36m0.4911\u001b[0m  0.0475\n",
            "     59        \u001b[36m0.4747\u001b[0m  0.0510\n",
            "     60        \u001b[36m0.4615\u001b[0m  0.0416\n",
            "     61        \u001b[36m0.4517\u001b[0m  0.0446\n",
            "     62        \u001b[36m0.4419\u001b[0m  0.0435\n",
            "     63        \u001b[36m0.4333\u001b[0m  0.0434\n",
            "     64        \u001b[36m0.4234\u001b[0m  0.0424\n",
            "     65        \u001b[36m0.4094\u001b[0m  0.0426\n",
            "     66        \u001b[36m0.3919\u001b[0m  0.0430\n",
            "     67        0.3937  0.0465\n",
            "     68        \u001b[36m0.3771\u001b[0m  0.0432\n",
            "     69        \u001b[36m0.3646\u001b[0m  0.0447\n",
            "     70        \u001b[36m0.3585\u001b[0m  0.0446\n",
            "     71        \u001b[36m0.3462\u001b[0m  0.0575\n",
            "     72        \u001b[36m0.3444\u001b[0m  0.0461\n",
            "     73        \u001b[36m0.3081\u001b[0m  0.0511\n",
            "     74        0.3518  0.0563\n",
            "     75        0.3795  0.0539\n",
            "     76        0.3349  0.0479\n",
            "     77        \u001b[36m0.2984\u001b[0m  0.0463\n",
            "     78        0.3009  0.0468\n",
            "     79        \u001b[36m0.2937\u001b[0m  0.0410\n",
            "     80        \u001b[36m0.2896\u001b[0m  0.0431\n",
            "     81        0.2909  0.0422\n",
            "     82        \u001b[36m0.2753\u001b[0m  0.0432\n",
            "     83        \u001b[36m0.2666\u001b[0m  0.0452\n",
            "     84        \u001b[36m0.2554\u001b[0m  0.0545\n",
            "     85        0.2589  0.0438\n",
            "     86        \u001b[36m0.2553\u001b[0m  0.0435\n",
            "     87        \u001b[36m0.2422\u001b[0m  0.0422\n",
            "     88        0.2446  0.0427\n",
            "     89        0.2484  0.0489\n",
            "     90        \u001b[36m0.2337\u001b[0m  0.0429\n",
            "     91        \u001b[36m0.2244\u001b[0m  0.0433\n",
            "     92        0.2398  0.0627\n",
            "     93        0.2302  0.0461\n",
            "     94        \u001b[36m0.2160\u001b[0m  0.0471\n",
            "     95        0.2204  0.0492\n",
            "     96        0.2164  0.0492\n",
            "     97        0.2199  0.0450\n",
            "     98        \u001b[36m0.2123\u001b[0m  0.0437\n",
            "     99        \u001b[36m0.2114\u001b[0m  0.0429\n",
            "    100        0.2124  0.0439\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0286\n",
            "      2       37.3239  0.0368\n",
            "      3       37.3239  0.0303\n",
            "      4       37.3239  0.0361\n",
            "      5       37.3239  0.0376\n",
            "      6       37.3239  0.0330\n",
            "      7       37.3239  0.0334\n",
            "      8       37.3239  0.0330\n",
            "      9       37.3239  0.0326\n",
            "     10       37.3239  0.0304\n",
            "     11       37.3239  0.0320\n",
            "     12       37.3239  0.0356\n",
            "     13       37.3239  0.0353\n",
            "     14       37.3239  0.0340\n",
            "     15       37.3239  0.0346\n",
            "     16       37.3239  0.0368\n",
            "     17       37.3239  0.0447\n",
            "     18       37.3239  0.0465\n",
            "     19       37.3239  0.0387\n",
            "     20       37.3239  0.0385\n",
            "     21       37.3239  0.0331\n",
            "     22       37.3239  0.0345\n",
            "     23       37.3239  0.0375\n",
            "     24       37.3239  0.0311\n",
            "     25       37.3239  0.0324\n",
            "     26       37.3239  0.0372\n",
            "     27       37.3239  0.0382\n",
            "     28       37.3239  0.0376\n",
            "     29       37.3239  0.0339\n",
            "     30       37.3239  0.0288\n",
            "     31       37.3239  0.0335\n",
            "     32       37.3239  0.0331\n",
            "     33       37.3239  0.0330\n",
            "     34       37.3239  0.0326\n",
            "     35       37.3239  0.0321\n",
            "     36       37.3239  0.0313\n",
            "     37       37.3239  0.0328\n",
            "     38       37.3239  0.0469\n",
            "     39       37.3239  0.0403\n",
            "     40       37.3239  0.0341\n",
            "     41       37.3239  0.0375\n",
            "     42       37.3239  0.0357\n",
            "     43       37.3239  0.0483\n",
            "     44       37.3239  0.0413\n",
            "     45       37.3239  0.0396\n",
            "     46       37.3239  0.0389\n",
            "     47       37.3239  0.0364\n",
            "     48       37.3239  0.0397\n",
            "     49       37.3239  0.0388\n",
            "     50       37.3239  0.0362\n",
            "     51       37.3239  0.0328\n",
            "     52       37.3239  0.0349\n",
            "     53       37.3239  0.0365\n",
            "     54       37.3239  0.0342\n",
            "     55       37.3239  0.0341\n",
            "     56       37.3239  0.0333\n",
            "     57       37.3239  0.0373\n",
            "     58       37.3239  0.0306\n",
            "     59       37.3239  0.0341\n",
            "     60       37.3239  0.0384\n",
            "     61       37.3239  0.0332\n",
            "     62       37.3239  0.0332\n",
            "     63       37.3239  0.0365\n",
            "     64       37.3239  0.0329\n",
            "     65       37.3239  0.0321\n",
            "     66       37.3239  0.0324\n",
            "     67       37.3239  0.0363\n",
            "     68       37.3239  0.0317\n",
            "     69       37.3239  0.0331\n",
            "     70       37.3239  0.0486\n",
            "     71       37.3239  0.0391\n",
            "     72       37.3239  0.0446\n",
            "     73       37.3239  0.0384\n",
            "     74       37.3239  0.0399\n",
            "     75       37.3239  0.0359\n",
            "     76       37.3239  0.0362\n",
            "     77       37.3239  0.0322\n",
            "     78       37.3239  0.0373\n",
            "     79       37.3239  0.0307\n",
            "     80       37.3239  0.0404\n",
            "     81       37.3239  0.0320\n",
            "     82       37.3239  0.0328\n",
            "     83       37.3239  0.0349\n",
            "     84       37.3239  0.0311\n",
            "     85       37.3239  0.0320\n",
            "     86       37.3239  0.0327\n",
            "     87       37.3239  0.0362\n",
            "     88       37.3239  0.0345\n",
            "     89       37.3239  0.0379\n",
            "     90       37.3239  0.0338\n",
            "     91       37.3239  0.0431\n",
            "     92       37.3239  0.0331\n",
            "     93       37.3239  0.0359\n",
            "     94       37.3239  0.0400\n",
            "     95       37.3239  0.0358\n",
            "     96       37.3239  0.0521\n",
            "     97       37.3239  0.0342\n",
            "     98       37.3239  0.0423\n",
            "     99       37.3239  0.0368\n",
            "    100       37.3239  0.0353\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0301\n",
            "      2       37.1930  0.0387\n",
            "      3       37.1930  0.0414\n",
            "      4       37.1930  0.0301\n",
            "      5       37.1930  0.0380\n",
            "      6       37.1930  0.0341\n",
            "      7       37.1930  0.0347\n",
            "      8       37.1930  0.0337\n",
            "      9       37.1930  0.0333\n",
            "     10       37.1930  0.0327\n",
            "     11       37.1930  0.0313\n",
            "     12       37.1930  0.0343\n",
            "     13       37.1930  0.0370\n",
            "     14       37.1930  0.0368\n",
            "     15       37.1930  0.0320\n",
            "     16       37.1930  0.0324\n",
            "     17       37.1930  0.0350\n",
            "     18       37.1930  0.0411\n",
            "     19       37.1930  0.0341\n",
            "     20       37.1930  0.0330\n",
            "     21       37.1930  0.0383\n",
            "     22       37.1930  0.0550\n",
            "     23       37.1930  0.0331\n",
            "     24       37.1930  0.0432\n",
            "     25       37.1930  0.0399\n",
            "     26       37.1930  0.0354\n",
            "     27       37.1930  0.0326\n",
            "     28       37.1930  0.0338\n",
            "     29       37.1930  0.0322\n",
            "     30       37.1930  0.0329\n",
            "     31       37.1930  0.0319\n",
            "     32       37.1930  0.0356\n",
            "     33       37.1930  0.0330\n",
            "     34       37.1930  0.0385\n",
            "     35       37.1930  0.0381\n",
            "     36       37.1930  0.0377\n",
            "     37       37.1930  0.0313\n",
            "     38       37.1930  0.0342\n",
            "     39       37.1930  0.0329\n",
            "     40       37.1930  0.0388\n",
            "     41       37.1930  0.0357\n",
            "     42       37.1930  0.0324\n",
            "     43       37.1930  0.0323\n",
            "     44       37.1930  0.0340\n",
            "     45       37.1930  0.0352\n",
            "     46       37.1930  0.0342\n",
            "     47       37.1930  0.0332\n",
            "     48       37.1930  0.0448\n",
            "     49       37.1930  0.0454\n",
            "     50       37.1930  0.0320\n",
            "     51       37.1930  0.0338\n",
            "     52       37.1930  0.0308\n",
            "     53       37.1930  0.0322\n",
            "     54       37.1930  0.0331\n",
            "     55       37.1930  0.0421\n",
            "     56       37.1930  0.0342\n",
            "     57       37.1930  0.0341\n",
            "     58       37.1930  0.0326\n",
            "     59       37.1930  0.0379\n",
            "     60       37.1930  0.0332\n",
            "     61       37.1930  0.0327\n",
            "     62       37.1930  0.0303\n",
            "     63       37.1930  0.0283\n",
            "     64       37.1930  0.0327\n",
            "     65       37.1930  0.0329\n",
            "     66       37.1930  0.0326\n",
            "     67       37.1930  0.0340\n",
            "     68       37.1930  0.0344\n",
            "     69       37.1930  0.0367\n",
            "     70       37.1930  0.0349\n",
            "     71       37.1930  0.0354\n",
            "     72       37.1930  0.0350\n",
            "     73       37.1930  0.0341\n",
            "     74       37.1930  0.0300\n",
            "     75       37.1930  0.0395\n",
            "     76       37.1930  0.0484\n",
            "     77       37.1930  0.0382\n",
            "     78       37.1930  0.0360\n",
            "     79       37.1930  0.0464\n",
            "     80       37.1930  0.0334\n",
            "     81       37.1930  0.0326\n",
            "     82       37.1930  0.0347\n",
            "     83       37.1930  0.0321\n",
            "     84       37.1930  0.0290\n",
            "     85       37.1930  0.0353\n",
            "     86       37.1930  0.0347\n",
            "     87       37.1930  0.0335\n",
            "     88       37.1930  0.0363\n",
            "     89       37.1930  0.0313\n",
            "     90       37.1930  0.0364\n",
            "     91       37.1930  0.0319\n",
            "     92       37.1930  0.0330\n",
            "     93       37.1930  0.0355\n",
            "     94       37.1930  0.0351\n",
            "     95       37.1930  0.0406\n",
            "     96       37.1930  0.0309\n",
            "     97       37.1930  0.0335\n",
            "     98       37.1930  0.0371\n",
            "     99       37.1930  0.0335\n",
            "    100       37.1930  0.0376\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m43.3099\u001b[0m  0.0527\n",
            "      2       43.3099  0.0591\n",
            "      3       \u001b[36m42.8362\u001b[0m  0.0514\n",
            "      4       \u001b[36m42.6056\u001b[0m  0.0552\n",
            "      5       42.6056  0.0428\n",
            "      6       \u001b[36m42.2908\u001b[0m  0.0431\n",
            "      7       \u001b[36m41.9712\u001b[0m  0.0404\n",
            "      8       \u001b[36m41.1972\u001b[0m  0.0407\n",
            "      9       41.1972  0.0409\n",
            "     10       41.1972  0.0439\n",
            "     11       41.1972  0.0435\n",
            "     12       41.1972  0.0436\n",
            "     13       41.1972  0.0440\n",
            "     14       41.1972  0.0453\n",
            "     15       41.1972  0.0478\n",
            "     16       41.4167  0.0464\n",
            "     17       41.9014  0.0417\n",
            "     18       41.9014  0.0431\n",
            "     19       41.9014  0.0430\n",
            "     20       41.9014  0.0417\n",
            "     21       41.9014  0.0463\n",
            "     22       42.2535  0.0440\n",
            "     23       42.2535  0.0582\n",
            "     24       42.2535  0.0484\n",
            "     25       42.2535  0.0534\n",
            "     26       42.2535  0.0428\n",
            "     27       42.6056  0.0421\n",
            "     28       42.6056  0.0439\n",
            "     29       42.6056  0.0400\n",
            "     30       42.6056  0.0453\n",
            "     31       42.6056  0.0449\n",
            "     32       42.6056  0.0455\n",
            "     33       42.6056  0.0423\n",
            "     34       42.6056  0.0442\n",
            "     35       42.6056  0.0457\n",
            "     36       42.6056  0.0516\n",
            "     37       42.6056  0.0427\n",
            "     38       42.6056  0.0452\n",
            "     39       42.6056  0.0422\n",
            "     40       42.6060  0.0462\n",
            "     41       42.6056  0.0433\n",
            "     42       42.5305  0.0525\n",
            "     43       42.5252  0.0486\n",
            "     44       42.5226  0.0536\n",
            "     45       42.5201  0.0449\n",
            "     46       42.5177  0.0448\n",
            "     47       42.5152  0.0441\n",
            "     48       42.5127  0.0511\n",
            "     49       42.5102  0.0434\n",
            "     50       42.5077  0.0411\n",
            "     51       42.5052  0.0481\n",
            "     52       42.5027  0.0438\n",
            "     53       42.5002  0.0433\n",
            "     54       42.4978  0.0433\n",
            "     55       42.4953  0.0435\n",
            "     56       42.4928  0.0438\n",
            "     57       42.4904  0.0438\n",
            "     58       42.4879  0.0414\n",
            "     59       42.4855  0.0467\n",
            "     60       42.4831  0.0444\n",
            "     61       42.4806  0.0424\n",
            "     62       42.4782  0.0478\n",
            "     63       42.4758  0.0535\n",
            "     64       42.4734  0.0508\n",
            "     65       42.4711  0.0457\n",
            "     66       42.4687  0.0488\n",
            "     67       42.4664  0.0576\n",
            "     68       42.4641  0.0419\n",
            "     69       42.4618  0.0422\n",
            "     70       42.4595  0.0417\n",
            "     71       42.4572  0.0407\n",
            "     72       42.4549  0.0433\n",
            "     73       42.4527  0.0442\n",
            "     74       42.4505  0.0449\n",
            "     75       42.4483  0.0450\n",
            "     76       42.4461  0.0521\n",
            "     77       42.4440  0.0465\n",
            "     78       42.4418  0.0426\n",
            "     79       42.4397  0.0425\n",
            "     80       42.4376  0.0426\n",
            "     81       42.4356  0.0456\n",
            "     82       42.4335  0.0444\n",
            "     83       42.4315  0.0541\n",
            "     84       42.4295  0.0444\n",
            "     85       42.4276  0.0638\n",
            "     86       42.4256  0.0476\n",
            "     87       42.4237  0.0435\n",
            "     88       42.4218  0.0408\n",
            "     89       42.4199  0.0459\n",
            "     90       42.4181  0.0434\n",
            "     91       42.4163  0.0441\n",
            "     92       42.4145  0.0474\n",
            "     93       42.4127  0.0500\n",
            "     94       42.4109  0.0432\n",
            "     95       42.4092  0.0414\n",
            "     96       42.4075  0.0444\n",
            "     97       42.4058  0.0471\n",
            "     98       42.4042  0.0474\n",
            "     99       42.4026  0.0439\n",
            "    100       42.4010  0.0433\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0422\n",
            "      2       37.1930  0.0502\n",
            "      3       37.1930  0.0548\n",
            "      4       37.1930  0.0520\n",
            "      5       37.1930  0.0488\n",
            "      6       37.1930  0.0476\n",
            "      7       37.1930  0.0548\n",
            "      8       37.1930  0.0429\n",
            "      9       37.1930  0.0461\n",
            "     10       37.1930  0.0493\n",
            "     11       37.1930  0.0440\n",
            "     12       37.1930  0.0407\n",
            "     13       37.1930  0.0431\n",
            "     14       37.1930  0.0435\n",
            "     15       37.1930  0.0442\n",
            "     16       37.1930  0.0464\n",
            "     17       37.1930  0.0450\n",
            "     18       37.1930  0.0481\n",
            "     19       37.1930  0.0422\n",
            "     20       37.1930  0.0447\n",
            "     21       37.1930  0.0467\n",
            "     22       37.1930  0.0493\n",
            "     23       37.1930  0.0475\n",
            "     24       37.1930  0.0524\n",
            "     25       37.1930  0.0416\n",
            "     26       37.1930  0.0426\n",
            "     27       37.1930  0.0443\n",
            "     28       37.1930  0.0601\n",
            "     29       37.1930  0.0411\n",
            "     30       37.1930  0.0424\n",
            "     31       37.1930  0.0419\n",
            "     32       37.1930  0.0465\n",
            "     33       37.1930  0.0440\n",
            "     34       37.1930  0.0445\n",
            "     35       37.1930  0.0437\n",
            "     36       37.1930  0.0448\n",
            "     37       37.1930  0.0447\n",
            "     38       37.1930  0.0439\n",
            "     39       37.1930  0.0417\n",
            "     40       37.1930  0.0437\n",
            "     41       37.1930  0.0500\n",
            "     42       37.1930  0.0514\n",
            "     43       37.1930  0.0466\n",
            "     44       37.1930  0.0535\n",
            "     45       37.1930  0.0484\n",
            "     46       37.1930  0.0481\n",
            "     47       37.1930  0.0491\n",
            "     48       37.1930  0.0544\n",
            "     49       \u001b[36m29.8862\u001b[0m  0.0525\n",
            "     50        \u001b[36m3.1374\u001b[0m  0.0447\n",
            "     51        \u001b[36m2.7579\u001b[0m  0.0422\n",
            "     52        \u001b[36m2.4067\u001b[0m  0.0446\n",
            "     53        \u001b[36m0.9961\u001b[0m  0.0500\n",
            "     54        1.4072  0.0436\n",
            "     55        \u001b[36m0.6231\u001b[0m  0.0451\n",
            "     56        0.6464  0.0421\n",
            "     57        \u001b[36m0.5823\u001b[0m  0.0449\n",
            "     58        \u001b[36m0.5498\u001b[0m  0.0455\n",
            "     59        \u001b[36m0.5280\u001b[0m  0.0454\n",
            "     60        \u001b[36m0.5193\u001b[0m  0.0502\n",
            "     61        \u001b[36m0.5089\u001b[0m  0.0434\n",
            "     62        \u001b[36m0.5062\u001b[0m  0.0473\n",
            "     63        \u001b[36m0.4975\u001b[0m  0.0593\n",
            "     64        \u001b[36m0.4900\u001b[0m  0.0468\n",
            "     65        \u001b[36m0.4858\u001b[0m  0.0579\n",
            "     66        \u001b[36m0.4810\u001b[0m  0.0503\n",
            "     67        \u001b[36m0.4763\u001b[0m  0.0464\n",
            "     68        \u001b[36m0.4718\u001b[0m  0.0464\n",
            "     69        \u001b[36m0.4703\u001b[0m  0.0479\n",
            "     70        \u001b[36m0.4615\u001b[0m  0.0531\n",
            "     71        \u001b[36m0.4582\u001b[0m  0.0488\n",
            "     72        \u001b[36m0.4563\u001b[0m  0.0443\n",
            "     73        \u001b[36m0.4521\u001b[0m  0.0449\n",
            "     74        \u001b[36m0.4488\u001b[0m  0.0457\n",
            "     75        \u001b[36m0.4453\u001b[0m  0.0458\n",
            "     76        \u001b[36m0.4430\u001b[0m  0.0430\n",
            "     77        0.4450  0.0442\n",
            "     78        \u001b[36m0.4417\u001b[0m  0.0484\n",
            "     79        \u001b[36m0.4345\u001b[0m  0.0475\n",
            "     80        0.4441  0.0478\n",
            "     81        0.4357  0.0515\n",
            "     82        \u001b[36m0.4330\u001b[0m  0.0466\n",
            "     83        \u001b[36m0.4274\u001b[0m  0.0513\n",
            "     84        0.4318  0.0528\n",
            "     85        0.4289  0.0472\n",
            "     86        0.4282  0.0445\n",
            "     87        \u001b[36m0.4251\u001b[0m  0.0435\n",
            "     88        \u001b[36m0.4224\u001b[0m  0.0465\n",
            "     89        \u001b[36m0.4219\u001b[0m  0.0495\n",
            "     90        \u001b[36m0.4196\u001b[0m  0.0526\n",
            "     91        0.4206  0.0447\n",
            "     92        \u001b[36m0.4166\u001b[0m  0.0432\n",
            "     93        0.4197  0.0437\n",
            "     94        \u001b[36m0.4130\u001b[0m  0.0439\n",
            "     95        0.4158  0.0437\n",
            "     96        \u001b[36m0.4091\u001b[0m  0.0443\n",
            "     97        \u001b[36m0.4064\u001b[0m  0.0471\n",
            "     98        0.4073  0.0494\n",
            "     99        0.4095  0.0449\n",
            "    100        \u001b[36m0.4053\u001b[0m  0.0437\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m20.4559\u001b[0m  0.0280\n",
            "      2       \u001b[36m15.3731\u001b[0m  0.0339\n",
            "      3       \u001b[36m10.0444\u001b[0m  0.0379\n",
            "      4        \u001b[36m5.6611\u001b[0m  0.0380\n",
            "      5        \u001b[36m5.6558\u001b[0m  0.0393\n",
            "      6        \u001b[36m5.6512\u001b[0m  0.0388\n",
            "      7        \u001b[36m5.6471\u001b[0m  0.0387\n",
            "      8        \u001b[36m5.6436\u001b[0m  0.0361\n",
            "      9        \u001b[36m5.6405\u001b[0m  0.0372\n",
            "     10        \u001b[36m5.6378\u001b[0m  0.0316\n",
            "     11        \u001b[36m5.6355\u001b[0m  0.0324\n",
            "     12        \u001b[36m5.6334\u001b[0m  0.0345\n",
            "     13        \u001b[36m5.6316\u001b[0m  0.0371\n",
            "     14        \u001b[36m5.6300\u001b[0m  0.0404\n",
            "     15        \u001b[36m5.6286\u001b[0m  0.0330\n",
            "     16        \u001b[36m5.6274\u001b[0m  0.0337\n",
            "     17        \u001b[36m5.6263\u001b[0m  0.0377\n",
            "     18        \u001b[36m5.6254\u001b[0m  0.0330\n",
            "     19        \u001b[36m5.6246\u001b[0m  0.0323\n",
            "     20        \u001b[36m5.6239\u001b[0m  0.0333\n",
            "     21        \u001b[36m5.6232\u001b[0m  0.0346\n",
            "     22        \u001b[36m5.6227\u001b[0m  0.0328\n",
            "     23        \u001b[36m5.6222\u001b[0m  0.0370\n",
            "     24        \u001b[36m5.6217\u001b[0m  0.0318\n",
            "     25        \u001b[36m5.6213\u001b[0m  0.0322\n",
            "     26        \u001b[36m5.6210\u001b[0m  0.0331\n",
            "     27        \u001b[36m5.6207\u001b[0m  0.0367\n",
            "     28        \u001b[36m5.6204\u001b[0m  0.0354\n",
            "     29        \u001b[36m5.6202\u001b[0m  0.0380\n",
            "     30        \u001b[36m5.6200\u001b[0m  0.0336\n",
            "     31        \u001b[36m5.6198\u001b[0m  0.0353\n",
            "     32        \u001b[36m5.6196\u001b[0m  0.0315\n",
            "     33        \u001b[36m5.6195\u001b[0m  0.0384\n",
            "     34        \u001b[36m5.6194\u001b[0m  0.0323\n",
            "     35        \u001b[36m5.6193\u001b[0m  0.0316\n",
            "     36        \u001b[36m5.6192\u001b[0m  0.0358\n",
            "     37        \u001b[36m5.6191\u001b[0m  0.0344\n",
            "     38        \u001b[36m5.6190\u001b[0m  0.0340\n",
            "     39        \u001b[36m5.6189\u001b[0m  0.0305\n",
            "     40        \u001b[36m5.6188\u001b[0m  0.0326\n",
            "     41        \u001b[36m5.6188\u001b[0m  0.0459\n",
            "     42        \u001b[36m5.6187\u001b[0m  0.0311\n",
            "     43        \u001b[36m5.6187\u001b[0m  0.0378\n",
            "     44        \u001b[36m5.6186\u001b[0m  0.0340\n",
            "     45        \u001b[36m5.6186\u001b[0m  0.0342\n",
            "     46        \u001b[36m5.6186\u001b[0m  0.0331\n",
            "     47        \u001b[36m5.6185\u001b[0m  0.0336\n",
            "     48        \u001b[36m5.6185\u001b[0m  0.0316\n",
            "     49        \u001b[36m5.6185\u001b[0m  0.0390\n",
            "     50        \u001b[36m5.6185\u001b[0m  0.0318\n",
            "     51        \u001b[36m5.6184\u001b[0m  0.0342\n",
            "     52        \u001b[36m5.6184\u001b[0m  0.0364\n",
            "     53        \u001b[36m5.6184\u001b[0m  0.0386\n",
            "     54        \u001b[36m5.6184\u001b[0m  0.0330\n",
            "     55        \u001b[36m5.6184\u001b[0m  0.0352\n",
            "     56        \u001b[36m5.6184\u001b[0m  0.0320\n",
            "     57        \u001b[36m5.6184\u001b[0m  0.0381\n",
            "     58        \u001b[36m5.6184\u001b[0m  0.0310\n",
            "     59        \u001b[36m5.6183\u001b[0m  0.0348\n",
            "     60        \u001b[36m5.6183\u001b[0m  0.0340\n",
            "     61        \u001b[36m5.6183\u001b[0m  0.0301\n",
            "     62        \u001b[36m5.6183\u001b[0m  0.0298\n",
            "     63        \u001b[36m5.6183\u001b[0m  0.0390\n",
            "     64        \u001b[36m5.6183\u001b[0m  0.0351\n",
            "     65        \u001b[36m5.6183\u001b[0m  0.0393\n",
            "     66        \u001b[36m5.6183\u001b[0m  0.0348\n",
            "     67        \u001b[36m5.6183\u001b[0m  0.0370\n",
            "     68        \u001b[36m5.6183\u001b[0m  0.0401\n",
            "     69        \u001b[36m5.6183\u001b[0m  0.0320\n",
            "     70        \u001b[36m5.6183\u001b[0m  0.0363\n",
            "     71        5.6183  0.0364\n",
            "     72        5.6183  0.0327\n",
            "     73        5.6183  0.0334\n",
            "     74        5.6183  0.0348\n",
            "     75        5.6183  0.0404\n",
            "     76        5.6183  0.0354\n",
            "     77        5.6183  0.0349\n",
            "     78        5.6183  0.0340\n",
            "     79        5.6183  0.0368\n",
            "     80        5.6183  0.0379\n",
            "     81        5.6183  0.0355\n",
            "     82        5.6183  0.0352\n",
            "     83        5.6183  0.0364\n",
            "     84        5.6183  0.0357\n",
            "     85        5.6183  0.0320\n",
            "     86        5.6183  0.0318\n",
            "     87        5.6183  0.0344\n",
            "     88        5.6183  0.0397\n",
            "     89        5.6184  0.0327\n",
            "     90        5.6184  0.0334\n",
            "     91        5.6184  0.0334\n",
            "     92        5.6184  0.0344\n",
            "     93        5.6184  0.0326\n",
            "     94        5.6184  0.0335\n",
            "     95        5.6184  0.0403\n",
            "     96        5.6184  0.0355\n",
            "     97        5.6184  0.0307\n",
            "     98        5.6184  0.0305\n",
            "     99        5.6184  0.0293\n",
            "    100        5.6184  0.0334\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m62.8070\u001b[0m  0.0285\n",
            "      2       62.8070  0.0402\n",
            "      3       62.8070  0.0322\n",
            "      4       62.8070  0.0348\n",
            "      5       62.8070  0.0334\n",
            "      6       62.8070  0.0342\n",
            "      7       62.8070  0.0340\n",
            "      8       62.8070  0.0329\n",
            "      9       62.8070  0.0404\n",
            "     10       62.8070  0.0406\n",
            "     11       62.8070  0.0366\n",
            "     12       62.8070  0.0406\n",
            "     13       62.8070  0.0361\n",
            "     14       62.8070  0.0347\n",
            "     15       62.8070  0.0334\n",
            "     16       62.8070  0.0289\n",
            "     17       62.8070  0.0307\n",
            "     18       62.8070  0.0433\n",
            "     19       62.8070  0.0350\n",
            "     20       62.8070  0.0382\n",
            "     21       62.8070  0.0425\n",
            "     22       62.8070  0.0452\n",
            "     23       62.8070  0.0299\n",
            "     24       62.8070  0.0340\n",
            "     25       62.8070  0.0378\n",
            "     26       62.8070  0.0326\n",
            "     27       62.8070  0.0374\n",
            "     28       62.8070  0.0378\n",
            "     29       62.8070  0.0386\n",
            "     30       62.8070  0.0388\n",
            "     31       62.8070  0.0369\n",
            "     32       62.8070  0.0355\n",
            "     33       62.8070  0.0339\n",
            "     34       62.8070  0.0358\n",
            "     35       62.8070  0.0377\n",
            "     36       62.8070  0.0413\n",
            "     37       62.8070  0.0387\n",
            "     38       62.8070  0.0317\n",
            "     39       62.8070  0.0338\n",
            "     40       62.8070  0.0368\n",
            "     41       62.8070  0.0295\n",
            "     42       62.8070  0.0317\n",
            "     43       62.8070  0.0425\n",
            "     44       62.8070  0.0304\n",
            "     45       62.8070  0.0346\n",
            "     46       62.8070  0.0324\n",
            "     47       62.8070  0.0328\n",
            "     48       62.8070  0.0406\n",
            "     49       62.8070  0.0323\n",
            "     50       62.8070  0.0323\n",
            "     51       62.8070  0.0330\n",
            "     52       62.8070  0.0343\n",
            "     53       62.8070  0.0304\n",
            "     54       62.8070  0.0332\n",
            "     55       62.8070  0.0354\n",
            "     56       62.8070  0.0357\n",
            "     57       62.8070  0.0309\n",
            "     58       62.8070  0.0316\n",
            "     59       62.8070  0.0335\n",
            "     60       62.8070  0.0313\n",
            "     61       62.8070  0.0346\n",
            "     62       62.8070  0.0414\n",
            "     63       62.8070  0.0362\n",
            "     64       62.8070  0.0328\n",
            "     65       62.8070  0.0334\n",
            "     66       62.8070  0.0309\n",
            "     67       62.8070  0.0305\n",
            "     68       62.8070  0.0343\n",
            "     69       62.8070  0.0351\n",
            "     70       62.8070  0.0378\n",
            "     71       62.8070  0.0338\n",
            "     72       62.8070  0.0328\n",
            "     73       62.8070  0.0305\n",
            "     74       62.8070  0.0324\n",
            "     75       62.8070  0.0363\n",
            "     76       62.8070  0.0393\n",
            "     77       62.8070  0.0340\n",
            "     78       62.8070  0.0307\n",
            "     79       62.8070  0.0291\n",
            "     80       62.8070  0.0349\n",
            "     81       62.8070  0.0350\n",
            "     82       62.8070  0.0340\n",
            "     83       62.8070  0.0336\n",
            "     84       62.8070  0.0325\n",
            "     85       62.8070  0.0311\n",
            "     86       62.8070  0.0324\n",
            "     87       62.8070  0.0307\n",
            "     88       62.8070  0.0303\n",
            "     89       62.8070  0.0385\n",
            "     90       62.8070  0.0354\n",
            "     91       62.8070  0.0342\n",
            "     92       62.8070  0.0356\n",
            "     93       62.8070  0.0339\n",
            "     94       62.8070  0.0351\n",
            "     95       62.8070  0.0327\n",
            "     96       62.8070  0.0330\n",
            "     97       62.8070  0.0310\n",
            "     98       62.8070  0.0311\n",
            "     99       62.8070  0.0327\n",
            "    100       62.8070  0.0383\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.7034\u001b[0m  0.0455\n",
            "      2       \u001b[36m37.6761\u001b[0m  0.0450\n",
            "      3       37.6761  0.0564\n",
            "      4       37.6761  0.0473\n",
            "      5       37.6761  0.0432\n",
            "      6       38.3803  0.0446\n",
            "      7       38.3806  0.0458\n",
            "      8       38.0282  0.0430\n",
            "      9       38.0282  0.0429\n",
            "     10       38.0282  0.0424\n",
            "     11       38.0282  0.0452\n",
            "     12       38.0282  0.0507\n",
            "     13       38.0282  0.0444\n",
            "     14       38.0282  0.0474\n",
            "     15       38.0282  0.0449\n",
            "     16       38.0282  0.0522\n",
            "     17       38.0282  0.0409\n",
            "     18       38.0282  0.0501\n",
            "     19       38.0282  0.0466\n",
            "     20       38.0282  0.0414\n",
            "     21       38.0282  0.0413\n",
            "     22       38.0282  0.0464\n",
            "     23       38.0282  0.0478\n",
            "     24       38.0282  0.0525\n",
            "     25       38.0282  0.0411\n",
            "     26       38.0282  0.0449\n",
            "     27       38.0282  0.0476\n",
            "     28       38.0282  0.0455\n",
            "     29       38.0282  0.0413\n",
            "     30       38.0282  0.0470\n",
            "     31       38.0282  0.0466\n",
            "     32       38.0282  0.0445\n",
            "     33       38.0282  0.0474\n",
            "     34       38.0282  0.0483\n",
            "     35       38.0282  0.0431\n",
            "     36       37.9848  0.0491\n",
            "     37       37.9317  0.0487\n",
            "     38       37.8780  0.0407\n",
            "     39       37.8236  0.0465\n",
            "     40       37.7677  0.0441\n",
            "     41       37.6761  0.0425\n",
            "     42       37.6761  0.0433\n",
            "     43       37.6761  0.0442\n",
            "     44       37.6761  0.0436\n",
            "     45       37.6761  0.0541\n",
            "     46       37.6761  0.0505\n",
            "     47       37.6761  0.0486\n",
            "     48       37.6761  0.0429\n",
            "     49       37.6761  0.0431\n",
            "     50       37.6761  0.0492\n",
            "     51       37.6761  0.0479\n",
            "     52       37.6761  0.0451\n",
            "     53       37.6761  0.0492\n",
            "     54       37.6761  0.0513\n",
            "     55       37.6761  0.0520\n",
            "     56       37.6761  0.0459\n",
            "     57       37.6761  0.0439\n",
            "     58       37.6761  0.0428\n",
            "     59       37.6761  0.0443\n",
            "     60       37.6761  0.0443\n",
            "     61       37.6761  0.0510\n",
            "     62       37.6761  0.0421\n",
            "     63       37.6761  0.0442\n",
            "     64       37.6761  0.0464\n",
            "     65       37.6761  0.0470\n",
            "     66       37.6761  0.0564\n",
            "     67       37.6761  0.0502\n",
            "     68       37.6761  0.0463\n",
            "     69       37.6761  0.0495\n",
            "     70       37.6761  0.0461\n",
            "     71       37.6761  0.0501\n",
            "     72       37.6761  0.0512\n",
            "     73       37.6761  0.0531\n",
            "     74       37.6761  0.0486\n",
            "     75       37.6761  0.0454\n",
            "     76       37.6761  0.0446\n",
            "     77       37.6761  0.0454\n",
            "     78       37.6761  0.0480\n",
            "     79       37.6761  0.0490\n",
            "     80       37.6761  0.0490\n",
            "     81       37.6761  0.0461\n",
            "     82       37.6761  0.0444\n",
            "     83       37.6761  0.0486\n",
            "     84       37.6761  0.0444\n",
            "     85       37.6761  0.0487\n",
            "     86       37.6761  0.0532\n",
            "     87       37.6761  0.0478\n",
            "     88       37.6761  0.0489\n",
            "     89       37.6761  0.0473\n",
            "     90       37.6761  0.0535\n",
            "     91       37.6761  0.0543\n",
            "     92       37.6761  0.0669\n",
            "     93       37.6761  0.0503\n",
            "     94       37.6761  0.0510\n",
            "     95       37.6761  0.0553\n",
            "     96       37.6761  0.0550\n",
            "     97       37.6761  0.0437\n",
            "     98       37.6761  0.0448\n",
            "     99       37.6761  0.0448\n",
            "    100       37.6761  0.0442\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0408\n",
            "      2       37.1930  0.0450\n",
            "      3       37.1930  0.0477\n",
            "      4       37.1930  0.0433\n",
            "      5       37.1930  0.0558\n",
            "      6       37.1930  0.0449\n",
            "      7       37.1930  0.0432\n",
            "      8       37.1930  0.0480\n",
            "      9       37.1930  0.0570\n",
            "     10       \u001b[36m36.8432\u001b[0m  0.0535\n",
            "     11       \u001b[36m36.8421\u001b[0m  0.0502\n",
            "     12       36.8421  0.0537\n",
            "     13       36.8421  0.0487\n",
            "     14       36.8421  0.0434\n",
            "     15       36.8421  0.0437\n",
            "     16       36.8421  0.0454\n",
            "     17       36.8421  0.0427\n",
            "     18       36.8421  0.0490\n",
            "     19       36.8421  0.0450\n",
            "     20       36.8421  0.0514\n",
            "     21       36.8421  0.0469\n",
            "     22       36.8421  0.0441\n",
            "     23       36.8421  0.0430\n",
            "     24       36.8421  0.0455\n",
            "     25       36.8421  0.0578\n",
            "     26       36.8421  0.0483\n",
            "     27       36.8421  0.0447\n",
            "     28       36.8421  0.0528\n",
            "     29       36.8421  0.0521\n",
            "     30       36.8421  0.0618\n",
            "     31       36.8421  0.0559\n",
            "     32       36.8421  0.0508\n",
            "     33       36.8421  0.0524\n",
            "     34       36.8421  0.0499\n",
            "     35       36.8421  0.0443\n",
            "     36       36.8421  0.0478\n",
            "     37       36.8421  0.0508\n",
            "     38       36.8421  0.0480\n",
            "     39       36.8421  0.0438\n",
            "     40       36.8421  0.0433\n",
            "     41       36.8421  0.0487\n",
            "     42       36.8421  0.0427\n",
            "     43       36.8421  0.0456\n",
            "     44       36.8421  0.0489\n",
            "     45       36.8421  0.0594\n",
            "     46       36.8421  0.0551\n",
            "     47       36.8421  0.0467\n",
            "     48       36.8421  0.0662\n",
            "     49       36.8421  0.0477\n",
            "     50       36.8421  0.0535\n",
            "     51       36.8421  0.0561\n",
            "     52       36.8421  0.0473\n",
            "     53       36.8421  0.0481\n",
            "     54       36.8421  0.0513\n",
            "     55       36.8421  0.0442\n",
            "     56       36.8421  0.0506\n",
            "     57       36.8421  0.0458\n",
            "     58       36.8421  0.0442\n",
            "     59       36.8421  0.0559\n",
            "     60       36.8421  0.0473\n",
            "     61       36.8421  0.0561\n",
            "     62       36.8421  0.0456\n",
            "     63       36.8421  0.0596\n",
            "     64       36.8421  0.0646\n",
            "     65       36.8421  0.0494\n",
            "     66       36.8421  0.0465\n",
            "     67       36.8421  0.0526\n",
            "     68       36.8421  0.0811\n",
            "     69       36.8421  0.0927\n",
            "     70       36.8421  0.0699\n",
            "     71       36.8421  0.0691\n",
            "     72       36.8421  0.0690\n",
            "     73       36.8421  0.0800\n",
            "     74       36.8421  0.0902\n",
            "     75       36.8421  0.0953\n",
            "     76       36.8421  0.0903\n",
            "     77       36.8421  0.0826\n",
            "     78       36.8421  0.1003\n",
            "     79       36.8421  0.0832\n",
            "     80       36.8421  0.0781\n",
            "     81       36.8421  0.0785\n",
            "     82       36.8421  0.0744\n",
            "     83       36.8421  0.0719\n",
            "     84       36.8421  0.0763\n",
            "     85       36.8421  0.0771\n",
            "     86       36.8421  0.0708\n",
            "     87       36.8421  0.0870\n",
            "     88       36.8421  0.0678\n",
            "     89       36.8421  0.0789\n",
            "     90       36.8421  0.0761\n",
            "     91       36.8421  0.0936\n",
            "     92       36.8421  0.0767\n",
            "     93       36.8421  0.0797\n",
            "     94       36.8421  0.0802\n",
            "     95       36.8421  0.0733\n",
            "     96       36.8421  0.0712\n",
            "     97       36.8421  0.0938\n",
            "     98       36.8421  0.0989\n",
            "     99       36.8421  0.0808\n",
            "    100       36.8421  0.0716\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m61.2676\u001b[0m  0.0754\n",
            "      2       61.2676  0.0738\n",
            "      3       61.2676  0.0582\n",
            "      4       61.2676  0.0478\n",
            "      5       61.2676  0.0608\n",
            "      6       61.2676  0.0432\n",
            "      7       61.2676  0.0480\n",
            "      8       61.2676  0.0464\n",
            "      9       61.2676  0.0514\n",
            "     10       61.2676  0.0547\n",
            "     11       61.2676  0.0567\n",
            "     12       61.2676  0.0498\n",
            "     13       61.2676  0.0511\n",
            "     14       61.2676  0.0547\n",
            "     15       61.2676  0.0569\n",
            "     16       61.2676  0.0603\n",
            "     17       61.2676  0.0664\n",
            "     18       61.2676  0.0586\n",
            "     19       61.2676  0.0556\n",
            "     20       61.2676  0.0630\n",
            "     21       61.2676  0.0550\n",
            "     22       61.2676  0.0549\n",
            "     23       61.2676  0.0516\n",
            "     24       61.2676  0.0536\n",
            "     25       61.2676  0.0550\n",
            "     26       61.2676  0.0529\n",
            "     27       61.2676  0.0562\n",
            "     28       61.2676  0.0649\n",
            "     29       61.2676  0.0630\n",
            "     30       61.2676  0.0589\n",
            "     31       61.2676  0.0546\n",
            "     32       61.2676  0.0603\n",
            "     33       61.2676  0.0607\n",
            "     34       61.2676  0.0493\n",
            "     35       61.2676  0.0583\n",
            "     36       61.2676  0.0443\n",
            "     37       61.2676  0.0566\n",
            "     38       61.2676  0.0541\n",
            "     39       61.2676  0.0340\n",
            "     40       61.2676  0.0343\n",
            "     41       61.2676  0.0429\n",
            "     42       61.2676  0.0313\n",
            "     43       61.2676  0.0314\n",
            "     44       61.2676  0.0337\n",
            "     45       61.2676  0.0347\n",
            "     46       61.2676  0.0304\n",
            "     47       61.2676  0.0308\n",
            "     48       61.2676  0.0296\n",
            "     49       61.2676  0.0373\n",
            "     50       61.2676  0.0374\n",
            "     51       61.2676  0.0344\n",
            "     52       61.2676  0.0378\n",
            "     53       61.2676  0.0373\n",
            "     54       61.2676  0.0306\n",
            "     55       61.2676  0.0333\n",
            "     56       61.2676  0.0351\n",
            "     57       61.2676  0.0342\n",
            "     58       61.2676  0.0346\n",
            "     59       61.2676  0.0364\n",
            "     60       61.2676  0.0493\n",
            "     61       61.2676  0.0362\n",
            "     62       61.2676  0.0374\n",
            "     63       61.2676  0.0335\n",
            "     64       61.2676  0.0317\n",
            "     65       61.2676  0.0302\n",
            "     66       61.2676  0.0344\n",
            "     67       61.2676  0.0320\n",
            "     68       61.2676  0.0334\n",
            "     69       61.2676  0.0326\n",
            "     70       61.2676  0.0363\n",
            "     71       61.2676  0.0321\n",
            "     72       61.2676  0.0330\n",
            "     73       61.2676  0.0406\n",
            "     74       61.2676  0.0321\n",
            "     75       61.2676  0.0330\n",
            "     76       61.2676  0.0340\n",
            "     77       61.2676  0.0291\n",
            "     78       61.2676  0.0309\n",
            "     79       61.2676  0.0381\n",
            "     80       61.2676  0.0354\n",
            "     81       61.2676  0.0324\n",
            "     82       61.2676  0.0342\n",
            "     83       61.2676  0.0391\n",
            "     84       61.2676  0.0368\n",
            "     85       61.2676  0.0358\n",
            "     86       61.2676  0.0344\n",
            "     87       61.2676  0.0474\n",
            "     88       61.2676  0.0364\n",
            "     89       61.2676  0.0339\n",
            "     90       61.2676  0.0325\n",
            "     91       61.2676  0.0346\n",
            "     92       61.2676  0.0386\n",
            "     93       61.2676  0.0312\n",
            "     94       61.2676  0.0384\n",
            "     95       61.2676  0.0319\n",
            "     96       61.2676  0.0317\n",
            "     97       61.2676  0.0387\n",
            "     98       61.2676  0.0378\n",
            "     99       61.2676  0.0392\n",
            "    100       61.2676  0.0363\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m40.7018\u001b[0m  0.0287\n",
            "      2       40.7018  0.0345\n",
            "      3       40.7018  0.0340\n",
            "      4       40.7018  0.0375\n",
            "      5       40.7018  0.0347\n",
            "      6       40.7018  0.0305\n",
            "      7       40.7018  0.0380\n",
            "      8       40.7018  0.0382\n",
            "      9       40.7018  0.0361\n",
            "     10       40.7018  0.0359\n",
            "     11       40.7018  0.0369\n",
            "     12       40.7018  0.0370\n",
            "     13       40.7018  0.0372\n",
            "     14       40.7018  0.0442\n",
            "     15       40.7018  0.0347\n",
            "     16       40.7018  0.0377\n",
            "     17       40.7018  0.0388\n",
            "     18       40.7018  0.0672\n",
            "     19       40.7018  0.0667\n",
            "     20       40.7018  0.0562\n",
            "     21       40.7018  0.0501\n",
            "     22       40.7018  0.0633\n",
            "     23       40.7018  0.0595\n",
            "     24       40.7018  0.0681\n",
            "     25       40.7018  0.1515\n",
            "     26       40.7018  0.0711\n",
            "     27       40.7018  0.0851\n",
            "     28       40.7018  0.0350\n",
            "     29       40.7018  0.0487\n",
            "     30       40.7018  0.0363\n",
            "     31       40.7018  0.0349\n",
            "     32       40.7018  0.0499\n",
            "     33       40.7018  0.0315\n",
            "     34       40.7018  0.0352\n",
            "     35       40.7018  0.0460\n",
            "     36       40.7018  0.0526\n",
            "     37       40.7018  0.0548\n",
            "     38       40.7018  0.0555\n",
            "     39       40.7018  0.0498\n",
            "     40       40.7018  0.0445\n",
            "     41       40.7018  0.0516\n",
            "     42       40.7018  0.1244\n",
            "     43       40.7018  0.1435\n",
            "     44       40.7018  0.1271\n",
            "     45       40.7018  0.0448\n",
            "     46       40.7018  0.0537\n",
            "     47       40.7018  0.0378\n",
            "     48       40.7018  0.0339\n",
            "     49       40.7018  0.0406\n",
            "     50       40.7018  0.0368\n",
            "     51       40.7018  0.0342\n",
            "     52       40.7018  0.0448\n",
            "     53       40.7018  0.0373\n",
            "     54       40.7018  0.0336\n",
            "     55       40.7018  0.0381\n",
            "     56       40.7018  0.0342\n",
            "     57       40.7018  0.0362\n",
            "     58       40.7018  0.0380\n",
            "     59       40.7018  0.0345\n",
            "     60       40.7018  0.0337\n",
            "     61       40.7018  0.0340\n",
            "     62       40.7018  0.0452\n",
            "     63       40.7018  0.0433\n",
            "     64       40.7018  0.0407\n",
            "     65       40.7018  0.0398\n",
            "     66       40.7018  0.0411\n",
            "     67       40.7018  0.0430\n",
            "     68       40.7018  0.0479\n",
            "     69       40.7018  0.0368\n",
            "     70       40.7018  0.0336\n",
            "     71       40.7018  0.0334\n",
            "     72       40.7018  0.0364\n",
            "     73       40.7018  0.0405\n",
            "     74       40.7018  0.0336\n",
            "     75       40.7018  0.0326\n",
            "     76       40.7018  0.0330\n",
            "     77       40.7018  0.0349\n",
            "     78       40.7018  0.0326\n",
            "     79       40.7018  0.0311\n",
            "     80       40.7018  0.0353\n",
            "     81       40.7018  0.0385\n",
            "     82       40.7018  0.0379\n",
            "     83       40.7018  0.0358\n",
            "     84       40.7018  0.0356\n",
            "     85       40.7018  0.0370\n",
            "     86       40.7018  0.0382\n",
            "     87       40.7018  0.0394\n",
            "     88       40.7018  0.0343\n",
            "     89       40.7018  0.0359\n",
            "     90       40.7018  0.0383\n",
            "     91       40.7018  0.0348\n",
            "     92       40.7018  0.0368\n",
            "     93       40.7018  0.0347\n",
            "     94       40.7018  0.0408\n",
            "     95       40.7018  0.0415\n",
            "     96       40.7018  0.0412\n",
            "     97       40.7018  0.0446\n",
            "     98       40.7018  0.0351\n",
            "     99       40.7018  0.0342\n",
            "    100       40.7018  0.0330\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.5368\u001b[0m  0.0445\n",
            "      2        \u001b[36m1.4737\u001b[0m  0.0440\n",
            "      3        \u001b[36m1.4087\u001b[0m  0.0473\n",
            "      4        \u001b[36m1.3425\u001b[0m  0.0413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m1.2750\u001b[0m  0.0482\n",
            "      6        \u001b[36m1.2043\u001b[0m  0.0525\n",
            "      7        \u001b[36m1.1248\u001b[0m  0.0463\n",
            "      8        \u001b[36m1.0262\u001b[0m  0.0480\n",
            "      9        \u001b[36m0.9034\u001b[0m  0.0490\n",
            "     10        \u001b[36m0.7837\u001b[0m  0.0476\n",
            "     11        \u001b[36m0.7113\u001b[0m  0.0526\n",
            "     12        \u001b[36m0.6831\u001b[0m  0.0581\n",
            "     13        \u001b[36m0.6739\u001b[0m  0.0503\n",
            "     14        \u001b[36m0.6708\u001b[0m  0.0487\n",
            "     15        \u001b[36m0.6695\u001b[0m  0.0569\n",
            "     16        \u001b[36m0.6688\u001b[0m  0.0473\n",
            "     17        \u001b[36m0.6684\u001b[0m  0.0441\n",
            "     18        \u001b[36m0.6682\u001b[0m  0.0464\n",
            "     19        \u001b[36m0.6680\u001b[0m  0.0476\n",
            "     20        \u001b[36m0.6678\u001b[0m  0.0503\n",
            "     21        \u001b[36m0.6677\u001b[0m  0.0441\n",
            "     22        \u001b[36m0.6676\u001b[0m  0.0450\n",
            "     23        \u001b[36m0.6675\u001b[0m  0.0455\n",
            "     24        \u001b[36m0.6674\u001b[0m  0.0421\n",
            "     25        \u001b[36m0.6673\u001b[0m  0.0504\n",
            "     26        \u001b[36m0.6673\u001b[0m  0.0438\n",
            "     27        \u001b[36m0.6672\u001b[0m  0.0426\n",
            "     28        \u001b[36m0.6672\u001b[0m  0.0449\n",
            "     29        \u001b[36m0.6671\u001b[0m  0.0489\n",
            "     30        \u001b[36m0.6671\u001b[0m  0.0507\n",
            "     31        \u001b[36m0.6671\u001b[0m  0.0474\n",
            "     32        \u001b[36m0.6670\u001b[0m  0.0470\n",
            "     33        \u001b[36m0.6670\u001b[0m  0.0507\n",
            "     34        \u001b[36m0.6670\u001b[0m  0.0489\n",
            "     35        \u001b[36m0.6670\u001b[0m  0.0490\n",
            "     36        \u001b[36m0.6669\u001b[0m  0.0532\n",
            "     37        \u001b[36m0.6669\u001b[0m  0.0459\n",
            "     38        \u001b[36m0.6669\u001b[0m  0.0510\n",
            "     39        \u001b[36m0.6669\u001b[0m  0.0444\n",
            "     40        \u001b[36m0.6669\u001b[0m  0.0427\n",
            "     41        \u001b[36m0.6668\u001b[0m  0.0485\n",
            "     42        \u001b[36m0.6668\u001b[0m  0.0501\n",
            "     43        \u001b[36m0.6645\u001b[0m  0.0448\n",
            "     44        0.6773  0.0490\n",
            "     45        0.6666  0.0485\n",
            "     46        0.6669  0.0486\n",
            "     47        0.6684  0.0467\n",
            "     48        0.6680  0.0437\n",
            "     49        0.6677  0.0511\n",
            "     50        0.6674  0.0507\n",
            "     51        0.6672  0.0497\n",
            "     52        0.6670  0.0503\n",
            "     53        0.6668  0.0500\n",
            "     54        0.6667  0.0568\n",
            "     55        0.6665  0.0504\n",
            "     56        0.6664  0.0583\n",
            "     57        0.6663  0.0434\n",
            "     58        0.6661  0.0439\n",
            "     59        0.6660  0.0417\n",
            "     60        0.6659  0.0431\n",
            "     61        0.6659  0.0436\n",
            "     62        0.6658  0.0446\n",
            "     63        0.6657  0.0435\n",
            "     64        0.6656  0.0435\n",
            "     65        0.6656  0.0471\n",
            "     66        0.6655  0.0463\n",
            "     67        0.6654  0.0439\n",
            "     68        0.6654  0.0436\n",
            "     69        0.6653  0.0468\n",
            "     70        0.6653  0.0451\n",
            "     71        0.6652  0.0475\n",
            "     72        0.6652  0.0444\n",
            "     73        0.6652  0.0448\n",
            "     74        0.6651  0.0473\n",
            "     75        0.6651  0.0459\n",
            "     76        0.6651  0.0442\n",
            "     77        0.6650  0.0484\n",
            "     78        0.6650  0.0527\n",
            "     79        0.6650  0.0419\n",
            "     80        0.6649  0.0491\n",
            "     81        0.6649  0.0451\n",
            "     82        0.6649  0.0429\n",
            "     83        0.6649  0.0434\n",
            "     84        0.6649  0.0456\n",
            "     85        0.6648  0.0443\n",
            "     86        \u001b[36m0.6625\u001b[0m  0.0445\n",
            "     87        \u001b[36m0.6599\u001b[0m  0.0487\n",
            "     88        \u001b[36m0.6556\u001b[0m  0.0484\n",
            "     89        \u001b[36m0.6505\u001b[0m  0.0522\n",
            "     90        \u001b[36m0.6370\u001b[0m  0.0435\n",
            "     91        0.6471  0.0456\n",
            "     92        0.6453  0.0461\n",
            "     93        0.6417  0.0488\n",
            "     94        \u001b[36m0.6268\u001b[0m  0.0438\n",
            "     95        \u001b[36m0.6153\u001b[0m  0.0499\n",
            "     96        0.6246  0.0438\n",
            "     97        0.6202  0.0453\n",
            "     98        \u001b[36m0.6074\u001b[0m  0.0513\n",
            "     99        \u001b[36m0.6004\u001b[0m  0.0469\n",
            "    100        0.6216  0.0441\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.1739\u001b[0m  0.0399\n",
            "      2        \u001b[36m2.0999\u001b[0m  0.0451\n",
            "      3        \u001b[36m2.0230\u001b[0m  0.0403\n",
            "      4        \u001b[36m1.9371\u001b[0m  0.0429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m1.8350\u001b[0m  0.0532\n",
            "      6        \u001b[36m1.7018\u001b[0m  0.0417\n",
            "      7        \u001b[36m1.5176\u001b[0m  0.0473\n",
            "      8        \u001b[36m1.2739\u001b[0m  0.0427\n",
            "      9        \u001b[36m1.0005\u001b[0m  0.0528\n",
            "     10        \u001b[36m0.7907\u001b[0m  0.0502\n",
            "     11        \u001b[36m0.7006\u001b[0m  0.0582\n",
            "     12        \u001b[36m0.6760\u001b[0m  0.0542\n",
            "     13        \u001b[36m0.6708\u001b[0m  0.0510\n",
            "     14        \u001b[36m0.6696\u001b[0m  0.0523\n",
            "     15        \u001b[36m0.6692\u001b[0m  0.0471\n",
            "     16        \u001b[36m0.6689\u001b[0m  0.0492\n",
            "     17        \u001b[36m0.6686\u001b[0m  0.0429\n",
            "     18        \u001b[36m0.6684\u001b[0m  0.0553\n",
            "     19        \u001b[36m0.6682\u001b[0m  0.0455\n",
            "     20        \u001b[36m0.6681\u001b[0m  0.0457\n",
            "     21        \u001b[36m0.6680\u001b[0m  0.0494\n",
            "     22        \u001b[36m0.6679\u001b[0m  0.0449\n",
            "     23        \u001b[36m0.6678\u001b[0m  0.0401\n",
            "     24        \u001b[36m0.6678\u001b[0m  0.0424\n",
            "     25        \u001b[36m0.6677\u001b[0m  0.0471\n",
            "     26        \u001b[36m0.6677\u001b[0m  0.0484\n",
            "     27        \u001b[36m0.6676\u001b[0m  0.0452\n",
            "     28        \u001b[36m0.6676\u001b[0m  0.0495\n",
            "     29        \u001b[36m0.6675\u001b[0m  0.0546\n",
            "     30        \u001b[36m0.6675\u001b[0m  0.0460\n",
            "     31        \u001b[36m0.6675\u001b[0m  0.0586\n",
            "     32        \u001b[36m0.6674\u001b[0m  0.0483\n",
            "     33        \u001b[36m0.6674\u001b[0m  0.0475\n",
            "     34        \u001b[36m0.6674\u001b[0m  0.0434\n",
            "     35        \u001b[36m0.6673\u001b[0m  0.0438\n",
            "     36        \u001b[36m0.6673\u001b[0m  0.0449\n",
            "     37        \u001b[36m0.6673\u001b[0m  0.0433\n",
            "     38        \u001b[36m0.6673\u001b[0m  0.0469\n",
            "     39        \u001b[36m0.6673\u001b[0m  0.0416\n",
            "     40        \u001b[36m0.6672\u001b[0m  0.0520\n",
            "     41        \u001b[36m0.6672\u001b[0m  0.0484\n",
            "     42        \u001b[36m0.6672\u001b[0m  0.0501\n",
            "     43        \u001b[36m0.6672\u001b[0m  0.0439\n",
            "     44        \u001b[36m0.6672\u001b[0m  0.0457\n",
            "     45        \u001b[36m0.6671\u001b[0m  0.0486\n",
            "     46        \u001b[36m0.6671\u001b[0m  0.0440\n",
            "     47        \u001b[36m0.6671\u001b[0m  0.0476\n",
            "     48        \u001b[36m0.6671\u001b[0m  0.0470\n",
            "     49        \u001b[36m0.6671\u001b[0m  0.0497\n",
            "     50        \u001b[36m0.6667\u001b[0m  0.0517\n",
            "     51        0.6671  0.0482\n",
            "     52        \u001b[36m0.6652\u001b[0m  0.0486\n",
            "     53        0.6690  0.0448\n",
            "     54        0.6666  0.0499\n",
            "     55        0.6667  0.0441\n",
            "     56        0.6669  0.0435\n",
            "     57        0.6669  0.0438\n",
            "     58        0.6745  0.0505\n",
            "     59        0.6661  0.0495\n",
            "     60        0.6655  0.0426\n",
            "     61        \u001b[36m0.6646\u001b[0m  0.0511\n",
            "     62        \u001b[36m0.6610\u001b[0m  0.0575\n",
            "     63        \u001b[36m0.6594\u001b[0m  0.0460\n",
            "     64        \u001b[36m0.6472\u001b[0m  0.0473\n",
            "     65        0.6765  0.0455\n",
            "     66        0.6714  0.0483\n",
            "     67        0.6671  0.0425\n",
            "     68        0.6650  0.0447\n",
            "     69        0.6640  0.0539\n",
            "     70        0.6635  0.0502\n",
            "     71        0.6631  0.0494\n",
            "     72        0.6629  0.0463\n",
            "     73        0.6628  0.0442\n",
            "     74        0.6627  0.0458\n",
            "     75        0.6622  0.0442\n",
            "     76        0.6621  0.0504\n",
            "     77        0.6620  0.0443\n",
            "     78        0.6619  0.0461\n",
            "     79        0.6618  0.0503\n",
            "     80        0.6617  0.0487\n",
            "     81        0.6616  0.0485\n",
            "     82        0.6616  0.0454\n",
            "     83        0.6615  0.0428\n",
            "     84        0.6615  0.0533\n",
            "     85        0.6614  0.0445\n",
            "     86        0.6613  0.0440\n",
            "     87        0.6613  0.0438\n",
            "     88        0.6613  0.0438\n",
            "     89        0.6612  0.0441\n",
            "     90        0.6612  0.0460\n",
            "     91        0.6611  0.0553\n",
            "     92        0.6611  0.0533\n",
            "     93        0.6611  0.0491\n",
            "     94        0.6610  0.0509\n",
            "     95        0.6610  0.0469\n",
            "     96        0.6610  0.0444\n",
            "     97        0.6609  0.0412\n",
            "     98        0.6609  0.0419\n",
            "     99        0.6609  0.0446\n",
            "    100        0.6609  0.0574\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.3734\u001b[0m  0.0323\n",
            "      2        \u001b[36m1.3417\u001b[0m  0.0356\n",
            "      3        \u001b[36m1.3104\u001b[0m  0.0332\n",
            "      4        \u001b[36m1.2796\u001b[0m  0.0340\n",
            "      5        \u001b[36m1.2493\u001b[0m  0.0384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m1.2197\u001b[0m  0.0395\n",
            "      7        \u001b[36m1.1906\u001b[0m  0.0364\n",
            "      8        \u001b[36m1.1622\u001b[0m  0.0333\n",
            "      9        \u001b[36m1.1344\u001b[0m  0.0366\n",
            "     10        \u001b[36m1.1074\u001b[0m  0.0337\n",
            "     11        \u001b[36m1.0812\u001b[0m  0.0340\n",
            "     12        \u001b[36m1.0557\u001b[0m  0.0462\n",
            "     13        \u001b[36m1.0310\u001b[0m  0.0373\n",
            "     14        \u001b[36m1.0073\u001b[0m  0.0343\n",
            "     15        \u001b[36m0.9843\u001b[0m  0.0372\n",
            "     16        \u001b[36m0.9623\u001b[0m  0.0317\n",
            "     17        \u001b[36m0.9412\u001b[0m  0.0375\n",
            "     18        \u001b[36m0.9211\u001b[0m  0.0370\n",
            "     19        \u001b[36m0.9019\u001b[0m  0.0447\n",
            "     20        \u001b[36m0.8836\u001b[0m  0.0353\n",
            "     21        \u001b[36m0.8664\u001b[0m  0.0391\n",
            "     22        \u001b[36m0.8500\u001b[0m  0.0334\n",
            "     23        \u001b[36m0.8347\u001b[0m  0.0364\n",
            "     24        \u001b[36m0.8202\u001b[0m  0.0352\n",
            "     25        \u001b[36m0.8067\u001b[0m  0.0426\n",
            "     26        \u001b[36m0.7940\u001b[0m  0.0371\n",
            "     27        \u001b[36m0.7823\u001b[0m  0.0349\n",
            "     28        \u001b[36m0.7714\u001b[0m  0.0318\n",
            "     29        \u001b[36m0.7612\u001b[0m  0.0315\n",
            "     30        \u001b[36m0.7519\u001b[0m  0.0355\n",
            "     31        \u001b[36m0.7433\u001b[0m  0.0317\n",
            "     32        \u001b[36m0.7353\u001b[0m  0.0321\n",
            "     33        \u001b[36m0.7281\u001b[0m  0.0333\n",
            "     34        \u001b[36m0.7214\u001b[0m  0.0411\n",
            "     35        \u001b[36m0.7154\u001b[0m  0.0313\n",
            "     36        \u001b[36m0.7098\u001b[0m  0.0342\n",
            "     37        \u001b[36m0.7048\u001b[0m  0.0316\n",
            "     38        \u001b[36m0.7003\u001b[0m  0.0337\n",
            "     39        \u001b[36m0.6961\u001b[0m  0.0346\n",
            "     40        \u001b[36m0.6924\u001b[0m  0.0334\n",
            "     41        \u001b[36m0.6890\u001b[0m  0.0345\n",
            "     42        \u001b[36m0.6860\u001b[0m  0.0343\n",
            "     43        \u001b[36m0.6833\u001b[0m  0.0325\n",
            "     44        \u001b[36m0.6808\u001b[0m  0.0316\n",
            "     45        \u001b[36m0.6786\u001b[0m  0.0332\n",
            "     46        \u001b[36m0.6767\u001b[0m  0.0334\n",
            "     47        \u001b[36m0.6749\u001b[0m  0.0384\n",
            "     48        \u001b[36m0.6733\u001b[0m  0.0317\n",
            "     49        \u001b[36m0.6719\u001b[0m  0.0320\n",
            "     50        \u001b[36m0.6706\u001b[0m  0.0357\n",
            "     51        \u001b[36m0.6695\u001b[0m  0.0349\n",
            "     52        \u001b[36m0.6685\u001b[0m  0.0429\n",
            "     53        \u001b[36m0.6676\u001b[0m  0.0329\n",
            "     54        \u001b[36m0.6669\u001b[0m  0.0355\n",
            "     55        \u001b[36m0.6662\u001b[0m  0.0340\n",
            "     56        \u001b[36m0.6655\u001b[0m  0.0385\n",
            "     57        \u001b[36m0.6650\u001b[0m  0.0370\n",
            "     58        \u001b[36m0.6645\u001b[0m  0.0349\n",
            "     59        \u001b[36m0.6641\u001b[0m  0.0366\n",
            "     60        \u001b[36m0.6637\u001b[0m  0.0344\n",
            "     61        \u001b[36m0.6634\u001b[0m  0.0378\n",
            "     62        \u001b[36m0.6631\u001b[0m  0.0358\n",
            "     63        \u001b[36m0.6628\u001b[0m  0.0412\n",
            "     64        \u001b[36m0.6626\u001b[0m  0.0371\n",
            "     65        \u001b[36m0.6624\u001b[0m  0.0452\n",
            "     66        \u001b[36m0.6622\u001b[0m  0.0397\n",
            "     67        \u001b[36m0.6621\u001b[0m  0.0403\n",
            "     68        \u001b[36m0.6619\u001b[0m  0.0386\n",
            "     69        \u001b[36m0.6618\u001b[0m  0.0414\n",
            "     70        \u001b[36m0.6617\u001b[0m  0.0343\n",
            "     71        \u001b[36m0.6616\u001b[0m  0.0338\n",
            "     72        \u001b[36m0.6615\u001b[0m  0.0354\n",
            "     73        \u001b[36m0.6615\u001b[0m  0.0334\n",
            "     74        \u001b[36m0.6614\u001b[0m  0.0356\n",
            "     75        \u001b[36m0.6613\u001b[0m  0.0355\n",
            "     76        \u001b[36m0.6613\u001b[0m  0.0354\n",
            "     77        \u001b[36m0.6613\u001b[0m  0.0394\n",
            "     78        \u001b[36m0.6612\u001b[0m  0.0375\n",
            "     79        \u001b[36m0.6612\u001b[0m  0.0405\n",
            "     80        \u001b[36m0.6612\u001b[0m  0.0363\n",
            "     81        \u001b[36m0.6612\u001b[0m  0.0346\n",
            "     82        \u001b[36m0.6611\u001b[0m  0.0354\n",
            "     83        \u001b[36m0.6611\u001b[0m  0.0351\n",
            "     84        \u001b[36m0.6611\u001b[0m  0.0339\n",
            "     85        \u001b[36m0.6611\u001b[0m  0.0361\n",
            "     86        \u001b[36m0.6611\u001b[0m  0.0313\n",
            "     87        \u001b[36m0.6611\u001b[0m  0.0336\n",
            "     88        \u001b[36m0.6611\u001b[0m  0.0340\n",
            "     89        \u001b[36m0.6611\u001b[0m  0.0381\n",
            "     90        \u001b[36m0.6611\u001b[0m  0.0386\n",
            "     91        \u001b[36m0.6611\u001b[0m  0.0378\n",
            "     92        \u001b[36m0.6611\u001b[0m  0.0378\n",
            "     93        \u001b[36m0.6611\u001b[0m  0.0373\n",
            "     94        \u001b[36m0.6611\u001b[0m  0.0355\n",
            "     95        \u001b[36m0.6611\u001b[0m  0.0362\n",
            "     96        \u001b[36m0.6611\u001b[0m  0.0337\n",
            "     97        0.6611  0.0351\n",
            "     98        0.6611  0.0324\n",
            "     99        0.6611  0.0319\n",
            "    100        0.6611  0.0346\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.2681\u001b[0m  0.0309\n",
            "      2        \u001b[36m1.2395\u001b[0m  0.0335\n",
            "      3        \u001b[36m1.2113\u001b[0m  0.0344\n",
            "      4        \u001b[36m1.1838\u001b[0m  0.0379\n",
            "      5        \u001b[36m1.1569\u001b[0m  0.0330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m1.1306\u001b[0m  0.0389\n",
            "      7        \u001b[36m1.1050\u001b[0m  0.0397\n",
            "      8        \u001b[36m1.0800\u001b[0m  0.0318\n",
            "      9        \u001b[36m1.0558\u001b[0m  0.0336\n",
            "     10        \u001b[36m1.0324\u001b[0m  0.0380\n",
            "     11        \u001b[36m1.0097\u001b[0m  0.0341\n",
            "     12        \u001b[36m0.9879\u001b[0m  0.0344\n",
            "     13        \u001b[36m0.9669\u001b[0m  0.0435\n",
            "     14        \u001b[36m0.9467\u001b[0m  0.0408\n",
            "     15        \u001b[36m0.9274\u001b[0m  0.0402\n",
            "     16        \u001b[36m0.9089\u001b[0m  0.0358\n",
            "     17        \u001b[36m0.8913\u001b[0m  0.0385\n",
            "     18        \u001b[36m0.8746\u001b[0m  0.0391\n",
            "     19        \u001b[36m0.8588\u001b[0m  0.0449\n",
            "     20        \u001b[36m0.8438\u001b[0m  0.0402\n",
            "     21        \u001b[36m0.8297\u001b[0m  0.0339\n",
            "     22        \u001b[36m0.8164\u001b[0m  0.0319\n",
            "     23        \u001b[36m0.8040\u001b[0m  0.0385\n",
            "     24        \u001b[36m0.7923\u001b[0m  0.0324\n",
            "     25        \u001b[36m0.7815\u001b[0m  0.0333\n",
            "     26        \u001b[36m0.7713\u001b[0m  0.0337\n",
            "     27        \u001b[36m0.7619\u001b[0m  0.0310\n",
            "     28        \u001b[36m0.7532\u001b[0m  0.0346\n",
            "     29        \u001b[36m0.7451\u001b[0m  0.0295\n",
            "     30        \u001b[36m0.7376\u001b[0m  0.0483\n",
            "     31        \u001b[36m0.7308\u001b[0m  0.0366\n",
            "     32        \u001b[36m0.7244\u001b[0m  0.0318\n",
            "     33        \u001b[36m0.7186\u001b[0m  0.0365\n",
            "     34        \u001b[36m0.7133\u001b[0m  0.0374\n",
            "     35        \u001b[36m0.7085\u001b[0m  0.0369\n",
            "     36        \u001b[36m0.7040\u001b[0m  0.0350\n",
            "     37        \u001b[36m0.7000\u001b[0m  0.0391\n",
            "     38        \u001b[36m0.6963\u001b[0m  0.0344\n",
            "     39        \u001b[36m0.6929\u001b[0m  0.0355\n",
            "     40        \u001b[36m0.6899\u001b[0m  0.0364\n",
            "     41        \u001b[36m0.6871\u001b[0m  0.0384\n",
            "     42        \u001b[36m0.6846\u001b[0m  0.0386\n",
            "     43        \u001b[36m0.6823\u001b[0m  0.0361\n",
            "     44        \u001b[36m0.6802\u001b[0m  0.0331\n",
            "     45        \u001b[36m0.6784\u001b[0m  0.0316\n",
            "     46        \u001b[36m0.6767\u001b[0m  0.0317\n",
            "     47        \u001b[36m0.6751\u001b[0m  0.0414\n",
            "     48        \u001b[36m0.6737\u001b[0m  0.0363\n",
            "     49        \u001b[36m0.6725\u001b[0m  0.0349\n",
            "     50        \u001b[36m0.6714\u001b[0m  0.0342\n",
            "     51        \u001b[36m0.6704\u001b[0m  0.0316\n",
            "     52        \u001b[36m0.6694\u001b[0m  0.0326\n",
            "     53        \u001b[36m0.6686\u001b[0m  0.0363\n",
            "     54        \u001b[36m0.6679\u001b[0m  0.0402\n",
            "     55        \u001b[36m0.6672\u001b[0m  0.0414\n",
            "     56        \u001b[36m0.6666\u001b[0m  0.0491\n",
            "     57        \u001b[36m0.6660\u001b[0m  0.0357\n",
            "     58        \u001b[36m0.6655\u001b[0m  0.0360\n",
            "     59        \u001b[36m0.6651\u001b[0m  0.0359\n",
            "     60        \u001b[36m0.6647\u001b[0m  0.0362\n",
            "     61        \u001b[36m0.6643\u001b[0m  0.0344\n",
            "     62        \u001b[36m0.6640\u001b[0m  0.0341\n",
            "     63        \u001b[36m0.6637\u001b[0m  0.0348\n",
            "     64        \u001b[36m0.6634\u001b[0m  0.0377\n",
            "     65        \u001b[36m0.6631\u001b[0m  0.0355\n",
            "     66        \u001b[36m0.6629\u001b[0m  0.0377\n",
            "     67        \u001b[36m0.6627\u001b[0m  0.0365\n",
            "     68        \u001b[36m0.6625\u001b[0m  0.0334\n",
            "     69        \u001b[36m0.6624\u001b[0m  0.0313\n",
            "     70        \u001b[36m0.6622\u001b[0m  0.0316\n",
            "     71        \u001b[36m0.6621\u001b[0m  0.0366\n",
            "     72        \u001b[36m0.6620\u001b[0m  0.0339\n",
            "     73        \u001b[36m0.6619\u001b[0m  0.0325\n",
            "     74        \u001b[36m0.6618\u001b[0m  0.0332\n",
            "     75        \u001b[36m0.6617\u001b[0m  0.0328\n",
            "     76        \u001b[36m0.6616\u001b[0m  0.0343\n",
            "     77        \u001b[36m0.6615\u001b[0m  0.0329\n",
            "     78        \u001b[36m0.6614\u001b[0m  0.0399\n",
            "     79        \u001b[36m0.6614\u001b[0m  0.0373\n",
            "     80        \u001b[36m0.6613\u001b[0m  0.0342\n",
            "     81        \u001b[36m0.6612\u001b[0m  0.0329\n",
            "     82        \u001b[36m0.6612\u001b[0m  0.0305\n",
            "     83        \u001b[36m0.6612\u001b[0m  0.0419\n",
            "     84        \u001b[36m0.6611\u001b[0m  0.0316\n",
            "     85        \u001b[36m0.6611\u001b[0m  0.0388\n",
            "     86        \u001b[36m0.6610\u001b[0m  0.0336\n",
            "     87        \u001b[36m0.6610\u001b[0m  0.0341\n",
            "     88        \u001b[36m0.6610\u001b[0m  0.0322\n",
            "     89        \u001b[36m0.6609\u001b[0m  0.0333\n",
            "     90        \u001b[36m0.6609\u001b[0m  0.0318\n",
            "     91        \u001b[36m0.6609\u001b[0m  0.0347\n",
            "     92        \u001b[36m0.6609\u001b[0m  0.0340\n",
            "     93        \u001b[36m0.6609\u001b[0m  0.0392\n",
            "     94        \u001b[36m0.6608\u001b[0m  0.0355\n",
            "     95        \u001b[36m0.6608\u001b[0m  0.0352\n",
            "     96        \u001b[36m0.6608\u001b[0m  0.0384\n",
            "     97        \u001b[36m0.6608\u001b[0m  0.0329\n",
            "     98        \u001b[36m0.6608\u001b[0m  0.0317\n",
            "     99        \u001b[36m0.6608\u001b[0m  0.0318\n",
            "    100        \u001b[36m0.6608\u001b[0m  0.0333\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.6159\u001b[0m  0.0397\n",
            "      2        \u001b[36m2.4893\u001b[0m  0.0484\n",
            "      3        \u001b[36m2.3577\u001b[0m  0.0447\n",
            "      4        \u001b[36m2.2231\u001b[0m  0.0462\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m2.0863\u001b[0m  0.0453\n",
            "      6        \u001b[36m1.9423\u001b[0m  0.0414\n",
            "      7        \u001b[36m1.7729\u001b[0m  0.0415\n",
            "      8        \u001b[36m1.5720\u001b[0m  0.0532\n",
            "      9        \u001b[36m1.2599\u001b[0m  0.0486\n",
            "     10        \u001b[36m0.9193\u001b[0m  0.0447\n",
            "     11        \u001b[36m0.7454\u001b[0m  0.0549\n",
            "     12        \u001b[36m0.6888\u001b[0m  0.0543\n",
            "     13        \u001b[36m0.6747\u001b[0m  0.0475\n",
            "     14        \u001b[36m0.6707\u001b[0m  0.0584\n",
            "     15        \u001b[36m0.6692\u001b[0m  0.0539\n",
            "     16        \u001b[36m0.6684\u001b[0m  0.0525\n",
            "     17        \u001b[36m0.6680\u001b[0m  0.0501\n",
            "     18        \u001b[36m0.6677\u001b[0m  0.0512\n",
            "     19        \u001b[36m0.6675\u001b[0m  0.0514\n",
            "     20        \u001b[36m0.6674\u001b[0m  0.0467\n",
            "     21        \u001b[36m0.6620\u001b[0m  0.0445\n",
            "     22        0.6713  0.0436\n",
            "     23        0.6679  0.0432\n",
            "     24        0.6665  0.0446\n",
            "     25        0.6659  0.0441\n",
            "     26        0.6651  0.0420\n",
            "     27        0.6648  0.0494\n",
            "     28        0.6646  0.0522\n",
            "     29        0.6633  0.0480\n",
            "     30        0.6641  0.0476\n",
            "     31        0.6632  0.0458\n",
            "     32        0.6657  0.0467\n",
            "     33        0.6633  0.0483\n",
            "     34        0.6632  0.0648\n",
            "     35        0.6621  0.0467\n",
            "     36        \u001b[36m0.6620\u001b[0m  0.0455\n",
            "     37        \u001b[36m0.6617\u001b[0m  0.0496\n",
            "     38        \u001b[36m0.6615\u001b[0m  0.0461\n",
            "     39        \u001b[36m0.6612\u001b[0m  0.0442\n",
            "     40        \u001b[36m0.6610\u001b[0m  0.0447\n",
            "     41        \u001b[36m0.6606\u001b[0m  0.0425\n",
            "     42        \u001b[36m0.6605\u001b[0m  0.0451\n",
            "     43        \u001b[36m0.6603\u001b[0m  0.0473\n",
            "     44        \u001b[36m0.6598\u001b[0m  0.0467\n",
            "     45        \u001b[36m0.6369\u001b[0m  0.0434\n",
            "     46        0.6742  0.0481\n",
            "     47        0.6648  0.0440\n",
            "     48        0.6616  0.0590\n",
            "     49        0.6613  0.0457\n",
            "     50        0.6570  0.0453\n",
            "     51        0.6549  0.0473\n",
            "     52        0.6544  0.0491\n",
            "     53        0.6539  0.0553\n",
            "     54        0.6535  0.0510\n",
            "     55        0.6530  0.0486\n",
            "     56        0.6527  0.0565\n",
            "     57        0.6517  0.0470\n",
            "     58        0.6509  0.0444\n",
            "     59        0.6498  0.0429\n",
            "     60        0.6487  0.0456\n",
            "     61        0.6476  0.0494\n",
            "     62        0.6467  0.0472\n",
            "     63        0.6460  0.0485\n",
            "     64        \u001b[36m0.6252\u001b[0m  0.0510\n",
            "     65        0.6533  0.0473\n",
            "     66        0.6465  0.0501\n",
            "     67        0.6373  0.0510\n",
            "     68        0.6518  0.0591\n",
            "     69        0.6458  0.0521\n",
            "     70        0.6435  0.0437\n",
            "     71        0.6426  0.0447\n",
            "     72        0.6420  0.0460\n",
            "     73        0.6415  0.0484\n",
            "     74        0.6409  0.0473\n",
            "     75        0.6405  0.0455\n",
            "     76        0.6400  0.0570\n",
            "     77        0.6394  0.0431\n",
            "     78        0.6389  0.0500\n",
            "     79        0.6384  0.0423\n",
            "     80        0.6354  0.0427\n",
            "     81        0.6349  0.0431\n",
            "     82        0.6344  0.0424\n",
            "     83        0.6341  0.0419\n",
            "     84        0.6337  0.0424\n",
            "     85        0.6334  0.0432\n",
            "     86        0.6331  0.0452\n",
            "     87        0.6328  0.0520\n",
            "     88        0.6326  0.0451\n",
            "     89        0.6323  0.0542\n",
            "     90        0.6318  0.0444\n",
            "     91        \u001b[36m0.6217\u001b[0m  0.0532\n",
            "     92        0.6607  0.0503\n",
            "     93        0.6418  0.0490\n",
            "     94        0.6382  0.0494\n",
            "     95        0.6357  0.0488\n",
            "     96        0.6333  0.0452\n",
            "     97        0.6314  0.0456\n",
            "     98        0.6298  0.0441\n",
            "     99        0.6284  0.0464\n",
            "    100        0.6272  0.0432\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.9011\u001b[0m  0.0449\n",
            "      2        \u001b[36m2.7699\u001b[0m  0.0523\n",
            "      3        \u001b[36m2.6397\u001b[0m  0.0456\n",
            "      4        \u001b[36m2.5065\u001b[0m  0.0449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m2.3719\u001b[0m  0.0458\n",
            "      6        \u001b[36m2.2365\u001b[0m  0.0479\n",
            "      7        \u001b[36m2.0996\u001b[0m  0.0454\n",
            "      8        \u001b[36m1.9484\u001b[0m  0.0489\n",
            "      9        \u001b[36m1.6858\u001b[0m  0.0599\n",
            "     10        \u001b[36m1.1956\u001b[0m  0.0518\n",
            "     11        \u001b[36m0.8452\u001b[0m  0.0548\n",
            "     12        \u001b[36m0.7213\u001b[0m  0.0487\n",
            "     13        \u001b[36m0.6804\u001b[0m  0.0460\n",
            "     14        \u001b[36m0.6743\u001b[0m  0.0431\n",
            "     15        \u001b[36m0.6728\u001b[0m  0.0460\n",
            "     16        \u001b[36m0.6718\u001b[0m  0.0451\n",
            "     17        \u001b[36m0.6711\u001b[0m  0.0443\n",
            "     18        \u001b[36m0.6706\u001b[0m  0.0510\n",
            "     19        \u001b[36m0.6702\u001b[0m  0.0478\n",
            "     20        \u001b[36m0.6699\u001b[0m  0.0636\n",
            "     21        \u001b[36m0.6696\u001b[0m  0.0456\n",
            "     22        \u001b[36m0.6694\u001b[0m  0.0441\n",
            "     23        \u001b[36m0.6692\u001b[0m  0.0457\n",
            "     24        \u001b[36m0.6691\u001b[0m  0.0439\n",
            "     25        \u001b[36m0.6690\u001b[0m  0.0463\n",
            "     26        \u001b[36m0.6689\u001b[0m  0.0465\n",
            "     27        \u001b[36m0.6688\u001b[0m  0.0490\n",
            "     28        \u001b[36m0.6687\u001b[0m  0.0467\n",
            "     29        \u001b[36m0.6686\u001b[0m  0.0515\n",
            "     30        \u001b[36m0.6685\u001b[0m  0.0469\n",
            "     31        \u001b[36m0.6685\u001b[0m  0.0470\n",
            "     32        \u001b[36m0.6684\u001b[0m  0.0567\n",
            "     33        \u001b[36m0.6683\u001b[0m  0.0508\n",
            "     34        \u001b[36m0.6683\u001b[0m  0.0478\n",
            "     35        \u001b[36m0.6682\u001b[0m  0.0468\n",
            "     36        \u001b[36m0.6682\u001b[0m  0.0496\n",
            "     37        \u001b[36m0.6681\u001b[0m  0.0429\n",
            "     38        0.6682  0.0416\n",
            "     39        0.6682  0.0522\n",
            "     40        0.6681  0.0434\n",
            "     41        \u001b[36m0.6680\u001b[0m  0.0435\n",
            "     42        \u001b[36m0.6572\u001b[0m  0.0418\n",
            "     43        0.6736  0.0469\n",
            "     44        0.6723  0.0445\n",
            "     45        0.6704  0.0419\n",
            "     46        0.6632  0.0571\n",
            "     47        \u001b[36m0.6458\u001b[0m  0.0495\n",
            "     48        \u001b[36m0.6286\u001b[0m  0.0494\n",
            "     49        0.6382  0.0614\n",
            "     50        \u001b[36m0.6189\u001b[0m  0.0524\n",
            "     51        \u001b[36m0.6167\u001b[0m  0.0489\n",
            "     52        0.6239  0.0592\n",
            "     53        \u001b[36m0.6137\u001b[0m  0.0483\n",
            "     54        \u001b[36m0.5978\u001b[0m  0.0521\n",
            "     55        \u001b[36m0.5853\u001b[0m  0.0454\n",
            "     56        \u001b[36m0.5850\u001b[0m  0.0439\n",
            "     57        0.6877  0.0421\n",
            "     58        0.6692  0.0421\n",
            "     59        0.6607  0.0497\n",
            "     60        0.6565  0.0466\n",
            "     61        0.6539  0.0505\n",
            "     62        0.6537  0.0506\n",
            "     63        0.6032  0.0443\n",
            "     64        \u001b[36m0.5734\u001b[0m  0.0455\n",
            "     65        0.6427  0.0475\n",
            "     66        0.6132  0.0482\n",
            "     67        0.6025  0.0457\n",
            "     68        0.5922  0.0449\n",
            "     69        0.5885  0.0591\n",
            "     70        0.5770  0.0563\n",
            "     71        \u001b[36m0.5665\u001b[0m  0.0575\n",
            "     72        \u001b[36m0.5560\u001b[0m  0.0535\n",
            "     73        \u001b[36m0.5553\u001b[0m  0.0615\n",
            "     74        \u001b[36m0.5494\u001b[0m  0.0439\n",
            "     75        0.5779  0.0464\n",
            "     76        0.5552  0.0510\n",
            "     77        0.5668  0.0444\n",
            "     78        0.5696  0.0551\n",
            "     79        0.5699  0.0435\n",
            "     80        0.5694  0.0430\n",
            "     81        0.5690  0.0451\n",
            "     82        0.5687  0.0514\n",
            "     83        0.5684  0.0476\n",
            "     84        0.5682  0.0477\n",
            "     85        0.5679  0.0452\n",
            "     86        0.5678  0.0553\n",
            "     87        0.5684  0.0497\n",
            "     88        0.5680  0.0584\n",
            "     89        0.5676  0.0534\n",
            "     90        0.5672  0.0544\n",
            "     91        0.5668  0.0482\n",
            "     92        0.5664  0.0520\n",
            "     93        0.5661  0.0453\n",
            "     94        0.5658  0.0416\n",
            "     95        0.5654  0.0449\n",
            "     96        0.5651  0.0489\n",
            "     97        0.5667  0.0478\n",
            "     98        0.5656  0.0458\n",
            "     99        0.5655  0.0429\n",
            "    100        0.5654  0.0430\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m3.4318\u001b[0m  0.0290\n",
            "      2        \u001b[36m3.3617\u001b[0m  0.0349\n",
            "      3        \u001b[36m3.2916\u001b[0m  0.0376\n",
            "      4        \u001b[36m3.2217\u001b[0m  0.0371\n",
            "      5        \u001b[36m3.1516\u001b[0m  0.0345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m3.0815\u001b[0m  0.0367\n",
            "      7        \u001b[36m3.0115\u001b[0m  0.0348\n",
            "      8        \u001b[36m2.9415\u001b[0m  0.0334\n",
            "      9        \u001b[36m2.8716\u001b[0m  0.0368\n",
            "     10        \u001b[36m2.8017\u001b[0m  0.0467\n",
            "     11        \u001b[36m2.7318\u001b[0m  0.0364\n",
            "     12        \u001b[36m2.6619\u001b[0m  0.0341\n",
            "     13        \u001b[36m2.5921\u001b[0m  0.0342\n",
            "     14        \u001b[36m2.5224\u001b[0m  0.0378\n",
            "     15        \u001b[36m2.4528\u001b[0m  0.0347\n",
            "     16        \u001b[36m2.3832\u001b[0m  0.0340\n",
            "     17        \u001b[36m2.3138\u001b[0m  0.0400\n",
            "     18        \u001b[36m2.2445\u001b[0m  0.0494\n",
            "     19        \u001b[36m2.1754\u001b[0m  0.0379\n",
            "     20        \u001b[36m2.1065\u001b[0m  0.0323\n",
            "     21        \u001b[36m2.0379\u001b[0m  0.0349\n",
            "     22        \u001b[36m1.9695\u001b[0m  0.0335\n",
            "     23        \u001b[36m1.9015\u001b[0m  0.0361\n",
            "     24        \u001b[36m1.8339\u001b[0m  0.0346\n",
            "     25        \u001b[36m1.7669\u001b[0m  0.0296\n",
            "     26        \u001b[36m1.7004\u001b[0m  0.0308\n",
            "     27        \u001b[36m1.6346\u001b[0m  0.0409\n",
            "     28        \u001b[36m1.5697\u001b[0m  0.0342\n",
            "     29        \u001b[36m1.5058\u001b[0m  0.0377\n",
            "     30        \u001b[36m1.4430\u001b[0m  0.0371\n",
            "     31        \u001b[36m1.3815\u001b[0m  0.0373\n",
            "     32        \u001b[36m1.3216\u001b[0m  0.0389\n",
            "     33        \u001b[36m1.2634\u001b[0m  0.0368\n",
            "     34        \u001b[36m1.2073\u001b[0m  0.0407\n",
            "     35        \u001b[36m1.1534\u001b[0m  0.0373\n",
            "     36        \u001b[36m1.1020\u001b[0m  0.0380\n",
            "     37        \u001b[36m1.0534\u001b[0m  0.0384\n",
            "     38        \u001b[36m1.0077\u001b[0m  0.0365\n",
            "     39        \u001b[36m0.9652\u001b[0m  0.0367\n",
            "     40        \u001b[36m0.9260\u001b[0m  0.0366\n",
            "     41        \u001b[36m0.8902\u001b[0m  0.0358\n",
            "     42        \u001b[36m0.8578\u001b[0m  0.0329\n",
            "     43        \u001b[36m0.8288\u001b[0m  0.0349\n",
            "     44        \u001b[36m0.8031\u001b[0m  0.0343\n",
            "     45        \u001b[36m0.7805\u001b[0m  0.0354\n",
            "     46        \u001b[36m0.7609\u001b[0m  0.0324\n",
            "     47        \u001b[36m0.7441\u001b[0m  0.0312\n",
            "     48        \u001b[36m0.7297\u001b[0m  0.0319\n",
            "     49        \u001b[36m0.7176\u001b[0m  0.0392\n",
            "     50        \u001b[36m0.7074\u001b[0m  0.0344\n",
            "     51        \u001b[36m0.6988\u001b[0m  0.0352\n",
            "     52        \u001b[36m0.6918\u001b[0m  0.0332\n",
            "     53        \u001b[36m0.6860\u001b[0m  0.0352\n",
            "     54        \u001b[36m0.6812\u001b[0m  0.0341\n",
            "     55        \u001b[36m0.6773\u001b[0m  0.0379\n",
            "     56        \u001b[36m0.6742\u001b[0m  0.0369\n",
            "     57        \u001b[36m0.6716\u001b[0m  0.0383\n",
            "     58        \u001b[36m0.6695\u001b[0m  0.0360\n",
            "     59        \u001b[36m0.6679\u001b[0m  0.0338\n",
            "     60        \u001b[36m0.6665\u001b[0m  0.0339\n",
            "     61        \u001b[36m0.6655\u001b[0m  0.0371\n",
            "     62        \u001b[36m0.6646\u001b[0m  0.0414\n",
            "     63        \u001b[36m0.6639\u001b[0m  0.0408\n",
            "     64        \u001b[36m0.6634\u001b[0m  0.0366\n",
            "     65        \u001b[36m0.6630\u001b[0m  0.0414\n",
            "     66        \u001b[36m0.6626\u001b[0m  0.0371\n",
            "     67        \u001b[36m0.6623\u001b[0m  0.0344\n",
            "     68        \u001b[36m0.6621\u001b[0m  0.0340\n",
            "     69        \u001b[36m0.6620\u001b[0m  0.0332\n",
            "     70        \u001b[36m0.6618\u001b[0m  0.0329\n",
            "     71        \u001b[36m0.6617\u001b[0m  0.0396\n",
            "     72        \u001b[36m0.6616\u001b[0m  0.0418\n",
            "     73        \u001b[36m0.6616\u001b[0m  0.0373\n",
            "     74        \u001b[36m0.6615\u001b[0m  0.0289\n",
            "     75        \u001b[36m0.6615\u001b[0m  0.0293\n",
            "     76        \u001b[36m0.6615\u001b[0m  0.0353\n",
            "     77        \u001b[36m0.6614\u001b[0m  0.0314\n",
            "     78        \u001b[36m0.6614\u001b[0m  0.0310\n",
            "     79        \u001b[36m0.6614\u001b[0m  0.0338\n",
            "     80        \u001b[36m0.6614\u001b[0m  0.0335\n",
            "     81        \u001b[36m0.6614\u001b[0m  0.0370\n",
            "     82        \u001b[36m0.6614\u001b[0m  0.0370\n",
            "     83        \u001b[36m0.6614\u001b[0m  0.0381\n",
            "     84        \u001b[36m0.6614\u001b[0m  0.0377\n",
            "     85        \u001b[36m0.6614\u001b[0m  0.0386\n",
            "     86        \u001b[36m0.6614\u001b[0m  0.0364\n",
            "     87        \u001b[36m0.6614\u001b[0m  0.0369\n",
            "     88        \u001b[36m0.6614\u001b[0m  0.0426\n",
            "     89        \u001b[36m0.6614\u001b[0m  0.0409\n",
            "     90        \u001b[36m0.6614\u001b[0m  0.0348\n",
            "     91        0.6614  0.0377\n",
            "     92        0.6614  0.0354\n",
            "     93        0.6614  0.0347\n",
            "     94        0.6614  0.0399\n",
            "     95        0.6614  0.0357\n",
            "     96        0.6614  0.0373\n",
            "     97        0.6614  0.0394\n",
            "     98        0.6614  0.0354\n",
            "     99        0.6614  0.0352\n",
            "    100        0.6614  0.0337\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.9061\u001b[0m  0.0299\n",
            "      2        \u001b[36m2.8392\u001b[0m  0.0351\n",
            "      3        \u001b[36m2.7724\u001b[0m  0.0335\n",
            "      4        \u001b[36m2.7056\u001b[0m  0.0343\n",
            "      5        \u001b[36m2.6389\u001b[0m  0.0398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m2.5722\u001b[0m  0.0392\n",
            "      7        \u001b[36m2.5055\u001b[0m  0.0353\n",
            "      8        \u001b[36m2.4390\u001b[0m  0.0374\n",
            "      9        \u001b[36m2.3725\u001b[0m  0.0435\n",
            "     10        \u001b[36m2.3062\u001b[0m  0.0389\n",
            "     11        \u001b[36m2.2400\u001b[0m  0.0338\n",
            "     12        \u001b[36m2.1739\u001b[0m  0.0349\n",
            "     13        \u001b[36m2.1081\u001b[0m  0.0368\n",
            "     14        \u001b[36m2.0425\u001b[0m  0.0502\n",
            "     15        \u001b[36m1.9771\u001b[0m  0.0341\n",
            "     16        \u001b[36m1.9121\u001b[0m  0.0368\n",
            "     17        \u001b[36m1.8474\u001b[0m  0.0327\n",
            "     18        \u001b[36m1.7832\u001b[0m  0.0350\n",
            "     19        \u001b[36m1.7196\u001b[0m  0.0378\n",
            "     20        \u001b[36m1.6565\u001b[0m  0.0320\n",
            "     21        \u001b[36m1.5942\u001b[0m  0.0344\n",
            "     22        \u001b[36m1.5327\u001b[0m  0.0339\n",
            "     23        \u001b[36m1.4723\u001b[0m  0.0331\n",
            "     24        \u001b[36m1.4130\u001b[0m  0.0417\n",
            "     25        \u001b[36m1.3550\u001b[0m  0.0337\n",
            "     26        \u001b[36m1.2986\u001b[0m  0.0339\n",
            "     27        \u001b[36m1.2438\u001b[0m  0.0359\n",
            "     28        \u001b[36m1.1910\u001b[0m  0.0322\n",
            "     29        \u001b[36m1.1404\u001b[0m  0.0326\n",
            "     30        \u001b[36m1.0921\u001b[0m  0.0323\n",
            "     31        \u001b[36m1.0464\u001b[0m  0.0363\n",
            "     32        \u001b[36m1.0034\u001b[0m  0.0363\n",
            "     33        \u001b[36m0.9634\u001b[0m  0.0381\n",
            "     34        \u001b[36m0.9264\u001b[0m  0.0360\n",
            "     35        \u001b[36m0.8925\u001b[0m  0.0378\n",
            "     36        \u001b[36m0.8617\u001b[0m  0.0338\n",
            "     37        \u001b[36m0.8340\u001b[0m  0.0338\n",
            "     38        \u001b[36m0.8093\u001b[0m  0.0325\n",
            "     39        \u001b[36m0.7875\u001b[0m  0.0392\n",
            "     40        \u001b[36m0.7684\u001b[0m  0.0379\n",
            "     41        \u001b[36m0.7518\u001b[0m  0.0494\n",
            "     42        \u001b[36m0.7375\u001b[0m  0.0347\n",
            "     43        \u001b[36m0.7252\u001b[0m  0.0323\n",
            "     44        \u001b[36m0.7147\u001b[0m  0.0398\n",
            "     45        \u001b[36m0.7059\u001b[0m  0.0352\n",
            "     46        \u001b[36m0.6984\u001b[0m  0.0349\n",
            "     47        \u001b[36m0.6922\u001b[0m  0.0344\n",
            "     48        \u001b[36m0.6870\u001b[0m  0.0378\n",
            "     49        \u001b[36m0.6826\u001b[0m  0.0357\n",
            "     50        \u001b[36m0.6790\u001b[0m  0.0306\n",
            "     51        \u001b[36m0.6760\u001b[0m  0.0330\n",
            "     52        \u001b[36m0.6735\u001b[0m  0.0329\n",
            "     53        \u001b[36m0.6714\u001b[0m  0.0321\n",
            "     54        \u001b[36m0.6697\u001b[0m  0.0338\n",
            "     55        \u001b[36m0.6683\u001b[0m  0.0322\n",
            "     56        \u001b[36m0.6671\u001b[0m  0.0362\n",
            "     57        \u001b[36m0.6662\u001b[0m  0.0360\n",
            "     58        \u001b[36m0.6654\u001b[0m  0.0349\n",
            "     59        \u001b[36m0.6647\u001b[0m  0.0345\n",
            "     60        \u001b[36m0.6641\u001b[0m  0.0355\n",
            "     61        \u001b[36m0.6637\u001b[0m  0.0369\n",
            "     62        \u001b[36m0.6633\u001b[0m  0.0342\n",
            "     63        \u001b[36m0.6630\u001b[0m  0.0343\n",
            "     64        \u001b[36m0.6627\u001b[0m  0.0345\n",
            "     65        \u001b[36m0.6625\u001b[0m  0.0336\n",
            "     66        \u001b[36m0.6623\u001b[0m  0.0374\n",
            "     67        \u001b[36m0.6621\u001b[0m  0.0310\n",
            "     68        \u001b[36m0.6620\u001b[0m  0.0454\n",
            "     69        \u001b[36m0.6618\u001b[0m  0.0322\n",
            "     70        \u001b[36m0.6617\u001b[0m  0.0285\n",
            "     71        \u001b[36m0.6616\u001b[0m  0.0359\n",
            "     72        \u001b[36m0.6616\u001b[0m  0.0366\n",
            "     73        \u001b[36m0.6615\u001b[0m  0.0348\n",
            "     74        \u001b[36m0.6614\u001b[0m  0.0336\n",
            "     75        \u001b[36m0.6614\u001b[0m  0.0315\n",
            "     76        \u001b[36m0.6614\u001b[0m  0.0335\n",
            "     77        \u001b[36m0.6613\u001b[0m  0.0330\n",
            "     78        \u001b[36m0.6613\u001b[0m  0.0305\n",
            "     79        \u001b[36m0.6613\u001b[0m  0.0323\n",
            "     80        \u001b[36m0.6612\u001b[0m  0.0310\n",
            "     81        \u001b[36m0.6612\u001b[0m  0.0348\n",
            "     82        \u001b[36m0.6612\u001b[0m  0.0357\n",
            "     83        \u001b[36m0.6612\u001b[0m  0.0342\n",
            "     84        \u001b[36m0.6612\u001b[0m  0.0341\n",
            "     85        \u001b[36m0.6612\u001b[0m  0.0372\n",
            "     86        \u001b[36m0.6611\u001b[0m  0.0337\n",
            "     87        \u001b[36m0.6611\u001b[0m  0.0399\n",
            "     88        \u001b[36m0.6611\u001b[0m  0.1141\n",
            "     89        \u001b[36m0.6611\u001b[0m  0.1885\n",
            "     90        \u001b[36m0.6611\u001b[0m  0.0994\n",
            "     91        \u001b[36m0.6611\u001b[0m  0.0427\n",
            "     92        \u001b[36m0.6611\u001b[0m  0.0561\n",
            "     93        \u001b[36m0.6611\u001b[0m  0.0765\n",
            "     94        \u001b[36m0.6611\u001b[0m  0.0390\n",
            "     95        \u001b[36m0.6611\u001b[0m  0.0324\n",
            "     96        \u001b[36m0.6611\u001b[0m  0.0347\n",
            "     97        \u001b[36m0.6611\u001b[0m  0.0367\n",
            "     98        \u001b[36m0.6611\u001b[0m  0.0368\n",
            "     99        \u001b[36m0.6611\u001b[0m  0.0349\n",
            "    100        \u001b[36m0.6611\u001b[0m  0.0358\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.2179\u001b[0m  0.0448\n",
            "      2        \u001b[36m0.9876\u001b[0m  0.0511\n",
            "      3        \u001b[36m0.8024\u001b[0m  0.0428\n",
            "      4        \u001b[36m0.6865\u001b[0m  0.0479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.6479\u001b[0m  0.0569\n",
            "      6        \u001b[36m0.6403\u001b[0m  0.0482\n",
            "      7        \u001b[36m0.6391\u001b[0m  0.0480\n",
            "      8        \u001b[36m0.6388\u001b[0m  0.0582\n",
            "      9        \u001b[36m0.6376\u001b[0m  0.0435\n",
            "     10        \u001b[36m0.6363\u001b[0m  0.0460\n",
            "     11        \u001b[36m0.6352\u001b[0m  0.0443\n",
            "     12        \u001b[36m0.6340\u001b[0m  0.0505\n",
            "     13        \u001b[36m0.6325\u001b[0m  0.0441\n",
            "     14        \u001b[36m0.6296\u001b[0m  0.0454\n",
            "     15        \u001b[36m0.6266\u001b[0m  0.0440\n",
            "     16        \u001b[36m0.6256\u001b[0m  0.0456\n",
            "     17        \u001b[36m0.6247\u001b[0m  0.0499\n",
            "     18        \u001b[36m0.6238\u001b[0m  0.0461\n",
            "     19        \u001b[36m0.6229\u001b[0m  0.0424\n",
            "     20        \u001b[36m0.6220\u001b[0m  0.0447\n",
            "     21        \u001b[36m0.6212\u001b[0m  0.0445\n",
            "     22        \u001b[36m0.6203\u001b[0m  0.0532\n",
            "     23        \u001b[36m0.6195\u001b[0m  0.0491\n",
            "     24        \u001b[36m0.6187\u001b[0m  0.0605\n",
            "     25        \u001b[36m0.6180\u001b[0m  0.0606\n",
            "     26        \u001b[36m0.6173\u001b[0m  0.0501\n",
            "     27        \u001b[36m0.6165\u001b[0m  0.0492\n",
            "     28        \u001b[36m0.6159\u001b[0m  0.0664\n",
            "     29        \u001b[36m0.6081\u001b[0m  0.0626\n",
            "     30        \u001b[36m0.6054\u001b[0m  0.0457\n",
            "     31        \u001b[36m0.6028\u001b[0m  0.0443\n",
            "     32        0.6041  0.0463\n",
            "     33        \u001b[36m0.6015\u001b[0m  0.0435\n",
            "     34        \u001b[36m0.6010\u001b[0m  0.0482\n",
            "     35        \u001b[36m0.6005\u001b[0m  0.0457\n",
            "     36        \u001b[36m0.6001\u001b[0m  0.0426\n",
            "     37        \u001b[36m0.5996\u001b[0m  0.0484\n",
            "     38        \u001b[36m0.5992\u001b[0m  0.0471\n",
            "     39        \u001b[36m0.5988\u001b[0m  0.0446\n",
            "     40        \u001b[36m0.5984\u001b[0m  0.0467\n",
            "     41        \u001b[36m0.5980\u001b[0m  0.0485\n",
            "     42        \u001b[36m0.5976\u001b[0m  0.0474\n",
            "     43        \u001b[36m0.5971\u001b[0m  0.0549\n",
            "     44        \u001b[36m0.5967\u001b[0m  0.0537\n",
            "     45        \u001b[36m0.5964\u001b[0m  0.0490\n",
            "     46        \u001b[36m0.5961\u001b[0m  0.0464\n",
            "     47        \u001b[36m0.5958\u001b[0m  0.0506\n",
            "     48        \u001b[36m0.5954\u001b[0m  0.0438\n",
            "     49        \u001b[36m0.5951\u001b[0m  0.0462\n",
            "     50        \u001b[36m0.5948\u001b[0m  0.0455\n",
            "     51        0.5957  0.0469\n",
            "     52        \u001b[36m0.5931\u001b[0m  0.0467\n",
            "     53        \u001b[36m0.5925\u001b[0m  0.0467\n",
            "     54        \u001b[36m0.5922\u001b[0m  0.0431\n",
            "     55        \u001b[36m0.5919\u001b[0m  0.0489\n",
            "     56        \u001b[36m0.5917\u001b[0m  0.0466\n",
            "     57        \u001b[36m0.5914\u001b[0m  0.0510\n",
            "     58        \u001b[36m0.5911\u001b[0m  0.0602\n",
            "     59        \u001b[36m0.5909\u001b[0m  0.0499\n",
            "     60        \u001b[36m0.5906\u001b[0m  0.0511\n",
            "     61        \u001b[36m0.5904\u001b[0m  0.0494\n",
            "     62        \u001b[36m0.5901\u001b[0m  0.0496\n",
            "     63        \u001b[36m0.5899\u001b[0m  0.0505\n",
            "     64        \u001b[36m0.5897\u001b[0m  0.0617\n",
            "     65        \u001b[36m0.5894\u001b[0m  0.0504\n",
            "     66        \u001b[36m0.5892\u001b[0m  0.0616\n",
            "     67        \u001b[36m0.5890\u001b[0m  0.0456\n",
            "     68        \u001b[36m0.5888\u001b[0m  0.0459\n",
            "     69        \u001b[36m0.5885\u001b[0m  0.0522\n",
            "     70        \u001b[36m0.5883\u001b[0m  0.0413\n",
            "     71        \u001b[36m0.5881\u001b[0m  0.0431\n",
            "     72        \u001b[36m0.5879\u001b[0m  0.0471\n",
            "     73        \u001b[36m0.5877\u001b[0m  0.0453\n",
            "     74        \u001b[36m0.5875\u001b[0m  0.0468\n",
            "     75        \u001b[36m0.5873\u001b[0m  0.0500\n",
            "     76        \u001b[36m0.5871\u001b[0m  0.0473\n",
            "     77        \u001b[36m0.5869\u001b[0m  0.0567\n",
            "     78        \u001b[36m0.5868\u001b[0m  0.0448\n",
            "     79        \u001b[36m0.5866\u001b[0m  0.0462\n",
            "     80        \u001b[36m0.5864\u001b[0m  0.0445\n",
            "     81        \u001b[36m0.5862\u001b[0m  0.0507\n",
            "     82        \u001b[36m0.5860\u001b[0m  0.0538\n",
            "     83        \u001b[36m0.5859\u001b[0m  0.0530\n",
            "     84        \u001b[36m0.5857\u001b[0m  0.0517\n",
            "     85        \u001b[36m0.5855\u001b[0m  0.0504\n",
            "     86        \u001b[36m0.5854\u001b[0m  0.0553\n",
            "     87        \u001b[36m0.5852\u001b[0m  0.0480\n",
            "     88        \u001b[36m0.5730\u001b[0m  0.0506\n",
            "     89        0.6772  0.0437\n",
            "     90        0.5785  0.0443\n",
            "     91        \u001b[36m0.5680\u001b[0m  0.0453\n",
            "     92        0.5680  0.0457\n",
            "     93        \u001b[36m0.5680\u001b[0m  0.0465\n",
            "     94        \u001b[36m0.5679\u001b[0m  0.0471\n",
            "     95        \u001b[36m0.5678\u001b[0m  0.0489\n",
            "     96        \u001b[36m0.5676\u001b[0m  0.0497\n",
            "     97        \u001b[36m0.5675\u001b[0m  0.0459\n",
            "     98        \u001b[36m0.5674\u001b[0m  0.0435\n",
            "     99        \u001b[36m0.5673\u001b[0m  0.0487\n",
            "    100        \u001b[36m0.5671\u001b[0m  0.0505\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9634\u001b[0m  0.0473\n",
            "      2        \u001b[36m0.9129\u001b[0m  0.0524\n",
            "      3        \u001b[36m0.8920\u001b[0m  0.0571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.8745\u001b[0m  0.0600\n",
            "      5        \u001b[36m0.8593\u001b[0m  0.0441\n",
            "      6        \u001b[36m0.8381\u001b[0m  0.0543\n",
            "      7        \u001b[36m0.8207\u001b[0m  0.0439\n",
            "      8        \u001b[36m0.8017\u001b[0m  0.0441\n",
            "      9        \u001b[36m0.7740\u001b[0m  0.0466\n",
            "     10        \u001b[36m0.7598\u001b[0m  0.0447\n",
            "     11        \u001b[36m0.7474\u001b[0m  0.0414\n",
            "     12        \u001b[36m0.7352\u001b[0m  0.0439\n",
            "     13        \u001b[36m0.7247\u001b[0m  0.0449\n",
            "     14        \u001b[36m0.7150\u001b[0m  0.0449\n",
            "     15        \u001b[36m0.7141\u001b[0m  0.0456\n",
            "     16        \u001b[36m0.7062\u001b[0m  0.0538\n",
            "     17        \u001b[36m0.6929\u001b[0m  0.0498\n",
            "     18        \u001b[36m0.6818\u001b[0m  0.0467\n",
            "     19        \u001b[36m0.6768\u001b[0m  0.0490\n",
            "     20        \u001b[36m0.6722\u001b[0m  0.0512\n",
            "     21        \u001b[36m0.6660\u001b[0m  0.0498\n",
            "     22        \u001b[36m0.6621\u001b[0m  0.0531\n",
            "     23        \u001b[36m0.6562\u001b[0m  0.0550\n",
            "     24        0.6592  0.0502\n",
            "     25        0.6565  0.0452\n",
            "     26        \u001b[36m0.6467\u001b[0m  0.0547\n",
            "     27        \u001b[36m0.6442\u001b[0m  0.0469\n",
            "     28        0.6487  0.0539\n",
            "     29        \u001b[36m0.6405\u001b[0m  0.0437\n",
            "     30        0.6433  0.0538\n",
            "     31        0.6447  0.0453\n",
            "     32        0.6443  0.0439\n",
            "     33        0.6415  0.0460\n",
            "     34        0.6409  0.0457\n",
            "     35        \u001b[36m0.6393\u001b[0m  0.0432\n",
            "     36        \u001b[36m0.6381\u001b[0m  0.0471\n",
            "     37        \u001b[36m0.6373\u001b[0m  0.0465\n",
            "     38        \u001b[36m0.6358\u001b[0m  0.0485\n",
            "     39        \u001b[36m0.6345\u001b[0m  0.0471\n",
            "     40        \u001b[36m0.6333\u001b[0m  0.0513\n",
            "     41        \u001b[36m0.6321\u001b[0m  0.0665\n",
            "     42        \u001b[36m0.6310\u001b[0m  0.0450\n",
            "     43        \u001b[36m0.6300\u001b[0m  0.0487\n",
            "     44        \u001b[36m0.6290\u001b[0m  0.0504\n",
            "     45        \u001b[36m0.6280\u001b[0m  0.0489\n",
            "     46        \u001b[36m0.6272\u001b[0m  0.0500\n",
            "     47        \u001b[36m0.6263\u001b[0m  0.0428\n",
            "     48        \u001b[36m0.6255\u001b[0m  0.0430\n",
            "     49        \u001b[36m0.6216\u001b[0m  0.0517\n",
            "     50        \u001b[36m0.6206\u001b[0m  0.0391\n",
            "     51        0.6209  0.0448\n",
            "     52        \u001b[36m0.6199\u001b[0m  0.0501\n",
            "     53        \u001b[36m0.6189\u001b[0m  0.0452\n",
            "     54        \u001b[36m0.6180\u001b[0m  0.0463\n",
            "     55        0.6181  0.0434\n",
            "     56        0.6189  0.0441\n",
            "     57        \u001b[36m0.6169\u001b[0m  0.0466\n",
            "     58        \u001b[36m0.6160\u001b[0m  0.0472\n",
            "     59        0.6162  0.0522\n",
            "     60        \u001b[36m0.6142\u001b[0m  0.0562\n",
            "     61        \u001b[36m0.6133\u001b[0m  0.0478\n",
            "     62        \u001b[36m0.6125\u001b[0m  0.0485\n",
            "     63        \u001b[36m0.6117\u001b[0m  0.0498\n",
            "     64        \u001b[36m0.6109\u001b[0m  0.0428\n",
            "     65        \u001b[36m0.6102\u001b[0m  0.0475\n",
            "     66        \u001b[36m0.6095\u001b[0m  0.0446\n",
            "     67        \u001b[36m0.6087\u001b[0m  0.0513\n",
            "     68        \u001b[36m0.6080\u001b[0m  0.0418\n",
            "     69        \u001b[36m0.6074\u001b[0m  0.0435\n",
            "     70        \u001b[36m0.6067\u001b[0m  0.0443\n",
            "     71        \u001b[36m0.6057\u001b[0m  0.0469\n",
            "     72        \u001b[36m0.6050\u001b[0m  0.0439\n",
            "     73        \u001b[36m0.6044\u001b[0m  0.0459\n",
            "     74        \u001b[36m0.6038\u001b[0m  0.0458\n",
            "     75        \u001b[36m0.6032\u001b[0m  0.0478\n",
            "     76        \u001b[36m0.6019\u001b[0m  0.0596\n",
            "     77        \u001b[36m0.6013\u001b[0m  0.0476\n",
            "     78        \u001b[36m0.6007\u001b[0m  0.0461\n",
            "     79        \u001b[36m0.6002\u001b[0m  0.0448\n",
            "     80        \u001b[36m0.5996\u001b[0m  0.0532\n",
            "     81        0.6062  0.0474\n",
            "     82        0.6063  0.0515\n",
            "     83        0.6051  0.0604\n",
            "     84        0.6040  0.0458\n",
            "     85        0.6031  0.0421\n",
            "     86        0.6022  0.0431\n",
            "     87        0.6014  0.0579\n",
            "     88        0.6006  0.0453\n",
            "     89        0.5999  0.0436\n",
            "     90        \u001b[36m0.5993\u001b[0m  0.0462\n",
            "     91        \u001b[36m0.5929\u001b[0m  0.0465\n",
            "     92        0.5947  0.0485\n",
            "     93        0.5943  0.0443\n",
            "     94        0.5939  0.0443\n",
            "     95        0.5935  0.0480\n",
            "     96        0.5931  0.0438\n",
            "     97        \u001b[36m0.5928\u001b[0m  0.0482\n",
            "     98        \u001b[36m0.5925\u001b[0m  0.0453\n",
            "     99        \u001b[36m0.5852\u001b[0m  0.0491\n",
            "    100        0.6071  0.0493\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.4214\u001b[0m  0.0298\n",
            "      2        \u001b[36m1.3371\u001b[0m  0.0352\n",
            "      3        \u001b[36m1.2674\u001b[0m  0.0383\n",
            "      4        \u001b[36m1.2075\u001b[0m  0.0344\n",
            "      5        \u001b[36m1.1565\u001b[0m  0.0337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m1.1131\u001b[0m  0.0371\n",
            "      7        \u001b[36m1.0764\u001b[0m  0.0408\n",
            "      8        \u001b[36m1.0453\u001b[0m  0.0325\n",
            "      9        \u001b[36m1.0188\u001b[0m  0.0426\n",
            "     10        \u001b[36m0.9962\u001b[0m  0.0324\n",
            "     11        \u001b[36m0.9769\u001b[0m  0.0350\n",
            "     12        \u001b[36m0.9602\u001b[0m  0.0391\n",
            "     13        \u001b[36m0.9458\u001b[0m  0.0312\n",
            "     14        \u001b[36m0.9332\u001b[0m  0.0332\n",
            "     15        \u001b[36m0.9222\u001b[0m  0.0333\n",
            "     16        \u001b[36m0.9126\u001b[0m  0.0374\n",
            "     17        \u001b[36m0.9040\u001b[0m  0.0296\n",
            "     18        \u001b[36m0.8963\u001b[0m  0.0308\n",
            "     19        \u001b[36m0.8894\u001b[0m  0.0441\n",
            "     20        \u001b[36m0.8832\u001b[0m  0.0339\n",
            "     21        \u001b[36m0.8776\u001b[0m  0.0347\n",
            "     22        \u001b[36m0.8725\u001b[0m  0.0352\n",
            "     23        \u001b[36m0.8677\u001b[0m  0.0359\n",
            "     24        \u001b[36m0.8634\u001b[0m  0.0373\n",
            "     25        \u001b[36m0.8593\u001b[0m  0.0365\n",
            "     26        \u001b[36m0.8555\u001b[0m  0.0384\n",
            "     27        \u001b[36m0.8520\u001b[0m  0.0352\n",
            "     28        \u001b[36m0.8487\u001b[0m  0.0398\n",
            "     29        \u001b[36m0.8455\u001b[0m  0.0486\n",
            "     30        \u001b[36m0.8425\u001b[0m  0.0385\n",
            "     31        \u001b[36m0.8396\u001b[0m  0.0331\n",
            "     32        \u001b[36m0.8369\u001b[0m  0.0320\n",
            "     33        \u001b[36m0.8342\u001b[0m  0.0349\n",
            "     34        \u001b[36m0.8317\u001b[0m  0.0341\n",
            "     35        \u001b[36m0.8292\u001b[0m  0.0342\n",
            "     36        \u001b[36m0.8268\u001b[0m  0.0397\n",
            "     37        \u001b[36m0.8245\u001b[0m  0.0332\n",
            "     38        \u001b[36m0.8222\u001b[0m  0.0326\n",
            "     39        \u001b[36m0.8200\u001b[0m  0.0317\n",
            "     40        \u001b[36m0.8179\u001b[0m  0.0334\n",
            "     41        \u001b[36m0.8158\u001b[0m  0.0393\n",
            "     42        \u001b[36m0.8137\u001b[0m  0.0423\n",
            "     43        \u001b[36m0.8117\u001b[0m  0.0365\n",
            "     44        \u001b[36m0.8097\u001b[0m  0.0347\n",
            "     45        \u001b[36m0.8077\u001b[0m  0.0361\n",
            "     46        \u001b[36m0.8058\u001b[0m  0.0379\n",
            "     47        \u001b[36m0.8039\u001b[0m  0.0328\n",
            "     48        \u001b[36m0.8020\u001b[0m  0.0352\n",
            "     49        \u001b[36m0.8002\u001b[0m  0.0362\n",
            "     50        \u001b[36m0.7983\u001b[0m  0.0365\n",
            "     51        \u001b[36m0.7965\u001b[0m  0.0376\n",
            "     52        \u001b[36m0.7948\u001b[0m  0.0383\n",
            "     53        \u001b[36m0.7930\u001b[0m  0.0372\n",
            "     54        \u001b[36m0.7913\u001b[0m  0.0444\n",
            "     55        \u001b[36m0.7896\u001b[0m  0.0338\n",
            "     56        \u001b[36m0.7879\u001b[0m  0.0343\n",
            "     57        \u001b[36m0.7862\u001b[0m  0.0337\n",
            "     58        \u001b[36m0.7846\u001b[0m  0.0361\n",
            "     59        \u001b[36m0.7830\u001b[0m  0.0333\n",
            "     60        \u001b[36m0.7814\u001b[0m  0.0374\n",
            "     61        \u001b[36m0.7798\u001b[0m  0.0373\n",
            "     62        \u001b[36m0.7782\u001b[0m  0.0477\n",
            "     63        \u001b[36m0.7767\u001b[0m  0.0353\n",
            "     64        \u001b[36m0.7752\u001b[0m  0.0349\n",
            "     65        \u001b[36m0.7737\u001b[0m  0.0376\n",
            "     66        \u001b[36m0.7722\u001b[0m  0.0383\n",
            "     67        \u001b[36m0.7707\u001b[0m  0.0351\n",
            "     68        \u001b[36m0.7693\u001b[0m  0.0338\n",
            "     69        \u001b[36m0.7678\u001b[0m  0.0407\n",
            "     70        \u001b[36m0.7664\u001b[0m  0.0374\n",
            "     71        \u001b[36m0.7650\u001b[0m  0.0383\n",
            "     72        \u001b[36m0.7636\u001b[0m  0.0355\n",
            "     73        \u001b[36m0.7623\u001b[0m  0.0399\n",
            "     74        \u001b[36m0.7609\u001b[0m  0.0408\n",
            "     75        \u001b[36m0.7596\u001b[0m  0.0406\n",
            "     76        \u001b[36m0.7583\u001b[0m  0.0374\n",
            "     77        \u001b[36m0.7570\u001b[0m  0.0379\n",
            "     78        \u001b[36m0.7557\u001b[0m  0.0380\n",
            "     79        \u001b[36m0.7545\u001b[0m  0.0386\n",
            "     80        \u001b[36m0.7532\u001b[0m  0.0336\n",
            "     81        \u001b[36m0.7520\u001b[0m  0.0350\n",
            "     82        \u001b[36m0.7508\u001b[0m  0.0339\n",
            "     83        \u001b[36m0.7496\u001b[0m  0.0503\n",
            "     84        \u001b[36m0.7484\u001b[0m  0.0379\n",
            "     85        \u001b[36m0.7472\u001b[0m  0.0384\n",
            "     86        \u001b[36m0.7460\u001b[0m  0.0358\n",
            "     87        \u001b[36m0.7449\u001b[0m  0.0430\n",
            "     88        \u001b[36m0.7438\u001b[0m  0.0340\n",
            "     89        \u001b[36m0.7426\u001b[0m  0.0332\n",
            "     90        \u001b[36m0.7415\u001b[0m  0.0320\n",
            "     91        \u001b[36m0.7404\u001b[0m  0.0316\n",
            "     92        \u001b[36m0.7393\u001b[0m  0.0336\n",
            "     93        \u001b[36m0.7383\u001b[0m  0.0367\n",
            "     94        \u001b[36m0.7372\u001b[0m  0.0377\n",
            "     95        \u001b[36m0.7362\u001b[0m  0.0402\n",
            "     96        \u001b[36m0.7351\u001b[0m  0.0417\n",
            "     97        \u001b[36m0.7341\u001b[0m  0.0387\n",
            "     98        \u001b[36m0.7331\u001b[0m  0.0408\n",
            "     99        \u001b[36m0.7321\u001b[0m  0.0383\n",
            "    100        \u001b[36m0.7311\u001b[0m  0.0423\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9140\u001b[0m  0.0467\n",
            "      2        \u001b[36m0.8501\u001b[0m  0.0434\n",
            "      3        \u001b[36m0.8008\u001b[0m  0.0359\n",
            "      4        \u001b[36m0.7635\u001b[0m  0.0412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.7365\u001b[0m  0.0340\n",
            "      6        \u001b[36m0.7175\u001b[0m  0.0335\n",
            "      7        \u001b[36m0.7044\u001b[0m  0.0376\n",
            "      8        \u001b[36m0.6953\u001b[0m  0.0351\n",
            "      9        \u001b[36m0.6891\u001b[0m  0.0351\n",
            "     10        \u001b[36m0.6847\u001b[0m  0.0331\n",
            "     11        \u001b[36m0.6816\u001b[0m  0.0361\n",
            "     12        \u001b[36m0.6793\u001b[0m  0.0442\n",
            "     13        \u001b[36m0.6777\u001b[0m  0.0348\n",
            "     14        \u001b[36m0.6764\u001b[0m  0.0347\n",
            "     15        \u001b[36m0.6754\u001b[0m  0.0346\n",
            "     16        \u001b[36m0.6746\u001b[0m  0.0374\n",
            "     17        \u001b[36m0.6739\u001b[0m  0.0352\n",
            "     18        \u001b[36m0.6733\u001b[0m  0.0337\n",
            "     19        \u001b[36m0.6728\u001b[0m  0.0324\n",
            "     20        \u001b[36m0.6723\u001b[0m  0.0330\n",
            "     21        \u001b[36m0.6719\u001b[0m  0.0339\n",
            "     22        \u001b[36m0.6715\u001b[0m  0.0417\n",
            "     23        \u001b[36m0.6711\u001b[0m  0.0402\n",
            "     24        \u001b[36m0.6707\u001b[0m  0.0466\n",
            "     25        \u001b[36m0.6703\u001b[0m  0.0498\n",
            "     26        \u001b[36m0.6699\u001b[0m  0.0455\n",
            "     27        \u001b[36m0.6696\u001b[0m  0.0373\n",
            "     28        \u001b[36m0.6692\u001b[0m  0.0359\n",
            "     29        \u001b[36m0.6688\u001b[0m  0.0380\n",
            "     30        \u001b[36m0.6685\u001b[0m  0.0358\n",
            "     31        \u001b[36m0.6681\u001b[0m  0.0335\n",
            "     32        \u001b[36m0.6677\u001b[0m  0.0360\n",
            "     33        \u001b[36m0.6674\u001b[0m  0.0351\n",
            "     34        \u001b[36m0.6670\u001b[0m  0.0382\n",
            "     35        \u001b[36m0.6667\u001b[0m  0.0468\n",
            "     36        \u001b[36m0.6663\u001b[0m  0.0378\n",
            "     37        \u001b[36m0.6659\u001b[0m  0.0438\n",
            "     38        \u001b[36m0.6656\u001b[0m  0.0337\n",
            "     39        \u001b[36m0.6652\u001b[0m  0.0351\n",
            "     40        \u001b[36m0.6649\u001b[0m  0.0386\n",
            "     41        \u001b[36m0.6646\u001b[0m  0.0354\n",
            "     42        \u001b[36m0.6642\u001b[0m  0.0342\n",
            "     43        \u001b[36m0.6639\u001b[0m  0.0392\n",
            "     44        \u001b[36m0.6635\u001b[0m  0.0402\n",
            "     45        \u001b[36m0.6632\u001b[0m  0.0387\n",
            "     46        \u001b[36m0.6629\u001b[0m  0.0338\n",
            "     47        \u001b[36m0.6625\u001b[0m  0.0387\n",
            "     48        \u001b[36m0.6622\u001b[0m  0.0380\n",
            "     49        \u001b[36m0.6619\u001b[0m  0.0341\n",
            "     50        \u001b[36m0.6616\u001b[0m  0.0381\n",
            "     51        \u001b[36m0.6612\u001b[0m  0.0344\n",
            "     52        \u001b[36m0.6609\u001b[0m  0.0332\n",
            "     53        \u001b[36m0.6606\u001b[0m  0.0344\n",
            "     54        \u001b[36m0.6603\u001b[0m  0.0346\n",
            "     55        \u001b[36m0.6600\u001b[0m  0.0354\n",
            "     56        \u001b[36m0.6597\u001b[0m  0.0353\n",
            "     57        \u001b[36m0.6594\u001b[0m  0.0394\n",
            "     58        \u001b[36m0.6591\u001b[0m  0.0350\n",
            "     59        \u001b[36m0.6588\u001b[0m  0.0355\n",
            "     60        \u001b[36m0.6585\u001b[0m  0.0350\n",
            "     61        \u001b[36m0.6582\u001b[0m  0.0380\n",
            "     62        \u001b[36m0.6579\u001b[0m  0.0311\n",
            "     63        \u001b[36m0.6576\u001b[0m  0.0423\n",
            "     64        \u001b[36m0.6573\u001b[0m  0.0303\n",
            "     65        \u001b[36m0.6570\u001b[0m  0.0309\n",
            "     66        \u001b[36m0.6567\u001b[0m  0.0352\n",
            "     67        \u001b[36m0.6564\u001b[0m  0.0373\n",
            "     68        \u001b[36m0.6561\u001b[0m  0.0357\n",
            "     69        \u001b[36m0.6559\u001b[0m  0.0355\n",
            "     70        \u001b[36m0.6556\u001b[0m  0.0341\n",
            "     71        \u001b[36m0.6553\u001b[0m  0.0419\n",
            "     72        \u001b[36m0.6550\u001b[0m  0.0386\n",
            "     73        \u001b[36m0.6548\u001b[0m  0.0378\n",
            "     74        \u001b[36m0.6545\u001b[0m  0.0396\n",
            "     75        \u001b[36m0.6542\u001b[0m  0.0404\n",
            "     76        \u001b[36m0.6539\u001b[0m  0.0397\n",
            "     77        \u001b[36m0.6537\u001b[0m  0.0449\n",
            "     78        \u001b[36m0.6534\u001b[0m  0.0519\n",
            "     79        \u001b[36m0.6531\u001b[0m  0.0359\n",
            "     80        \u001b[36m0.6529\u001b[0m  0.0332\n",
            "     81        \u001b[36m0.6526\u001b[0m  0.0338\n",
            "     82        \u001b[36m0.6524\u001b[0m  0.0340\n",
            "     83        \u001b[36m0.6521\u001b[0m  0.0368\n",
            "     84        \u001b[36m0.6519\u001b[0m  0.0345\n",
            "     85        \u001b[36m0.6516\u001b[0m  0.0336\n",
            "     86        \u001b[36m0.6514\u001b[0m  0.0321\n",
            "     87        \u001b[36m0.6511\u001b[0m  0.0340\n",
            "     88        \u001b[36m0.6509\u001b[0m  0.0294\n",
            "     89        \u001b[36m0.6506\u001b[0m  0.0468\n",
            "     90        \u001b[36m0.6504\u001b[0m  0.0363\n",
            "     91        \u001b[36m0.6501\u001b[0m  0.0367\n",
            "     92        \u001b[36m0.6499\u001b[0m  0.0382\n",
            "     93        \u001b[36m0.6496\u001b[0m  0.0348\n",
            "     94        \u001b[36m0.6494\u001b[0m  0.0410\n",
            "     95        \u001b[36m0.6492\u001b[0m  0.0351\n",
            "     96        \u001b[36m0.6489\u001b[0m  0.0368\n",
            "     97        \u001b[36m0.6487\u001b[0m  0.0413\n",
            "     98        \u001b[36m0.6485\u001b[0m  0.0395\n",
            "     99        \u001b[36m0.6482\u001b[0m  0.0397\n",
            "    100        \u001b[36m0.6480\u001b[0m  0.0403\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.2498\u001b[0m  0.0410\n",
            "      2        \u001b[36m1.1229\u001b[0m  0.0552\n",
            "      3        \u001b[36m1.0335\u001b[0m  0.0472\n",
            "      4        \u001b[36m0.9496\u001b[0m  0.0450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.8809\u001b[0m  0.0502\n",
            "      6        \u001b[36m0.8302\u001b[0m  0.0459\n",
            "      7        \u001b[36m0.8015\u001b[0m  0.0460\n",
            "      8        \u001b[36m0.7777\u001b[0m  0.0511\n",
            "      9        \u001b[36m0.7512\u001b[0m  0.0450\n",
            "     10        \u001b[36m0.7391\u001b[0m  0.0444\n",
            "     11        \u001b[36m0.7237\u001b[0m  0.0565\n",
            "     12        \u001b[36m0.7137\u001b[0m  0.0528\n",
            "     13        \u001b[36m0.7049\u001b[0m  0.0457\n",
            "     14        \u001b[36m0.6966\u001b[0m  0.0476\n",
            "     15        \u001b[36m0.6869\u001b[0m  0.0438\n",
            "     16        \u001b[36m0.6859\u001b[0m  0.0459\n",
            "     17        \u001b[36m0.6758\u001b[0m  0.0564\n",
            "     18        \u001b[36m0.6722\u001b[0m  0.0528\n",
            "     19        \u001b[36m0.6708\u001b[0m  0.0536\n",
            "     20        \u001b[36m0.6659\u001b[0m  0.0509\n",
            "     21        \u001b[36m0.6619\u001b[0m  0.0433\n",
            "     22        \u001b[36m0.6547\u001b[0m  0.0419\n",
            "     23        \u001b[36m0.6510\u001b[0m  0.0436\n",
            "     24        \u001b[36m0.6461\u001b[0m  0.0461\n",
            "     25        \u001b[36m0.6373\u001b[0m  0.0488\n",
            "     26        \u001b[36m0.6332\u001b[0m  0.0460\n",
            "     27        \u001b[36m0.6316\u001b[0m  0.0492\n",
            "     28        \u001b[36m0.6273\u001b[0m  0.0451\n",
            "     29        \u001b[36m0.6231\u001b[0m  0.0465\n",
            "     30        \u001b[36m0.6210\u001b[0m  0.0451\n",
            "     31        0.6312  0.0503\n",
            "     32        0.6291  0.0474\n",
            "     33        0.6250  0.0492\n",
            "     34        \u001b[36m0.6150\u001b[0m  0.0550\n",
            "     35        \u001b[36m0.6112\u001b[0m  0.0478\n",
            "     36        \u001b[36m0.6083\u001b[0m  0.0501\n",
            "     37        \u001b[36m0.6057\u001b[0m  0.0495\n",
            "     38        \u001b[36m0.6031\u001b[0m  0.0546\n",
            "     39        \u001b[36m0.6006\u001b[0m  0.0501\n",
            "     40        \u001b[36m0.5983\u001b[0m  0.0494\n",
            "     41        \u001b[36m0.5960\u001b[0m  0.0458\n",
            "     42        0.5976  0.0508\n",
            "     43        0.6013  0.0486\n",
            "     44        0.5987  0.0484\n",
            "     45        \u001b[36m0.5899\u001b[0m  0.0473\n",
            "     46        \u001b[36m0.5830\u001b[0m  0.0436\n",
            "     47        \u001b[36m0.5759\u001b[0m  0.0443\n",
            "     48        \u001b[36m0.5732\u001b[0m  0.0453\n",
            "     49        \u001b[36m0.5713\u001b[0m  0.0429\n",
            "     50        \u001b[36m0.5695\u001b[0m  0.0457\n",
            "     51        \u001b[36m0.5678\u001b[0m  0.0560\n",
            "     52        \u001b[36m0.5663\u001b[0m  0.0420\n",
            "     53        \u001b[36m0.5649\u001b[0m  0.0429\n",
            "     54        \u001b[36m0.5636\u001b[0m  0.0438\n",
            "     55        0.5709  0.0455\n",
            "     56        0.5701  0.0438\n",
            "     57        0.5660  0.0470\n",
            "     58        \u001b[36m0.5601\u001b[0m  0.0515\n",
            "     59        0.5610  0.0456\n",
            "     60        \u001b[36m0.5590\u001b[0m  0.0444\n",
            "     61        \u001b[36m0.5581\u001b[0m  0.0489\n",
            "     62        \u001b[36m0.5572\u001b[0m  0.0435\n",
            "     63        \u001b[36m0.5563\u001b[0m  0.0431\n",
            "     64        \u001b[36m0.5554\u001b[0m  0.0432\n",
            "     65        \u001b[36m0.5542\u001b[0m  0.0431\n",
            "     66        \u001b[36m0.5531\u001b[0m  0.0532\n",
            "     67        \u001b[36m0.5518\u001b[0m  0.0626\n",
            "     68        \u001b[36m0.5500\u001b[0m  0.0415\n",
            "     69        \u001b[36m0.5476\u001b[0m  0.0429\n",
            "     70        \u001b[36m0.5447\u001b[0m  0.0443\n",
            "     71        \u001b[36m0.5422\u001b[0m  0.0469\n",
            "     72        \u001b[36m0.5403\u001b[0m  0.0543\n",
            "     73        \u001b[36m0.5388\u001b[0m  0.0416\n",
            "     74        \u001b[36m0.5376\u001b[0m  0.0573\n",
            "     75        \u001b[36m0.5368\u001b[0m  0.0563\n",
            "     76        \u001b[36m0.5360\u001b[0m  0.0506\n",
            "     77        \u001b[36m0.5353\u001b[0m  0.0496\n",
            "     78        \u001b[36m0.5347\u001b[0m  0.0483\n",
            "     79        \u001b[36m0.5341\u001b[0m  0.0439\n",
            "     80        \u001b[36m0.5336\u001b[0m  0.0440\n",
            "     81        \u001b[36m0.5316\u001b[0m  0.0464\n",
            "     82        \u001b[36m0.5311\u001b[0m  0.0426\n",
            "     83        \u001b[36m0.5307\u001b[0m  0.0419\n",
            "     84        \u001b[36m0.5303\u001b[0m  0.0442\n",
            "     85        \u001b[36m0.5299\u001b[0m  0.0490\n",
            "     86        \u001b[36m0.5295\u001b[0m  0.0456\n",
            "     87        \u001b[36m0.5291\u001b[0m  0.0448\n",
            "     88        \u001b[36m0.5287\u001b[0m  0.0463\n",
            "     89        \u001b[36m0.5282\u001b[0m  0.0464\n",
            "     90        0.5434  0.0465\n",
            "     91        0.5451  0.0454\n",
            "     92        0.5427  0.0564\n",
            "     93        0.5407  0.0444\n",
            "     94        0.5395  0.0422\n",
            "     95        0.5381  0.0443\n",
            "     96        0.5434  0.0451\n",
            "     97        0.5337  0.0470\n",
            "     98        0.5359  0.0490\n",
            "     99        0.5423  0.0462\n",
            "    100        0.5369  0.0509\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7467\u001b[0m  0.0416\n",
            "      2        \u001b[36m0.6770\u001b[0m  0.0409\n",
            "      3        \u001b[36m0.6333\u001b[0m  0.0505\n",
            "      4        \u001b[36m0.6185\u001b[0m  0.0506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.6071\u001b[0m  0.0598\n",
            "      6        \u001b[36m0.5927\u001b[0m  0.0429\n",
            "      7        \u001b[36m0.5866\u001b[0m  0.0451\n",
            "      8        \u001b[36m0.5777\u001b[0m  0.0452\n",
            "      9        0.5794  0.0442\n",
            "     10        \u001b[36m0.5630\u001b[0m  0.0474\n",
            "     11        0.5665  0.0454\n",
            "     12        0.5861  0.0586\n",
            "     13        0.5795  0.0469\n",
            "     14        0.5717  0.0443\n",
            "     15        0.5660  0.0469\n",
            "     16        \u001b[36m0.5612\u001b[0m  0.0486\n",
            "     17        \u001b[36m0.5568\u001b[0m  0.0519\n",
            "     18        \u001b[36m0.5524\u001b[0m  0.0591\n",
            "     19        \u001b[36m0.5481\u001b[0m  0.0458\n",
            "     20        \u001b[36m0.5444\u001b[0m  0.0511\n",
            "     21        \u001b[36m0.5410\u001b[0m  0.0443\n",
            "     22        \u001b[36m0.5379\u001b[0m  0.0460\n",
            "     23        0.5414  0.0433\n",
            "     24        0.5380  0.0461\n",
            "     25        \u001b[36m0.5344\u001b[0m  0.0449\n",
            "     26        \u001b[36m0.5318\u001b[0m  0.0454\n",
            "     27        \u001b[36m0.5295\u001b[0m  0.0465\n",
            "     28        0.5304  0.0465\n",
            "     29        \u001b[36m0.5277\u001b[0m  0.0440\n",
            "     30        0.5302  0.0523\n",
            "     31        \u001b[36m0.5242\u001b[0m  0.0459\n",
            "     32        \u001b[36m0.5215\u001b[0m  0.0548\n",
            "     33        \u001b[36m0.5181\u001b[0m  0.0467\n",
            "     34        \u001b[36m0.5163\u001b[0m  0.0475\n",
            "     35        \u001b[36m0.5146\u001b[0m  0.0486\n",
            "     36        \u001b[36m0.5125\u001b[0m  0.0468\n",
            "     37        \u001b[36m0.5106\u001b[0m  0.0503\n",
            "     38        \u001b[36m0.5091\u001b[0m  0.0481\n",
            "     39        \u001b[36m0.5077\u001b[0m  0.0511\n",
            "     40        \u001b[36m0.5052\u001b[0m  0.0444\n",
            "     41        0.5067  0.0465\n",
            "     42        0.5055  0.0449\n",
            "     43        \u001b[36m0.5037\u001b[0m  0.0439\n",
            "     44        \u001b[36m0.5027\u001b[0m  0.0471\n",
            "     45        \u001b[36m0.5011\u001b[0m  0.0520\n",
            "     46        \u001b[36m0.5007\u001b[0m  0.0466\n",
            "     47        \u001b[36m0.4996\u001b[0m  0.0422\n",
            "     48        \u001b[36m0.4983\u001b[0m  0.0419\n",
            "     49        \u001b[36m0.4974\u001b[0m  0.0430\n",
            "     50        \u001b[36m0.4953\u001b[0m  0.0483\n",
            "     51        \u001b[36m0.4894\u001b[0m  0.0448\n",
            "     52        0.4983  0.0504\n",
            "     53        0.5079  0.0619\n",
            "     54        0.5060  0.0443\n",
            "     55        0.5043  0.0456\n",
            "     56        0.5029  0.0450\n",
            "     57        0.5016  0.0471\n",
            "     58        0.5004  0.0555\n",
            "     59        0.4993  0.0549\n",
            "     60        0.4983  0.0517\n",
            "     61        0.4973  0.0430\n",
            "     62        0.4965  0.0426\n",
            "     63        0.4956  0.0524\n",
            "     64        0.4951  0.0474\n",
            "     65        0.5079  0.0439\n",
            "     66        0.5068  0.0447\n",
            "     67        0.5115  0.0473\n",
            "     68        0.5111  0.0422\n",
            "     69        0.5122  0.0441\n",
            "     70        0.5116  0.0467\n",
            "     71        0.5093  0.0504\n",
            "     72        0.5080  0.0478\n",
            "     73        0.5071  0.0529\n",
            "     74        0.5063  0.0513\n",
            "     75        0.5057  0.0467\n",
            "     76        0.5045  0.0528\n",
            "     77        0.5040  0.0512\n",
            "     78        0.5033  0.0507\n",
            "     79        0.5028  0.0547\n",
            "     80        0.5068  0.0655\n",
            "     81        0.5061  0.0515\n",
            "     82        0.5055  0.0426\n",
            "     83        0.5050  0.0468\n",
            "     84        0.5044  0.0479\n",
            "     85        0.5039  0.0486\n",
            "     86        0.5034  0.0428\n",
            "     87        0.5029  0.0470\n",
            "     88        0.5025  0.0479\n",
            "     89        0.5021  0.0480\n",
            "     90        0.5014  0.0522\n",
            "     91        0.5011  0.0439\n",
            "     92        0.5056  0.0666\n",
            "     93        0.5050  0.0452\n",
            "     94        0.5035  0.0466\n",
            "     95        0.5025  0.0570\n",
            "     96        0.5017  0.0595\n",
            "     97        0.5009  0.0627\n",
            "     98        0.5002  0.0559\n",
            "     99        0.4994  0.0483\n",
            "    100        0.4974  0.0449\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8208\u001b[0m  0.0297\n",
            "      2        \u001b[36m0.8087\u001b[0m  0.0346\n",
            "      3        \u001b[36m0.7985\u001b[0m  0.0408\n",
            "      4        \u001b[36m0.7898\u001b[0m  0.0348\n",
            "      5        \u001b[36m0.7824\u001b[0m  0.0342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m0.7758\u001b[0m  0.0382\n",
            "      7        \u001b[36m0.7701\u001b[0m  0.0417\n",
            "      8        \u001b[36m0.7650\u001b[0m  0.0349\n",
            "      9        \u001b[36m0.7603\u001b[0m  0.0439\n",
            "     10        \u001b[36m0.7561\u001b[0m  0.0369\n",
            "     11        \u001b[36m0.7523\u001b[0m  0.0338\n",
            "     12        \u001b[36m0.7488\u001b[0m  0.0587\n",
            "     13        \u001b[36m0.7455\u001b[0m  0.0580\n",
            "     14        \u001b[36m0.7424\u001b[0m  0.0409\n",
            "     15        \u001b[36m0.7395\u001b[0m  0.0408\n",
            "     16        \u001b[36m0.7368\u001b[0m  0.0418\n",
            "     17        \u001b[36m0.7342\u001b[0m  0.0392\n",
            "     18        \u001b[36m0.7318\u001b[0m  0.0460\n",
            "     19        \u001b[36m0.7295\u001b[0m  0.0383\n",
            "     20        \u001b[36m0.7273\u001b[0m  0.0404\n",
            "     21        \u001b[36m0.7251\u001b[0m  0.0349\n",
            "     22        \u001b[36m0.7231\u001b[0m  0.0346\n",
            "     23        \u001b[36m0.7212\u001b[0m  0.0342\n",
            "     24        \u001b[36m0.7193\u001b[0m  0.0384\n",
            "     25        \u001b[36m0.7175\u001b[0m  0.0383\n",
            "     26        \u001b[36m0.7157\u001b[0m  0.0346\n",
            "     27        \u001b[36m0.7141\u001b[0m  0.0341\n",
            "     28        \u001b[36m0.7125\u001b[0m  0.0364\n",
            "     29        \u001b[36m0.7109\u001b[0m  0.0352\n",
            "     30        \u001b[36m0.7094\u001b[0m  0.0337\n",
            "     31        \u001b[36m0.7079\u001b[0m  0.0377\n",
            "     32        \u001b[36m0.7065\u001b[0m  0.0320\n",
            "     33        \u001b[36m0.7051\u001b[0m  0.0321\n",
            "     34        \u001b[36m0.7038\u001b[0m  0.0364\n",
            "     35        \u001b[36m0.7025\u001b[0m  0.0307\n",
            "     36        \u001b[36m0.7013\u001b[0m  0.0351\n",
            "     37        \u001b[36m0.7000\u001b[0m  0.0365\n",
            "     38        \u001b[36m0.6988\u001b[0m  0.0437\n",
            "     39        \u001b[36m0.6977\u001b[0m  0.0412\n",
            "     40        \u001b[36m0.6966\u001b[0m  0.0491\n",
            "     41        \u001b[36m0.6954\u001b[0m  0.0456\n",
            "     42        \u001b[36m0.6944\u001b[0m  0.0444\n",
            "     43        \u001b[36m0.6933\u001b[0m  0.0394\n",
            "     44        \u001b[36m0.6923\u001b[0m  0.0368\n",
            "     45        \u001b[36m0.6912\u001b[0m  0.0346\n",
            "     46        \u001b[36m0.6902\u001b[0m  0.0411\n",
            "     47        \u001b[36m0.6893\u001b[0m  0.0345\n",
            "     48        \u001b[36m0.6883\u001b[0m  0.0356\n",
            "     49        \u001b[36m0.6873\u001b[0m  0.0363\n",
            "     50        \u001b[36m0.6864\u001b[0m  0.0333\n",
            "     51        \u001b[36m0.6855\u001b[0m  0.0372\n",
            "     52        \u001b[36m0.6846\u001b[0m  0.0371\n",
            "     53        \u001b[36m0.6837\u001b[0m  0.0379\n",
            "     54        \u001b[36m0.6828\u001b[0m  0.0393\n",
            "     55        \u001b[36m0.6819\u001b[0m  0.0387\n",
            "     56        0.6858  0.0345\n",
            "     57        \u001b[36m0.6793\u001b[0m  0.0362\n",
            "     58        \u001b[36m0.6783\u001b[0m  0.0370\n",
            "     59        \u001b[36m0.6774\u001b[0m  0.0368\n",
            "     60        \u001b[36m0.6765\u001b[0m  0.0362\n",
            "     61        \u001b[36m0.6756\u001b[0m  0.0392\n",
            "     62        \u001b[36m0.6748\u001b[0m  0.0342\n",
            "     63        \u001b[36m0.6740\u001b[0m  0.0505\n",
            "     64        \u001b[36m0.6732\u001b[0m  0.0377\n",
            "     65        \u001b[36m0.6724\u001b[0m  0.0364\n",
            "     66        \u001b[36m0.6716\u001b[0m  0.0405\n",
            "     67        \u001b[36m0.6709\u001b[0m  0.0371\n",
            "     68        \u001b[36m0.6701\u001b[0m  0.0347\n",
            "     69        \u001b[36m0.6693\u001b[0m  0.0352\n",
            "     70        \u001b[36m0.6686\u001b[0m  0.0359\n",
            "     71        \u001b[36m0.6678\u001b[0m  0.0346\n",
            "     72        \u001b[36m0.6671\u001b[0m  0.0352\n",
            "     73        \u001b[36m0.6664\u001b[0m  0.0331\n",
            "     74        \u001b[36m0.6656\u001b[0m  0.0357\n",
            "     75        \u001b[36m0.6649\u001b[0m  0.0355\n",
            "     76        \u001b[36m0.6642\u001b[0m  0.0452\n",
            "     77        \u001b[36m0.6635\u001b[0m  0.0341\n",
            "     78        \u001b[36m0.6628\u001b[0m  0.0320\n",
            "     79        \u001b[36m0.6621\u001b[0m  0.0346\n",
            "     80        \u001b[36m0.6614\u001b[0m  0.0439\n",
            "     81        \u001b[36m0.6607\u001b[0m  0.0491\n",
            "     82        \u001b[36m0.6600\u001b[0m  0.0400\n",
            "     83        \u001b[36m0.6593\u001b[0m  0.0358\n",
            "     84        \u001b[36m0.6586\u001b[0m  0.0383\n",
            "     85        \u001b[36m0.6579\u001b[0m  0.0362\n",
            "     86        \u001b[36m0.6572\u001b[0m  0.0348\n",
            "     87        \u001b[36m0.6565\u001b[0m  0.0353\n",
            "     88        \u001b[36m0.6559\u001b[0m  0.0505\n",
            "     89        \u001b[36m0.6552\u001b[0m  0.0339\n",
            "     90        \u001b[36m0.6545\u001b[0m  0.0426\n",
            "     91        \u001b[36m0.6538\u001b[0m  0.0427\n",
            "     92        \u001b[36m0.6532\u001b[0m  0.0408\n",
            "     93        \u001b[36m0.6525\u001b[0m  0.0391\n",
            "     94        \u001b[36m0.6519\u001b[0m  0.0360\n",
            "     95        \u001b[36m0.6512\u001b[0m  0.0326\n",
            "     96        \u001b[36m0.6506\u001b[0m  0.0348\n",
            "     97        \u001b[36m0.6499\u001b[0m  0.0361\n",
            "     98        \u001b[36m0.6493\u001b[0m  0.0365\n",
            "     99        \u001b[36m0.6486\u001b[0m  0.0364\n",
            "    100        \u001b[36m0.6480\u001b[0m  0.0358\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m3.3193\u001b[0m  0.0329\n",
            "      2        \u001b[36m3.0141\u001b[0m  0.0342\n",
            "      3        \u001b[36m2.9323\u001b[0m  0.0346\n",
            "      4        \u001b[36m2.8555\u001b[0m  0.0359\n",
            "      5        \u001b[36m2.7793\u001b[0m  0.0335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m2.7027\u001b[0m  0.0386\n",
            "      7        \u001b[36m2.6258\u001b[0m  0.0378\n",
            "      8        \u001b[36m2.5484\u001b[0m  0.0337\n",
            "      9        \u001b[36m2.4706\u001b[0m  0.0366\n",
            "     10        \u001b[36m2.3926\u001b[0m  0.0345\n",
            "     11        \u001b[36m2.3145\u001b[0m  0.0339\n",
            "     12        \u001b[36m2.2363\u001b[0m  0.0367\n",
            "     13        \u001b[36m2.1526\u001b[0m  0.0380\n",
            "     14        \u001b[36m1.6598\u001b[0m  0.0430\n",
            "     15        \u001b[36m1.5897\u001b[0m  0.0351\n",
            "     16        \u001b[36m1.5547\u001b[0m  0.0382\n",
            "     17        \u001b[36m1.5213\u001b[0m  0.0395\n",
            "     18        \u001b[36m1.4901\u001b[0m  0.0348\n",
            "     19        \u001b[36m1.4604\u001b[0m  0.0324\n",
            "     20        \u001b[36m1.4321\u001b[0m  0.0426\n",
            "     21        \u001b[36m1.4052\u001b[0m  0.0348\n",
            "     22        \u001b[36m1.3796\u001b[0m  0.0308\n",
            "     23        \u001b[36m1.3552\u001b[0m  0.0403\n",
            "     24        \u001b[36m1.3321\u001b[0m  0.0326\n",
            "     25        \u001b[36m1.3100\u001b[0m  0.0334\n",
            "     26        \u001b[36m1.2889\u001b[0m  0.0354\n",
            "     27        \u001b[36m1.2687\u001b[0m  0.0315\n",
            "     28        \u001b[36m1.2494\u001b[0m  0.0344\n",
            "     29        \u001b[36m1.2309\u001b[0m  0.0335\n",
            "     30        \u001b[36m1.2131\u001b[0m  0.0350\n",
            "     31        \u001b[36m1.1959\u001b[0m  0.0416\n",
            "     32        \u001b[36m1.1794\u001b[0m  0.0329\n",
            "     33        \u001b[36m1.1634\u001b[0m  0.0350\n",
            "     34        \u001b[36m1.1479\u001b[0m  0.0433\n",
            "     35        \u001b[36m1.1330\u001b[0m  0.0361\n",
            "     36        \u001b[36m1.1184\u001b[0m  0.0392\n",
            "     37        \u001b[36m1.1043\u001b[0m  0.0402\n",
            "     38        \u001b[36m1.0906\u001b[0m  0.0390\n",
            "     39        \u001b[36m1.0773\u001b[0m  0.0433\n",
            "     40        \u001b[36m1.0644\u001b[0m  0.0391\n",
            "     41        \u001b[36m1.0518\u001b[0m  0.0318\n",
            "     42        \u001b[36m1.0396\u001b[0m  0.0355\n",
            "     43        \u001b[36m1.0277\u001b[0m  0.0345\n",
            "     44        \u001b[36m1.0162\u001b[0m  0.0378\n",
            "     45        \u001b[36m1.0050\u001b[0m  0.0368\n",
            "     46        \u001b[36m0.9942\u001b[0m  0.0365\n",
            "     47        \u001b[36m0.9837\u001b[0m  0.0423\n",
            "     48        \u001b[36m0.9735\u001b[0m  0.0345\n",
            "     49        \u001b[36m0.9637\u001b[0m  0.0348\n",
            "     50        \u001b[36m0.9542\u001b[0m  0.0349\n",
            "     51        \u001b[36m0.9451\u001b[0m  0.0421\n",
            "     52        \u001b[36m0.9362\u001b[0m  0.0350\n",
            "     53        \u001b[36m0.9278\u001b[0m  0.0389\n",
            "     54        \u001b[36m0.9196\u001b[0m  0.0325\n",
            "     55        \u001b[36m0.9159\u001b[0m  0.0338\n",
            "     56        \u001b[36m0.9079\u001b[0m  0.0366\n",
            "     57        \u001b[36m0.9003\u001b[0m  0.0330\n",
            "     58        \u001b[36m0.8930\u001b[0m  0.0316\n",
            "     59        \u001b[36m0.8860\u001b[0m  0.0359\n",
            "     60        \u001b[36m0.8793\u001b[0m  0.0367\n",
            "     61        \u001b[36m0.8728\u001b[0m  0.0422\n",
            "     62        \u001b[36m0.8666\u001b[0m  0.0381\n",
            "     63        \u001b[36m0.8607\u001b[0m  0.0373\n",
            "     64        \u001b[36m0.8550\u001b[0m  0.0403\n",
            "     65        \u001b[36m0.8495\u001b[0m  0.0511\n",
            "     66        \u001b[36m0.8442\u001b[0m  0.0355\n",
            "     67        \u001b[36m0.8392\u001b[0m  0.0361\n",
            "     68        \u001b[36m0.8343\u001b[0m  0.0335\n",
            "     69        \u001b[36m0.8296\u001b[0m  0.0321\n",
            "     70        \u001b[36m0.8251\u001b[0m  0.0345\n",
            "     71        \u001b[36m0.8207\u001b[0m  0.0339\n",
            "     72        \u001b[36m0.8165\u001b[0m  0.0337\n",
            "     73        \u001b[36m0.8125\u001b[0m  0.0339\n",
            "     74        \u001b[36m0.8086\u001b[0m  0.0359\n",
            "     75        \u001b[36m0.8048\u001b[0m  0.0337\n",
            "     76        \u001b[36m0.8011\u001b[0m  0.0306\n",
            "     77        \u001b[36m0.7976\u001b[0m  0.0330\n",
            "     78        \u001b[36m0.7942\u001b[0m  0.0370\n",
            "     79        \u001b[36m0.7908\u001b[0m  0.0349\n",
            "     80        \u001b[36m0.7876\u001b[0m  0.0327\n",
            "     81        \u001b[36m0.7845\u001b[0m  0.0378\n",
            "     82        \u001b[36m0.7814\u001b[0m  0.0376\n",
            "     83        \u001b[36m0.7784\u001b[0m  0.0361\n",
            "     84        \u001b[36m0.7756\u001b[0m  0.0365\n",
            "     85        \u001b[36m0.7728\u001b[0m  0.0341\n",
            "     86        \u001b[36m0.7700\u001b[0m  0.0364\n",
            "     87        \u001b[36m0.7674\u001b[0m  0.0389\n",
            "     88        \u001b[36m0.7648\u001b[0m  0.0354\n",
            "     89        \u001b[36m0.7622\u001b[0m  0.0361\n",
            "     90        \u001b[36m0.7598\u001b[0m  0.0384\n",
            "     91        \u001b[36m0.7573\u001b[0m  0.0413\n",
            "     92        \u001b[36m0.7550\u001b[0m  0.0471\n",
            "     93        \u001b[36m0.7527\u001b[0m  0.0351\n",
            "     94        \u001b[36m0.7504\u001b[0m  0.0347\n",
            "     95        \u001b[36m0.7482\u001b[0m  0.0360\n",
            "     96        \u001b[36m0.7460\u001b[0m  0.0379\n",
            "     97        \u001b[36m0.7439\u001b[0m  0.0322\n",
            "     98        \u001b[36m0.7418\u001b[0m  0.0368\n",
            "     99        \u001b[36m0.7398\u001b[0m  0.0338\n",
            "    100        \u001b[36m0.7378\u001b[0m  0.0339\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0467\n",
            "      2        1.0000  0.0513\n",
            "      3        1.0000  0.0493\n",
            "      4        1.0000  0.0541\n",
            "      5        1.0000  0.0506\n",
            "      6        1.0000  0.0476\n",
            "      7        1.0000  0.0554\n",
            "      8        1.0000  0.0492\n",
            "      9        1.0000  0.0498\n",
            "     10        1.0000  0.0486\n",
            "     11        1.0000  0.0535\n",
            "     12        1.0000  0.0542\n",
            "     13        1.0000  0.0632\n",
            "     14        1.0000  0.0547\n",
            "     15        1.0000  0.0485\n",
            "     16        1.0000  0.0497\n",
            "     17        1.0000  0.0470\n",
            "     18        1.0000  0.0467\n",
            "     19        1.0000  0.0472\n",
            "     20        1.0000  0.0478\n",
            "     21        1.0000  0.0503\n",
            "     22        1.0000  0.0513\n",
            "     23        1.0000  0.0502\n",
            "     24        1.0000  0.0519\n",
            "     25        1.0000  0.0488\n",
            "     26        1.0000  0.0578\n",
            "     27        1.0000  0.0547\n",
            "     28        1.0000  0.0549\n",
            "     29        1.0000  0.0606\n",
            "     30        1.0000  0.0527\n",
            "     31        1.0000  0.0663\n",
            "     32        1.0000  0.0512\n",
            "     33        1.0000  0.0486\n",
            "     34        1.0000  0.0547\n",
            "     35        \u001b[36m1.0000\u001b[0m  0.0529\n",
            "     36        \u001b[36m1.0000\u001b[0m  0.0554\n",
            "     37        \u001b[36m0.6235\u001b[0m  0.0524\n",
            "     38        \u001b[36m0.3733\u001b[0m  0.0491\n",
            "     39        \u001b[36m0.3733\u001b[0m  0.0486\n",
            "     40        \u001b[36m0.3733\u001b[0m  0.0501\n",
            "     41        \u001b[36m0.3733\u001b[0m  0.0511\n",
            "     42        \u001b[36m0.3733\u001b[0m  0.0500\n",
            "     43        \u001b[36m0.3733\u001b[0m  0.0465\n",
            "     44        \u001b[36m0.3733\u001b[0m  0.0525\n",
            "     45        \u001b[36m0.3732\u001b[0m  0.0571\n",
            "     46        \u001b[36m0.3732\u001b[0m  0.0591\n",
            "     47        \u001b[36m0.3732\u001b[0m  0.0552\n",
            "     48        \u001b[36m0.3732\u001b[0m  0.0558\n",
            "     49        \u001b[36m0.3732\u001b[0m  0.0573\n",
            "     50        \u001b[36m0.3732\u001b[0m  0.0661\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0442\n",
            "      2        1.0000  0.0516\n",
            "      3        1.0000  0.0475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        1.0000  0.0463\n",
            "      5        1.0000  0.0479\n",
            "      6        1.0000  0.0468\n",
            "      7        1.0000  0.0458\n",
            "      8        1.0000  0.0611\n",
            "      9        1.0000  0.0510\n",
            "     10        1.0000  0.0492\n",
            "     11        1.0000  0.0474\n",
            "     12        1.0000  0.0462\n",
            "     13        1.0000  0.0491\n",
            "     14        1.0000  0.0559\n",
            "     15        1.0000  0.0538\n",
            "     16        1.0000  0.0551\n",
            "     17        1.0000  0.0478\n",
            "     18        1.0000  0.0471\n",
            "     19        1.0000  0.0686\n",
            "     20        1.0000  0.0470\n",
            "     21        1.0000  0.0459\n",
            "     22        1.0000  0.0465\n",
            "     23        1.0000  0.0472\n",
            "     24        1.0000  0.0454\n",
            "     25        1.0000  0.0486\n",
            "     26        1.0000  0.0499\n",
            "     27        1.0000  0.0542\n",
            "     28        1.0000  0.0534\n",
            "     29        1.0000  0.0567\n",
            "     30        1.0000  0.0588\n",
            "     31        1.0000  0.0571\n",
            "     32        1.0000  0.0561\n",
            "     33        1.0000  0.0514\n",
            "     34        1.0000  0.0577\n",
            "     35        1.0000  0.0511\n",
            "     36        \u001b[36m1.0000\u001b[0m  0.0528\n",
            "     37        \u001b[36m0.6169\u001b[0m  0.0487\n",
            "     38        \u001b[36m0.3719\u001b[0m  0.0537\n",
            "     39        \u001b[36m0.3719\u001b[0m  0.0498\n",
            "     40        \u001b[36m0.3719\u001b[0m  0.0494\n",
            "     41        0.3719  0.0585\n",
            "     42        0.3719  0.0484\n",
            "     43        0.3719  0.0487\n",
            "     44        0.3719  0.0472\n",
            "     45        0.3719  0.0501\n",
            "     46        0.3719  0.0502\n",
            "     47        0.3719  0.0494\n",
            "     48        0.3719  0.0504\n",
            "     49        0.3719  0.0540\n",
            "     50        0.3719  0.0582\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0494\n",
            "      2        1.0000  0.0504\n",
            "      3        1.0000  0.0480\n",
            "      4        1.0000  0.0399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0457\n",
            "      6        1.0000  0.0360\n",
            "      7        1.0000  0.0460\n",
            "      8        1.0000  0.0410\n",
            "      9        1.0000  0.0433\n",
            "     10        1.0000  0.0390\n",
            "     11        1.0000  0.0385\n",
            "     12        1.0000  0.0400\n",
            "     13        1.0000  0.0342\n",
            "     14        1.0000  0.0371\n",
            "     15        1.0000  0.0376\n",
            "     16        1.0000  0.0367\n",
            "     17        1.0000  0.0381\n",
            "     18        1.0000  0.0407\n",
            "     19        1.0000  0.0414\n",
            "     20        1.0000  0.0373\n",
            "     21        1.0000  0.0439\n",
            "     22        1.0000  0.0430\n",
            "     23        1.0000  0.0456\n",
            "     24        1.0000  0.0433\n",
            "     25        1.0000  0.0399\n",
            "     26        1.0000  0.0397\n",
            "     27        1.0000  0.0391\n",
            "     28        1.0000  0.0420\n",
            "     29        1.0000  0.0453\n",
            "     30        1.0000  0.0347\n",
            "     31        1.0000  0.0455\n",
            "     32        1.0000  0.0343\n",
            "     33        1.0000  0.0369\n",
            "     34        1.0000  0.0351\n",
            "     35        1.0000  0.0366\n",
            "     36        1.0000  0.0372\n",
            "     37        1.0000  0.0376\n",
            "     38        1.0000  0.0494\n",
            "     39        1.0000  0.0365\n",
            "     40        1.0000  0.0461\n",
            "     41        1.0000  0.0507\n",
            "     42        1.0000  0.0413\n",
            "     43        1.0000  0.0435\n",
            "     44        1.0000  0.0393\n",
            "     45        1.0000  0.0443\n",
            "     46        1.0000  0.0486\n",
            "     47        1.0000  0.0513\n",
            "     48        1.0000  0.0442\n",
            "     49        1.0000  0.0429\n",
            "     50        1.0000  0.0380\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0331\n",
            "      2        1.0000  0.0409\n",
            "      3        1.0000  0.0459\n",
            "      4        1.0000  0.0356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0474\n",
            "      6        1.0000  0.0460\n",
            "      7        1.0000  0.0361\n",
            "      8        1.0000  0.0366\n",
            "      9        1.0000  0.0362\n",
            "     10        1.0000  0.0366\n",
            "     11        1.0000  0.0362\n",
            "     12        1.0000  0.0378\n",
            "     13        1.0000  0.0399\n",
            "     14        1.0000  0.0362\n",
            "     15        1.0000  0.0400\n",
            "     16        1.0000  0.0366\n",
            "     17        1.0000  0.0385\n",
            "     18        1.0000  0.0407\n",
            "     19        1.0000  0.0467\n",
            "     20        1.0000  0.0467\n",
            "     21        1.0000  0.0352\n",
            "     22        1.0000  0.0376\n",
            "     23        1.0000  0.0442\n",
            "     24        1.0000  0.0411\n",
            "     25        1.0000  0.0404\n",
            "     26        1.0000  0.0378\n",
            "     27        1.0000  0.0524\n",
            "     28        1.0000  0.0398\n",
            "     29        1.0000  0.0374\n",
            "     30        1.0000  0.0373\n",
            "     31        1.0000  0.0391\n",
            "     32        1.0000  0.0356\n",
            "     33        1.0000  0.0385\n",
            "     34        1.0000  0.0362\n",
            "     35        1.0000  0.0644\n",
            "     36        1.0000  0.0431\n",
            "     37        1.0000  0.0355\n",
            "     38        1.0000  0.0362\n",
            "     39        1.0000  0.0431\n",
            "     40        1.0000  0.0449\n",
            "     41        1.0000  0.0439\n",
            "     42        1.0000  0.0432\n",
            "     43        1.0000  0.0444\n",
            "     44        1.0000  0.0452\n",
            "     45        1.0000  0.0359\n",
            "     46        1.0000  0.0373\n",
            "     47        1.0000  0.0366\n",
            "     48        1.0000  0.0426\n",
            "     49        1.0000  0.0421\n",
            "     50        1.0000  0.0460\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0433\n",
            "      2        1.0000  0.0457\n",
            "      3        1.0000  0.0479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        1.0000  0.0540\n",
            "      5        1.0000  0.0495\n",
            "      6        1.0000  0.0572\n",
            "      7        1.0000  0.0467\n",
            "      8        1.0000  0.0574\n",
            "      9        1.0000  0.0490\n",
            "     10        1.0000  0.0499\n",
            "     11        1.0000  0.0514\n",
            "     12        1.0000  0.0498\n",
            "     13        1.0000  0.0553\n",
            "     14        1.0000  0.0643\n",
            "     15        1.0000  0.0586\n",
            "     16        1.0000  0.0481\n",
            "     17        1.0000  0.0485\n",
            "     18        1.0000  0.0494\n",
            "     19        1.0000  0.0537\n",
            "     20        1.0000  0.0534\n",
            "     21        1.0000  0.0487\n",
            "     22        1.0000  0.0564\n",
            "     23        1.0000  0.0481\n",
            "     24        1.0000  0.0523\n",
            "     25        1.0000  0.0513\n",
            "     26        1.0000  0.0487\n",
            "     27        1.0000  0.0507\n",
            "     28        1.0000  0.0491\n",
            "     29        1.0000  0.0504\n",
            "     30        1.0000  0.0591\n",
            "     31        1.0000  0.0543\n",
            "     32        1.0000  0.0483\n",
            "     33        1.0000  0.0696\n",
            "     34        1.0000  0.0552\n",
            "     35        1.0000  0.0502\n",
            "     36        1.0000  0.0573\n",
            "     37        1.0000  0.0572\n",
            "     38        1.0000  0.0462\n",
            "     39        1.0000  0.0491\n",
            "     40        1.0000  0.0456\n",
            "     41        1.0000  0.0486\n",
            "     42        1.0000  0.0515\n",
            "     43        \u001b[36m1.0000\u001b[0m  0.0510\n",
            "     44        \u001b[36m0.9962\u001b[0m  0.0541\n",
            "     45        \u001b[36m0.3733\u001b[0m  0.0492\n",
            "     46        \u001b[36m0.3732\u001b[0m  0.0576\n",
            "     47        \u001b[36m0.3732\u001b[0m  0.0555\n",
            "     48        0.3732  0.0502\n",
            "     49        0.3732  0.0505\n",
            "     50        0.3732  0.0509\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0428\n",
            "      2        1.0000  0.0514\n",
            "      3        1.0000  0.0458\n",
            "      4        1.0000  0.0473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0604\n",
            "      6        1.0000  0.0574\n",
            "      7        1.0000  0.0466\n",
            "      8        1.0000  0.0472\n",
            "      9        1.0000  0.0473\n",
            "     10        1.0000  0.0488\n",
            "     11        1.0000  0.0534\n",
            "     12        1.0000  0.0487\n",
            "     13        1.0000  0.0503\n",
            "     14        1.0000  0.0524\n",
            "     15        1.0000  0.0528\n",
            "     16        1.0000  0.0574\n",
            "     17        1.0000  0.0557\n",
            "     18        1.0000  0.0548\n",
            "     19        1.0000  0.0545\n",
            "     20        1.0000  0.0475\n",
            "     21        1.0000  0.0482\n",
            "     22        1.0000  0.0469\n",
            "     23        1.0000  0.0482\n",
            "     24        1.0000  0.0528\n",
            "     25        1.0000  0.0693\n",
            "     26        1.0000  0.0478\n",
            "     27        1.0000  0.0505\n",
            "     28        1.0000  0.0467\n",
            "     29        1.0000  0.0530\n",
            "     30        1.0000  0.0471\n",
            "     31        1.0000  0.0470\n",
            "     32        1.0000  0.0497\n",
            "     33        1.0000  0.0646\n",
            "     34        1.0000  0.0493\n",
            "     35        1.0000  0.0590\n",
            "     36        1.0000  0.0624\n",
            "     37        1.0000  0.0507\n",
            "     38        1.0000  0.0530\n",
            "     39        1.0000  0.0508\n",
            "     40        1.0000  0.0516\n",
            "     41        \u001b[36m0.8195\u001b[0m  0.0556\n",
            "     42        \u001b[36m0.3719\u001b[0m  0.0480\n",
            "     43        \u001b[36m0.3719\u001b[0m  0.0622\n",
            "     44        0.3719  0.0450\n",
            "     45        0.3719  0.0464\n",
            "     46        0.3719  0.0475\n",
            "     47        0.3719  0.0502\n",
            "     48        0.3719  0.0493\n",
            "     49        0.3719  0.0560\n",
            "     50        0.3719  0.0520\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0364\n",
            "      2        1.0000  0.0442\n",
            "      3        1.0000  0.0399\n",
            "      4        1.0000  0.0446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0502\n",
            "      6        1.0000  0.0408\n",
            "      7        1.0000  0.0422\n",
            "      8        1.0000  0.0402\n",
            "      9        1.0000  0.0471\n",
            "     10        1.0000  0.0403\n",
            "     11        1.0000  0.0388\n",
            "     12        1.0000  0.0372\n",
            "     13        1.0000  0.0415\n",
            "     14        1.0000  0.0483\n",
            "     15        1.0000  0.0393\n",
            "     16        1.0000  0.0423\n",
            "     17        1.0000  0.0353\n",
            "     18        1.0000  0.0427\n",
            "     19        1.0000  0.0391\n",
            "     20        1.0000  0.0401\n",
            "     21        1.0000  0.0417\n",
            "     22        1.0000  0.0437\n",
            "     23        1.0000  0.0408\n",
            "     24        1.0000  0.0481\n",
            "     25        1.0000  0.0411\n",
            "     26        1.0000  0.0416\n",
            "     27        1.0000  0.0421\n",
            "     28        1.0000  0.0381\n",
            "     29        1.0000  0.0408\n",
            "     30        1.0000  0.0379\n",
            "     31        1.0000  0.0414\n",
            "     32        1.0000  0.0355\n",
            "     33        1.0000  0.0378\n",
            "     34        1.0000  0.0398\n",
            "     35        1.0000  0.0367\n",
            "     36        1.0000  0.0422\n",
            "     37        1.0000  0.0415\n",
            "     38        1.0000  0.0384\n",
            "     39        1.0000  0.0358\n",
            "     40        1.0000  0.0367\n",
            "     41        1.0000  0.0390\n",
            "     42        1.0000  0.0424\n",
            "     43        1.0000  0.0412\n",
            "     44        1.0000  0.0399\n",
            "     45        1.0000  0.0361\n",
            "     46        1.0000  0.0402\n",
            "     47        1.0000  0.0383\n",
            "     48        1.0000  0.0410\n",
            "     49        1.0000  0.0396\n",
            "     50        1.0000  0.0388\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0329\n",
            "      2        1.0000  0.0410\n",
            "      3        1.0000  0.0352\n",
            "      4        1.0000  0.0487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0461\n",
            "      6        1.0000  0.0376\n",
            "      7        1.0000  0.0439\n",
            "      8        1.0000  0.0383\n",
            "      9        1.0000  0.0398\n",
            "     10        1.0000  0.0371\n",
            "     11        1.0000  0.0474\n",
            "     12        1.0000  0.0368\n",
            "     13        1.0000  0.0402\n",
            "     14        1.0000  0.0398\n",
            "     15        1.0000  0.0325\n",
            "     16        1.0000  0.0373\n",
            "     17        1.0000  0.0384\n",
            "     18        1.0000  0.0439\n",
            "     19        1.0000  0.0434\n",
            "     20        1.0000  0.0404\n",
            "     21        1.0000  0.0401\n",
            "     22        1.0000  0.0422\n",
            "     23        1.0000  0.0522\n",
            "     24        1.0000  0.0411\n",
            "     25        1.0000  0.0425\n",
            "     26        1.0000  0.0375\n",
            "     27        1.0000  0.0395\n",
            "     28        1.0000  0.0458\n",
            "     29        1.0000  0.0363\n",
            "     30        1.0000  0.0380\n",
            "     31        1.0000  0.0400\n",
            "     32        1.0000  0.0380\n",
            "     33        1.0000  0.0385\n",
            "     34        1.0000  0.0401\n",
            "     35        1.0000  0.0463\n",
            "     36        1.0000  0.0352\n",
            "     37        1.0000  0.0411\n",
            "     38        1.0000  0.0414\n",
            "     39        1.0000  0.0415\n",
            "     40        1.0000  0.0424\n",
            "     41        1.0000  0.0419\n",
            "     42        1.0000  0.0346\n",
            "     43        1.0000  0.0369\n",
            "     44        1.0000  0.0395\n",
            "     45        1.0000  0.0543\n",
            "     46        1.0000  0.0635\n",
            "     47        1.0000  0.0473\n",
            "     48        1.0000  0.0433\n",
            "     49        1.0000  0.0410\n",
            "     50        1.0000  0.0392\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.5509\u001b[0m  0.0422\n",
            "      2        0.5547  0.0540\n",
            "      3        0.5551  0.0531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        0.5537  0.0542\n",
            "      5        \u001b[36m0.5450\u001b[0m  0.0574\n",
            "      6        \u001b[36m0.5415\u001b[0m  0.0670\n",
            "      7        \u001b[36m0.5359\u001b[0m  0.0486\n",
            "      8        \u001b[36m0.5343\u001b[0m  0.0539\n",
            "      9        \u001b[36m0.5319\u001b[0m  0.0493\n",
            "     10        \u001b[36m0.5214\u001b[0m  0.0525\n",
            "     11        \u001b[36m0.5198\u001b[0m  0.0576\n",
            "     12        \u001b[36m0.5162\u001b[0m  0.0559\n",
            "     13        \u001b[36m0.5097\u001b[0m  0.0531\n",
            "     14        \u001b[36m0.5050\u001b[0m  0.0531\n",
            "     15        \u001b[36m0.4917\u001b[0m  0.0534\n",
            "     16        \u001b[36m0.4784\u001b[0m  0.0499\n",
            "     17        \u001b[36m0.4389\u001b[0m  0.0484\n",
            "     18        \u001b[36m0.4321\u001b[0m  0.0483\n",
            "     19        \u001b[36m0.4319\u001b[0m  0.0563\n",
            "     20        \u001b[36m0.4250\u001b[0m  0.0522\n",
            "     21        \u001b[36m0.4065\u001b[0m  0.0520\n",
            "     22        \u001b[36m0.3959\u001b[0m  0.0477\n",
            "     23        \u001b[36m0.3958\u001b[0m  0.0491\n",
            "     24        \u001b[36m0.3958\u001b[0m  0.0646\n",
            "     25        \u001b[36m0.3957\u001b[0m  0.0567\n",
            "     26        \u001b[36m0.3957\u001b[0m  0.0479\n",
            "     27        \u001b[36m0.3956\u001b[0m  0.0501\n",
            "     28        \u001b[36m0.3955\u001b[0m  0.0520\n",
            "     29        \u001b[36m0.3954\u001b[0m  0.0651\n",
            "     30        \u001b[36m0.3954\u001b[0m  0.0610\n",
            "     31        \u001b[36m0.3944\u001b[0m  0.0519\n",
            "     32        \u001b[36m0.3879\u001b[0m  0.0532\n",
            "     33        \u001b[36m0.3866\u001b[0m  0.0492\n",
            "     34        \u001b[36m0.3866\u001b[0m  0.0490\n",
            "     35        \u001b[36m0.3865\u001b[0m  0.0541\n",
            "     36        \u001b[36m0.3865\u001b[0m  0.0568\n",
            "     37        \u001b[36m0.3865\u001b[0m  0.0528\n",
            "     38        \u001b[36m0.3865\u001b[0m  0.0520\n",
            "     39        \u001b[36m0.3864\u001b[0m  0.0456\n",
            "     40        \u001b[36m0.3864\u001b[0m  0.0480\n",
            "     41        \u001b[36m0.3864\u001b[0m  0.0506\n",
            "     42        \u001b[36m0.3864\u001b[0m  0.0526\n",
            "     43        \u001b[36m0.3863\u001b[0m  0.0525\n",
            "     44        \u001b[36m0.3863\u001b[0m  0.0567\n",
            "     45        \u001b[36m0.3863\u001b[0m  0.0516\n",
            "     46        \u001b[36m0.3863\u001b[0m  0.0515\n",
            "     47        \u001b[36m0.3862\u001b[0m  0.0609\n",
            "     48        \u001b[36m0.3862\u001b[0m  0.0565\n",
            "     49        \u001b[36m0.3862\u001b[0m  0.0533\n",
            "     50        \u001b[36m0.3861\u001b[0m  0.0546\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6561\u001b[0m  0.0413\n",
            "      2        0.6561  0.0592\n",
            "      3        0.6656  0.0513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.6393\u001b[0m  0.0539\n",
            "      5        \u001b[36m0.6351\u001b[0m  0.0515\n",
            "      6        \u001b[36m0.6281\u001b[0m  0.0458\n",
            "      7        \u001b[36m0.6211\u001b[0m  0.0462\n",
            "      8        0.6211  0.0501\n",
            "      9        \u001b[36m0.6035\u001b[0m  0.0476\n",
            "     10        \u001b[36m0.5754\u001b[0m  0.0518\n",
            "     11        \u001b[36m0.5754\u001b[0m  0.0577\n",
            "     12        \u001b[36m0.5719\u001b[0m  0.0534\n",
            "     13        \u001b[36m0.5684\u001b[0m  0.0551\n",
            "     14        0.5684  0.0546\n",
            "     15        \u001b[36m0.5684\u001b[0m  0.0527\n",
            "     16        \u001b[36m0.5649\u001b[0m  0.0572\n",
            "     17        \u001b[36m0.5614\u001b[0m  0.0518\n",
            "     18        \u001b[36m0.5579\u001b[0m  0.0526\n",
            "     19        0.5579  0.0528\n",
            "     20        0.5579  0.0471\n",
            "     21        0.5579  0.0519\n",
            "     22        0.5579  0.0482\n",
            "     23        0.5579  0.0476\n",
            "     24        0.5579  0.0507\n",
            "     25        0.5579  0.0558\n",
            "     26        0.5579  0.0474\n",
            "     27        0.5579  0.0506\n",
            "     28        0.5579  0.0495\n",
            "     29        0.5579  0.0540\n",
            "     30        0.5579  0.0595\n",
            "     31        0.5579  0.0548\n",
            "     32        \u001b[36m0.5579\u001b[0m  0.0491\n",
            "     33        \u001b[36m0.5544\u001b[0m  0.0533\n",
            "     34        0.5544  0.0523\n",
            "     35        0.5544  0.0571\n",
            "     36        \u001b[36m0.5544\u001b[0m  0.0599\n",
            "     37        \u001b[36m0.5509\u001b[0m  0.0522\n",
            "     38        0.5509  0.0506\n",
            "     39        0.5509  0.0505\n",
            "     40        0.5509  0.0535\n",
            "     41        0.5509  0.0471\n",
            "     42        0.5509  0.0463\n",
            "     43        0.5509  0.0489\n",
            "     44        0.5509  0.0479\n",
            "     45        0.5509  0.0450\n",
            "     46        0.5509  0.0467\n",
            "     47        0.5509  0.0559\n",
            "     48        0.5509  0.0646\n",
            "     49        0.5509  0.0541\n",
            "     50        \u001b[36m0.5509\u001b[0m  0.0538\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0348\n",
            "      2        1.0000  0.0405\n",
            "      3        1.0000  0.0420\n",
            "      4        1.0000  0.0418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0434\n",
            "      6        1.0000  0.0345\n",
            "      7        1.0000  0.0348\n",
            "      8        1.0000  0.0393\n",
            "      9        1.0000  0.0370\n",
            "     10        1.0000  0.0375\n",
            "     11        1.0000  0.0414\n",
            "     12        1.0000  0.0402\n",
            "     13        1.0000  0.0372\n",
            "     14        1.0000  0.0389\n",
            "     15        1.0000  0.0480\n",
            "     16        1.0000  0.0377\n",
            "     17        1.0000  0.0366\n",
            "     18        1.0000  0.0388\n",
            "     19        1.0000  0.0391\n",
            "     20        1.0000  0.0419\n",
            "     21        1.0000  0.0525\n",
            "     22        1.0000  0.0373\n",
            "     23        1.0000  0.0400\n",
            "     24        1.0000  0.0393\n",
            "     25        1.0000  0.0495\n",
            "     26        1.0000  0.0398\n",
            "     27        1.0000  0.0463\n",
            "     28        1.0000  0.0412\n",
            "     29        1.0000  0.0371\n",
            "     30        1.0000  0.0371\n",
            "     31        1.0000  0.0379\n",
            "     32        1.0000  0.0379\n",
            "     33        1.0000  0.0370\n",
            "     34        1.0000  0.0370\n",
            "     35        1.0000  0.0410\n",
            "     36        1.0000  0.0425\n",
            "     37        1.0000  0.0440\n",
            "     38        1.0000  0.0417\n",
            "     39        1.0000  0.0364\n",
            "     40        1.0000  0.0387\n",
            "     41        1.0000  0.0370\n",
            "     42        1.0000  0.0390\n",
            "     43        1.0000  0.0365\n",
            "     44        1.0000  0.0375\n",
            "     45        1.0000  0.0536\n",
            "     46        1.0000  0.0397\n",
            "     47        1.0000  0.0421\n",
            "     48        1.0000  0.0404\n",
            "     49        1.0000  0.0405\n",
            "     50        1.0000  0.0406\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0386\n",
            "      2        1.0000  0.0335\n",
            "      3        1.0000  0.0374\n",
            "      4        1.0000  0.0350\n",
            "      5        1.0000  0.0383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        1.0000  0.0424\n",
            "      7        1.0000  0.0417\n",
            "      8        1.0000  0.0428\n",
            "      9        1.0000  0.0381\n",
            "     10        1.0000  0.0367\n",
            "     11        1.0000  0.0434\n",
            "     12        1.0000  0.0381\n",
            "     13        1.0000  0.0384\n",
            "     14        1.0000  0.0386\n",
            "     15        1.0000  0.0407\n",
            "     16        1.0000  0.0460\n",
            "     17        1.0000  0.0375\n",
            "     18        1.0000  0.0471\n",
            "     19        1.0000  0.0444\n",
            "     20        1.0000  0.0385\n",
            "     21        1.0000  0.0419\n",
            "     22        1.0000  0.0405\n",
            "     23        1.0000  0.0402\n",
            "     24        1.0000  0.0390\n",
            "     25        1.0000  0.0394\n",
            "     26        1.0000  0.0382\n",
            "     27        1.0000  0.0454\n",
            "     28        1.0000  0.0377\n",
            "     29        1.0000  0.0385\n",
            "     30        1.0000  0.0352\n",
            "     31        1.0000  0.0339\n",
            "     32        1.0000  0.0358\n",
            "     33        1.0000  0.0380\n",
            "     34        1.0000  0.0364\n",
            "     35        1.0000  0.0366\n",
            "     36        1.0000  0.0364\n",
            "     37        1.0000  0.0402\n",
            "     38        1.0000  0.0450\n",
            "     39        1.0000  0.0353\n",
            "     40        1.0000  0.0371\n",
            "     41        1.0000  0.0372\n",
            "     42        1.0000  0.0349\n",
            "     43        1.0000  0.0488\n",
            "     44        1.0000  0.0440\n",
            "     45        1.0000  0.0383\n",
            "     46        1.0000  0.0417\n",
            "     47        1.0000  0.0414\n",
            "     48        1.0000  0.0390\n",
            "     49        1.0000  0.0483\n",
            "     50        1.0000  0.0451\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7326\u001b[0m  0.0449\n",
            "      2        \u001b[36m0.6972\u001b[0m  0.0517\n",
            "      3        \u001b[36m0.6937\u001b[0m  0.0483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        0.7042  0.0575\n",
            "      5        0.6972  0.0516\n",
            "      6        \u001b[36m0.6796\u001b[0m  0.0470\n",
            "      7        \u001b[36m0.6761\u001b[0m  0.0517\n",
            "      8        \u001b[36m0.6725\u001b[0m  0.0483\n",
            "      9        \u001b[36m0.6408\u001b[0m  0.0522\n",
            "     10        \u001b[36m0.6373\u001b[0m  0.0493\n",
            "     11        0.6373  0.0497\n",
            "     12        0.6373  0.0544\n",
            "     13        0.6373  0.0618\n",
            "     14        0.6373  0.0563\n",
            "     15        0.6373  0.0548\n",
            "     16        0.6373  0.0551\n",
            "     17        0.6373  0.0549\n",
            "     18        0.6373  0.0484\n",
            "     19        0.6373  0.0499\n",
            "     20        0.6373  0.0499\n",
            "     21        0.6373  0.0520\n",
            "     22        \u001b[36m0.6373\u001b[0m  0.0545\n",
            "     23        \u001b[36m0.6338\u001b[0m  0.0489\n",
            "     24        0.6338  0.0490\n",
            "     25        0.6338  0.0505\n",
            "     26        0.6338  0.0497\n",
            "     27        0.6338  0.0524\n",
            "     28        0.6338  0.0534\n",
            "     29        0.6338  0.0511\n",
            "     30        \u001b[36m0.6029\u001b[0m  0.0525\n",
            "     31        \u001b[36m0.5352\u001b[0m  0.0559\n",
            "     32        \u001b[36m0.5318\u001b[0m  0.0593\n",
            "     33        \u001b[36m0.5070\u001b[0m  0.0587\n",
            "     34        0.5070  0.0582\n",
            "     35        0.5070  0.0492\n",
            "     36        0.5070  0.0543\n",
            "     37        0.5070  0.0449\n",
            "     38        0.5070  0.0551\n",
            "     39        0.5070  0.0460\n",
            "     40        0.5070  0.0502\n",
            "     41        0.5070  0.0509\n",
            "     42        0.5070  0.0493\n",
            "     43        0.5070  0.0486\n",
            "     44        0.5070  0.0491\n",
            "     45        0.5070  0.0567\n",
            "     46        0.5070  0.0502\n",
            "     47        0.5070  0.0609\n",
            "     48        0.5070  0.0563\n",
            "     49        0.5070  0.0543\n",
            "     50        0.5070  0.0691\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9439\u001b[0m  0.0545\n",
            "      2        0.9474  0.0550\n",
            "      3        0.9509  0.0594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        0.9544  0.0574\n",
            "      5        0.9544  0.0516\n",
            "      6        0.9509  0.0532\n",
            "      7        \u001b[36m0.9333\u001b[0m  0.0498\n",
            "      8        \u001b[36m0.9298\u001b[0m  0.0544\n",
            "      9        \u001b[36m0.9018\u001b[0m  0.0582\n",
            "     10        \u001b[36m0.7789\u001b[0m  0.0568\n",
            "     11        \u001b[36m0.7754\u001b[0m  0.0545\n",
            "     12        0.7754  0.0571\n",
            "     13        0.7754  0.0530\n",
            "     14        0.7789  0.0570\n",
            "     15        0.7789  0.0547\n",
            "     16        0.7789  0.0556\n",
            "     17        0.7789  0.0684\n",
            "     18        0.7789  0.0553\n",
            "     19        0.7789  0.0602\n",
            "     20        0.7789  0.0627\n",
            "     21        0.7789  0.0597\n",
            "     22        0.7895  0.0453\n",
            "     23        0.7930  0.0463\n",
            "     24        0.7965  0.0616\n",
            "     25        0.7965  0.0481\n",
            "     26        0.7965  0.0469\n",
            "     27        0.7965  0.0562\n",
            "     28        0.7965  0.0467\n",
            "     29        0.7965  0.0526\n",
            "     30        0.7965  0.0498\n",
            "     31        0.7965  0.0484\n",
            "     32        0.8000  0.0613\n",
            "     33        0.8000  0.0529\n",
            "     34        0.8035  0.0568\n",
            "     35        0.8140  0.0557\n",
            "     36        0.8140  0.0582\n",
            "     37        0.8140  0.0589\n",
            "     38        0.8140  0.0635\n",
            "     39        0.8140  0.0584\n",
            "     40        0.8140  0.0494\n",
            "     41        0.8140  0.0533\n",
            "     42        0.8140  0.0478\n",
            "     43        0.8140  0.0487\n",
            "     44        0.8140  0.0513\n",
            "     45        0.8105  0.0491\n",
            "     46        0.8105  0.0487\n",
            "     47        0.8105  0.0630\n",
            "     48        0.8105  0.0584\n",
            "     49        0.8105  0.0631\n",
            "     50        0.8105  0.0535\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0365\n",
            "      2        1.0000  0.0529\n",
            "      3        1.0000  0.0437\n",
            "      4        1.0000  0.0425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0471\n",
            "      6        1.0000  0.0478\n",
            "      7        1.0000  0.0378\n",
            "      8        1.0000  0.0442\n",
            "      9        1.0000  0.0382\n",
            "     10        1.0000  0.0382\n",
            "     11        1.0000  0.0394\n",
            "     12        1.0000  0.0388\n",
            "     13        1.0000  0.0394\n",
            "     14        1.0000  0.0465\n",
            "     15        1.0000  0.0394\n",
            "     16        1.0000  0.0396\n",
            "     17        1.0000  0.0409\n",
            "     18        1.0000  0.0415\n",
            "     19        1.0000  0.0426\n",
            "     20        1.0000  0.0393\n",
            "     21        1.0000  0.0465\n",
            "     22        1.0000  0.0461\n",
            "     23        1.0000  0.0451\n",
            "     24        1.0000  0.0470\n",
            "     25        1.0000  0.0501\n",
            "     26        1.0000  0.0386\n",
            "     27        1.0000  0.0470\n",
            "     28        1.0000  0.0474\n",
            "     29        1.0000  0.0413\n",
            "     30        1.0000  0.0374\n",
            "     31        1.0000  0.0367\n",
            "     32        1.0000  0.0377\n",
            "     33        1.0000  0.0431\n",
            "     34        1.0000  0.0463\n",
            "     35        1.0000  0.0368\n",
            "     36        1.0000  0.0365\n",
            "     37        1.0000  0.0404\n",
            "     38        1.0000  0.0386\n",
            "     39        1.0000  0.0427\n",
            "     40        1.0000  0.0415\n",
            "     41        1.0000  0.0386\n",
            "     42        1.0000  0.0376\n",
            "     43        1.0000  0.0416\n",
            "     44        1.0000  0.0418\n",
            "     45        1.0000  0.0439\n",
            "     46        1.0000  0.0420\n",
            "     47        1.0000  0.0478\n",
            "     48        1.0000  0.0545\n",
            "     49        1.0000  0.0406\n",
            "     50        1.0000  0.0424\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.3860\u001b[0m  0.0312\n",
            "      2        0.3860  0.0414\n",
            "      3        0.3860  0.0440\n",
            "      4        0.3860  0.0398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        0.3860  0.0487\n",
            "      6        0.3860  0.0485\n",
            "      7        0.3860  0.0365\n",
            "      8        0.3860  0.0372\n",
            "      9        0.3860  0.0353\n",
            "     10        0.3860  0.0406\n",
            "     11        0.3860  0.0343\n",
            "     12        0.3860  0.0463\n",
            "     13        0.3860  0.0372\n",
            "     14        0.3860  0.0366\n",
            "     15        0.3860  0.0430\n",
            "     16        0.3860  0.0402\n",
            "     17        0.3860  0.0417\n",
            "     18        0.3860  0.0434\n",
            "     19        0.3860  0.0396\n",
            "     20        0.3860  0.0372\n",
            "     21        0.3860  0.0455\n",
            "     22        0.3860  0.0365\n",
            "     23        0.3860  0.0467\n",
            "     24        0.3860  0.0418\n",
            "     25        0.3860  0.0393\n",
            "     26        0.3860  0.0394\n",
            "     27        0.3860  0.0372\n",
            "     28        0.3860  0.0379\n",
            "     29        0.3860  0.0365\n",
            "     30        0.3860  0.0407\n",
            "     31        0.3860  0.0372\n",
            "     32        0.3860  0.0367\n",
            "     33        0.3860  0.0354\n",
            "     34        0.3860  0.0406\n",
            "     35        0.3860  0.0421\n",
            "     36        0.3860  0.0426\n",
            "     37        0.3860  0.0374\n",
            "     38        0.3860  0.0477\n",
            "     39        0.3860  0.0403\n",
            "     40        0.3860  0.0374\n",
            "     41        0.3860  0.0375\n",
            "     42        0.3860  0.0400\n",
            "     43        0.3860  0.0401\n",
            "     44        0.3860  0.0385\n",
            "     45        0.3860  0.0494\n",
            "     46        0.3860  0.0391\n",
            "     47        0.3860  0.0403\n",
            "     48        0.3860  0.0360\n",
            "     49        0.3860  0.0371\n",
            "     50        0.3860  0.0409\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9507\u001b[0m  0.0444\n",
            "      2        \u001b[36m0.9378\u001b[0m  0.0574\n",
            "      3        \u001b[36m0.9204\u001b[0m  0.0570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.8957\u001b[0m  0.0545\n",
            "      5        \u001b[36m0.8584\u001b[0m  0.0501\n",
            "      6        \u001b[36m0.7956\u001b[0m  0.0479\n",
            "      7        \u001b[36m0.6927\u001b[0m  0.0480\n",
            "      8        \u001b[36m0.5833\u001b[0m  0.0456\n",
            "      9        \u001b[36m0.5165\u001b[0m  0.0487\n",
            "     10        \u001b[36m0.4802\u001b[0m  0.0504\n",
            "     11        \u001b[36m0.4565\u001b[0m  0.0616\n",
            "     12        \u001b[36m0.4410\u001b[0m  0.0565\n",
            "     13        \u001b[36m0.4311\u001b[0m  0.0516\n",
            "     14        \u001b[36m0.4239\u001b[0m  0.0523\n",
            "     15        \u001b[36m0.4182\u001b[0m  0.0552\n",
            "     16        \u001b[36m0.4135\u001b[0m  0.0521\n",
            "     17        \u001b[36m0.4094\u001b[0m  0.0483\n",
            "     18        \u001b[36m0.4060\u001b[0m  0.0575\n",
            "     19        \u001b[36m0.4030\u001b[0m  0.0479\n",
            "     20        \u001b[36m0.4004\u001b[0m  0.0507\n",
            "     21        \u001b[36m0.3981\u001b[0m  0.0464\n",
            "     22        \u001b[36m0.3962\u001b[0m  0.0490\n",
            "     23        \u001b[36m0.3944\u001b[0m  0.0475\n",
            "     24        \u001b[36m0.3929\u001b[0m  0.0492\n",
            "     25        \u001b[36m0.3916\u001b[0m  0.0598\n",
            "     26        \u001b[36m0.3903\u001b[0m  0.0480\n",
            "     27        \u001b[36m0.3892\u001b[0m  0.0512\n",
            "     28        \u001b[36m0.3883\u001b[0m  0.0474\n",
            "     29        \u001b[36m0.3874\u001b[0m  0.0528\n",
            "     30        \u001b[36m0.3866\u001b[0m  0.0538\n",
            "     31        \u001b[36m0.3858\u001b[0m  0.0515\n",
            "     32        \u001b[36m0.3851\u001b[0m  0.0479\n",
            "     33        \u001b[36m0.3845\u001b[0m  0.0536\n",
            "     34        \u001b[36m0.3839\u001b[0m  0.0604\n",
            "     35        \u001b[36m0.3834\u001b[0m  0.0526\n",
            "     36        \u001b[36m0.3829\u001b[0m  0.0500\n",
            "     37        \u001b[36m0.3825\u001b[0m  0.0518\n",
            "     38        \u001b[36m0.3821\u001b[0m  0.0496\n",
            "     39        \u001b[36m0.3817\u001b[0m  0.0480\n",
            "     40        \u001b[36m0.3813\u001b[0m  0.0473\n",
            "     41        \u001b[36m0.3810\u001b[0m  0.0478\n",
            "     42        \u001b[36m0.3806\u001b[0m  0.0492\n",
            "     43        \u001b[36m0.3803\u001b[0m  0.0522\n",
            "     44        \u001b[36m0.3801\u001b[0m  0.0498\n",
            "     45        \u001b[36m0.3798\u001b[0m  0.0528\n",
            "     46        \u001b[36m0.3795\u001b[0m  0.0580\n",
            "     47        \u001b[36m0.3793\u001b[0m  0.0512\n",
            "     48        \u001b[36m0.3791\u001b[0m  0.0562\n",
            "     49        \u001b[36m0.3789\u001b[0m  0.0543\n",
            "     50        \u001b[36m0.3787\u001b[0m  0.0521\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9926\u001b[0m  0.0456\n",
            "      2        \u001b[36m0.9902\u001b[0m  0.0581\n",
            "      3        \u001b[36m0.9867\u001b[0m  0.0476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9806\u001b[0m  0.0558\n",
            "      5        \u001b[36m0.9673\u001b[0m  0.0471\n",
            "      6        \u001b[36m0.9281\u001b[0m  0.0503\n",
            "      7        \u001b[36m0.8158\u001b[0m  0.0544\n",
            "      8        \u001b[36m0.6392\u001b[0m  0.0457\n",
            "      9        \u001b[36m0.5162\u001b[0m  0.0490\n",
            "     10        \u001b[36m0.4617\u001b[0m  0.0486\n",
            "     11        \u001b[36m0.4356\u001b[0m  0.0465\n",
            "     12        \u001b[36m0.4195\u001b[0m  0.0521\n",
            "     13        \u001b[36m0.4069\u001b[0m  0.0477\n",
            "     14        \u001b[36m0.3986\u001b[0m  0.0537\n",
            "     15        \u001b[36m0.3944\u001b[0m  0.0584\n",
            "     16        \u001b[36m0.3917\u001b[0m  0.0537\n",
            "     17        \u001b[36m0.3897\u001b[0m  0.0568\n",
            "     18        \u001b[36m0.3881\u001b[0m  0.0529\n",
            "     19        \u001b[36m0.3868\u001b[0m  0.0541\n",
            "     20        \u001b[36m0.3856\u001b[0m  0.0543\n",
            "     21        \u001b[36m0.3846\u001b[0m  0.0549\n",
            "     22        \u001b[36m0.3837\u001b[0m  0.0472\n",
            "     23        \u001b[36m0.3829\u001b[0m  0.0542\n",
            "     24        \u001b[36m0.3822\u001b[0m  0.0480\n",
            "     25        \u001b[36m0.3815\u001b[0m  0.0457\n",
            "     26        \u001b[36m0.3810\u001b[0m  0.0474\n",
            "     27        \u001b[36m0.3805\u001b[0m  0.0453\n",
            "     28        \u001b[36m0.3800\u001b[0m  0.0472\n",
            "     29        \u001b[36m0.3795\u001b[0m  0.0505\n",
            "     30        \u001b[36m0.3791\u001b[0m  0.0508\n",
            "     31        \u001b[36m0.3788\u001b[0m  0.0511\n",
            "     32        \u001b[36m0.3784\u001b[0m  0.0533\n",
            "     33        \u001b[36m0.3781\u001b[0m  0.0500\n",
            "     34        \u001b[36m0.3778\u001b[0m  0.0542\n",
            "     35        \u001b[36m0.3776\u001b[0m  0.0545\n",
            "     36        \u001b[36m0.3773\u001b[0m  0.0529\n",
            "     37        \u001b[36m0.3771\u001b[0m  0.0592\n",
            "     38        \u001b[36m0.3769\u001b[0m  0.0523\n",
            "     39        \u001b[36m0.3767\u001b[0m  0.0475\n",
            "     40        \u001b[36m0.3765\u001b[0m  0.0728\n",
            "     41        \u001b[36m0.3763\u001b[0m  0.0490\n",
            "     42        \u001b[36m0.3761\u001b[0m  0.0480\n",
            "     43        \u001b[36m0.3760\u001b[0m  0.0488\n",
            "     44        \u001b[36m0.3758\u001b[0m  0.0489\n",
            "     45        \u001b[36m0.3757\u001b[0m  0.0506\n",
            "     46        \u001b[36m0.3755\u001b[0m  0.0485\n",
            "     47        \u001b[36m0.3754\u001b[0m  0.0492\n",
            "     48        \u001b[36m0.3753\u001b[0m  0.0499\n",
            "     49        \u001b[36m0.3752\u001b[0m  0.0527\n",
            "     50        \u001b[36m0.3751\u001b[0m  0.0574\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9873\u001b[0m  0.0345\n",
            "      2        \u001b[36m0.9873\u001b[0m  0.0463\n",
            "      3        \u001b[36m0.9872\u001b[0m  0.0451\n",
            "      4        \u001b[36m0.9872\u001b[0m  0.0388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.9871\u001b[0m  0.0503\n",
            "      6        \u001b[36m0.9871\u001b[0m  0.0379\n",
            "      7        \u001b[36m0.9871\u001b[0m  0.0390\n",
            "      8        \u001b[36m0.9870\u001b[0m  0.0347\n",
            "      9        \u001b[36m0.9870\u001b[0m  0.0358\n",
            "     10        \u001b[36m0.9869\u001b[0m  0.0467\n",
            "     11        \u001b[36m0.9869\u001b[0m  0.0370\n",
            "     12        \u001b[36m0.9868\u001b[0m  0.0410\n",
            "     13        \u001b[36m0.9868\u001b[0m  0.0348\n",
            "     14        \u001b[36m0.9868\u001b[0m  0.0373\n",
            "     15        \u001b[36m0.9867\u001b[0m  0.0397\n",
            "     16        \u001b[36m0.9867\u001b[0m  0.0400\n",
            "     17        \u001b[36m0.9866\u001b[0m  0.0372\n",
            "     18        \u001b[36m0.9866\u001b[0m  0.0370\n",
            "     19        \u001b[36m0.9865\u001b[0m  0.0431\n",
            "     20        \u001b[36m0.9865\u001b[0m  0.0430\n",
            "     21        \u001b[36m0.9864\u001b[0m  0.0484\n",
            "     22        \u001b[36m0.9864\u001b[0m  0.0409\n",
            "     23        \u001b[36m0.9864\u001b[0m  0.0406\n",
            "     24        \u001b[36m0.9863\u001b[0m  0.0472\n",
            "     25        \u001b[36m0.9863\u001b[0m  0.0402\n",
            "     26        \u001b[36m0.9862\u001b[0m  0.0394\n",
            "     27        \u001b[36m0.9862\u001b[0m  0.0384\n",
            "     28        \u001b[36m0.9861\u001b[0m  0.0486\n",
            "     29        \u001b[36m0.9861\u001b[0m  0.0510\n",
            "     30        \u001b[36m0.9860\u001b[0m  0.0484\n",
            "     31        \u001b[36m0.9860\u001b[0m  0.0418\n",
            "     32        \u001b[36m0.9859\u001b[0m  0.0357\n",
            "     33        \u001b[36m0.9859\u001b[0m  0.0546\n",
            "     34        \u001b[36m0.9858\u001b[0m  0.0413\n",
            "     35        \u001b[36m0.9858\u001b[0m  0.0455\n",
            "     36        \u001b[36m0.9857\u001b[0m  0.0387\n",
            "     37        \u001b[36m0.9857\u001b[0m  0.0389\n",
            "     38        \u001b[36m0.9856\u001b[0m  0.0377\n",
            "     39        \u001b[36m0.9856\u001b[0m  0.0399\n",
            "     40        \u001b[36m0.9855\u001b[0m  0.0367\n",
            "     41        \u001b[36m0.9855\u001b[0m  0.0436\n",
            "     42        \u001b[36m0.9854\u001b[0m  0.0369\n",
            "     43        \u001b[36m0.9854\u001b[0m  0.0382\n",
            "     44        \u001b[36m0.9853\u001b[0m  0.0457\n",
            "     45        \u001b[36m0.9853\u001b[0m  0.0380\n",
            "     46        \u001b[36m0.9852\u001b[0m  0.0407\n",
            "     47        \u001b[36m0.9852\u001b[0m  0.0448\n",
            "     48        \u001b[36m0.9851\u001b[0m  0.0513\n",
            "     49        \u001b[36m0.9851\u001b[0m  0.0400\n",
            "     50        \u001b[36m0.9850\u001b[0m  0.0436\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9833\u001b[0m  0.0334\n",
            "      2        \u001b[36m0.9832\u001b[0m  0.0425\n",
            "      3        \u001b[36m0.9831\u001b[0m  0.0456\n",
            "      4        \u001b[36m0.9831\u001b[0m  0.0404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.9830\u001b[0m  0.0572\n",
            "      6        \u001b[36m0.9829\u001b[0m  0.0492\n",
            "      7        \u001b[36m0.9828\u001b[0m  0.0391\n",
            "      8        \u001b[36m0.9828\u001b[0m  0.0409\n",
            "      9        \u001b[36m0.9827\u001b[0m  0.0378\n",
            "     10        \u001b[36m0.9826\u001b[0m  0.0369\n",
            "     11        \u001b[36m0.9825\u001b[0m  0.0380\n",
            "     12        \u001b[36m0.9825\u001b[0m  0.0348\n",
            "     13        \u001b[36m0.9824\u001b[0m  0.0443\n",
            "     14        \u001b[36m0.9823\u001b[0m  0.0442\n",
            "     15        \u001b[36m0.9822\u001b[0m  0.0417\n",
            "     16        \u001b[36m0.9822\u001b[0m  0.0467\n",
            "     17        \u001b[36m0.9821\u001b[0m  0.0441\n",
            "     18        \u001b[36m0.9820\u001b[0m  0.0483\n",
            "     19        \u001b[36m0.9819\u001b[0m  0.0521\n",
            "     20        \u001b[36m0.9818\u001b[0m  0.0540\n",
            "     21        \u001b[36m0.9817\u001b[0m  0.0474\n",
            "     22        \u001b[36m0.9817\u001b[0m  0.0431\n",
            "     23        \u001b[36m0.9816\u001b[0m  0.0393\n",
            "     24        \u001b[36m0.9815\u001b[0m  0.0509\n",
            "     25        \u001b[36m0.9814\u001b[0m  0.0431\n",
            "     26        \u001b[36m0.9813\u001b[0m  0.0395\n",
            "     27        \u001b[36m0.9812\u001b[0m  0.0385\n",
            "     28        \u001b[36m0.9812\u001b[0m  0.0479\n",
            "     29        \u001b[36m0.9811\u001b[0m  0.0404\n",
            "     30        \u001b[36m0.9810\u001b[0m  0.0398\n",
            "     31        \u001b[36m0.9809\u001b[0m  0.0375\n",
            "     32        \u001b[36m0.9808\u001b[0m  0.0399\n",
            "     33        \u001b[36m0.9807\u001b[0m  0.0393\n",
            "     34        \u001b[36m0.9806\u001b[0m  0.0362\n",
            "     35        \u001b[36m0.9805\u001b[0m  0.0406\n",
            "     36        \u001b[36m0.9804\u001b[0m  0.0405\n",
            "     37        \u001b[36m0.9803\u001b[0m  0.0400\n",
            "     38        \u001b[36m0.9802\u001b[0m  0.0378\n",
            "     39        \u001b[36m0.9801\u001b[0m  0.0426\n",
            "     40        \u001b[36m0.9800\u001b[0m  0.0380\n",
            "     41        \u001b[36m0.9799\u001b[0m  0.0391\n",
            "     42        \u001b[36m0.9798\u001b[0m  0.0478\n",
            "     43        \u001b[36m0.9797\u001b[0m  0.0447\n",
            "     44        \u001b[36m0.9796\u001b[0m  0.0453\n",
            "     45        \u001b[36m0.9795\u001b[0m  0.0515\n",
            "     46        \u001b[36m0.9794\u001b[0m  0.0422\n",
            "     47        \u001b[36m0.9793\u001b[0m  0.0389\n",
            "     48        \u001b[36m0.9792\u001b[0m  0.0390\n",
            "     49        \u001b[36m0.9791\u001b[0m  0.0399\n",
            "     50        \u001b[36m0.9790\u001b[0m  0.0447\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9999\u001b[0m  0.0574\n",
            "      2        \u001b[36m0.9998\u001b[0m  0.0553\n",
            "      3        \u001b[36m0.9997\u001b[0m  0.0510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9993\u001b[0m  0.0599\n",
            "      5        \u001b[36m0.9985\u001b[0m  0.0476\n",
            "      6        \u001b[36m0.9959\u001b[0m  0.0517\n",
            "      7        \u001b[36m0.9879\u001b[0m  0.0489\n",
            "      8        \u001b[36m0.9596\u001b[0m  0.0528\n",
            "      9        \u001b[36m0.8518\u001b[0m  0.0556\n",
            "     10        \u001b[36m0.6131\u001b[0m  0.0583\n",
            "     11        \u001b[36m0.4425\u001b[0m  0.0572\n",
            "     12        \u001b[36m0.3962\u001b[0m  0.0527\n",
            "     13        \u001b[36m0.3856\u001b[0m  0.0558\n",
            "     14        \u001b[36m0.3819\u001b[0m  0.0569\n",
            "     15        \u001b[36m0.3785\u001b[0m  0.0498\n",
            "     16        \u001b[36m0.3779\u001b[0m  0.0451\n",
            "     17        \u001b[36m0.3776\u001b[0m  0.0548\n",
            "     18        \u001b[36m0.3774\u001b[0m  0.0513\n",
            "     19        \u001b[36m0.3772\u001b[0m  0.0574\n",
            "     20        \u001b[36m0.3769\u001b[0m  0.0538\n",
            "     21        \u001b[36m0.3763\u001b[0m  0.0544\n",
            "     22        \u001b[36m0.3753\u001b[0m  0.0532\n",
            "     23        \u001b[36m0.3751\u001b[0m  0.0522\n",
            "     24        \u001b[36m0.3750\u001b[0m  0.0539\n",
            "     25        \u001b[36m0.3750\u001b[0m  0.0473\n",
            "     26        \u001b[36m0.3750\u001b[0m  0.0490\n",
            "     27        \u001b[36m0.3749\u001b[0m  0.0513\n",
            "     28        \u001b[36m0.3749\u001b[0m  0.0610\n",
            "     29        \u001b[36m0.3748\u001b[0m  0.0593\n",
            "     30        \u001b[36m0.3748\u001b[0m  0.0665\n",
            "     31        \u001b[36m0.3748\u001b[0m  0.0657\n",
            "     32        \u001b[36m0.3747\u001b[0m  0.0554\n",
            "     33        \u001b[36m0.3747\u001b[0m  0.0531\n",
            "     34        \u001b[36m0.3747\u001b[0m  0.0516\n",
            "     35        \u001b[36m0.3746\u001b[0m  0.0500\n",
            "     36        \u001b[36m0.3746\u001b[0m  0.0487\n",
            "     37        \u001b[36m0.3746\u001b[0m  0.0617\n",
            "     38        \u001b[36m0.3746\u001b[0m  0.0490\n",
            "     39        \u001b[36m0.3745\u001b[0m  0.0510\n",
            "     40        \u001b[36m0.3745\u001b[0m  0.0525\n",
            "     41        \u001b[36m0.3745\u001b[0m  0.0509\n",
            "     42        \u001b[36m0.3745\u001b[0m  0.0539\n",
            "     43        \u001b[36m0.3744\u001b[0m  0.0548\n",
            "     44        \u001b[36m0.3744\u001b[0m  0.0560\n",
            "     45        \u001b[36m0.3744\u001b[0m  0.0563\n",
            "     46        \u001b[36m0.3744\u001b[0m  0.0590\n",
            "     47        \u001b[36m0.3743\u001b[0m  0.0600\n",
            "     48        \u001b[36m0.3743\u001b[0m  0.0544\n",
            "     49        \u001b[36m0.3743\u001b[0m  0.0560\n",
            "     50        \u001b[36m0.3743\u001b[0m  0.0500\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9979\u001b[0m  0.0428\n",
            "      2        \u001b[36m0.9965\u001b[0m  0.0544\n",
            "      3        \u001b[36m0.9936\u001b[0m  0.0481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9869\u001b[0m  0.0583\n",
            "      5        \u001b[36m0.9701\u001b[0m  0.0562\n",
            "      6        \u001b[36m0.9268\u001b[0m  0.0473\n",
            "      7        \u001b[36m0.8295\u001b[0m  0.0494\n",
            "      8        \u001b[36m0.6821\u001b[0m  0.0638\n",
            "      9        \u001b[36m0.5553\u001b[0m  0.0545\n",
            "     10        \u001b[36m0.4843\u001b[0m  0.0493\n",
            "     11        \u001b[36m0.4482\u001b[0m  0.0568\n",
            "     12        \u001b[36m0.4282\u001b[0m  0.0546\n",
            "     13        \u001b[36m0.4157\u001b[0m  0.0552\n",
            "     14        \u001b[36m0.4058\u001b[0m  0.0674\n",
            "     15        \u001b[36m0.3933\u001b[0m  0.0522\n",
            "     16        \u001b[36m0.3868\u001b[0m  0.0469\n",
            "     17        \u001b[36m0.3851\u001b[0m  0.0506\n",
            "     18        \u001b[36m0.3840\u001b[0m  0.0495\n",
            "     19        \u001b[36m0.3830\u001b[0m  0.0556\n",
            "     20        \u001b[36m0.3822\u001b[0m  0.0646\n",
            "     21        \u001b[36m0.3815\u001b[0m  0.0569\n",
            "     22        \u001b[36m0.3808\u001b[0m  0.0495\n",
            "     23        \u001b[36m0.3796\u001b[0m  0.0568\n",
            "     24        \u001b[36m0.3777\u001b[0m  0.0452\n",
            "     25        \u001b[36m0.3772\u001b[0m  0.0471\n",
            "     26        \u001b[36m0.3769\u001b[0m  0.0504\n",
            "     27        \u001b[36m0.3767\u001b[0m  0.0545\n",
            "     28        \u001b[36m0.3765\u001b[0m  0.0498\n",
            "     29        \u001b[36m0.3763\u001b[0m  0.0555\n",
            "     30        \u001b[36m0.3762\u001b[0m  0.0596\n",
            "     31        \u001b[36m0.3760\u001b[0m  0.0566\n",
            "     32        \u001b[36m0.3759\u001b[0m  0.0591\n",
            "     33        \u001b[36m0.3757\u001b[0m  0.0499\n",
            "     34        \u001b[36m0.3756\u001b[0m  0.0554\n",
            "     35        \u001b[36m0.3755\u001b[0m  0.0482\n",
            "     36        \u001b[36m0.3753\u001b[0m  0.0540\n",
            "     37        \u001b[36m0.3752\u001b[0m  0.0511\n",
            "     38        \u001b[36m0.3751\u001b[0m  0.0502\n",
            "     39        \u001b[36m0.3750\u001b[0m  0.0479\n",
            "     40        \u001b[36m0.3749\u001b[0m  0.0524\n",
            "     41        \u001b[36m0.3748\u001b[0m  0.0571\n",
            "     42        \u001b[36m0.3747\u001b[0m  0.0497\n",
            "     43        \u001b[36m0.3746\u001b[0m  0.0547\n",
            "     44        \u001b[36m0.3745\u001b[0m  0.0618\n",
            "     45        \u001b[36m0.3745\u001b[0m  0.0567\n",
            "     46        \u001b[36m0.3744\u001b[0m  0.0547\n",
            "     47        \u001b[36m0.3743\u001b[0m  0.0546\n",
            "     48        \u001b[36m0.3742\u001b[0m  0.0496\n",
            "     49        \u001b[36m0.3742\u001b[0m  0.0576\n",
            "     50        \u001b[36m0.3741\u001b[0m  0.0576\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9999\u001b[0m  0.0444\n",
            "      2        0.9999  0.0390\n",
            "      3        0.9999  0.0396\n",
            "      4        0.9999  0.0365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        0.9999  0.0461\n",
            "      6        0.9999  0.0390\n",
            "      7        0.9999  0.0384\n",
            "      8        0.9999  0.0374\n",
            "      9        0.9999  0.0434\n",
            "     10        \u001b[36m0.9999\u001b[0m  0.0442\n",
            "     11        \u001b[36m0.9999\u001b[0m  0.0455\n",
            "     12        0.9999  0.0363\n",
            "     13        0.9999  0.0449\n",
            "     14        0.9999  0.0417\n",
            "     15        0.9999  0.0434\n",
            "     16        0.9999  0.0393\n",
            "     17        0.9999  0.0396\n",
            "     18        0.9999  0.0410\n",
            "     19        0.9999  0.0397\n",
            "     20        0.9999  0.0381\n",
            "     21        \u001b[36m0.9999\u001b[0m  0.0505\n",
            "     22        \u001b[36m0.9999\u001b[0m  0.0380\n",
            "     23        0.9999  0.0373\n",
            "     24        0.9999  0.0392\n",
            "     25        0.9999  0.0379\n",
            "     26        0.9999  0.0414\n",
            "     27        0.9999  0.0378\n",
            "     28        0.9999  0.0374\n",
            "     29        0.9999  0.0408\n",
            "     30        0.9999  0.0400\n",
            "     31        0.9999  0.0413\n",
            "     32        0.9999  0.0397\n",
            "     33        \u001b[36m0.9999\u001b[0m  0.0377\n",
            "     34        \u001b[36m0.9999\u001b[0m  0.0397\n",
            "     35        0.9999  0.0471\n",
            "     36        0.9999  0.0445\n",
            "     37        0.9999  0.0412\n",
            "     38        0.9999  0.0377\n",
            "     39        0.9999  0.0416\n",
            "     40        0.9999  0.0421\n",
            "     41        0.9999  0.0382\n",
            "     42        0.9999  0.0426\n",
            "     43        0.9999  0.0429\n",
            "     44        \u001b[36m0.9999\u001b[0m  0.0369\n",
            "     45        \u001b[36m0.9999\u001b[0m  0.0430\n",
            "     46        0.9999  0.0486\n",
            "     47        0.9999  0.0393\n",
            "     48        0.9999  0.0350\n",
            "     49        0.9999  0.0413\n",
            "     50        0.9999  0.0471\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9999\u001b[0m  0.0331\n",
            "      2        0.9999  0.0405\n",
            "      3        0.9999  0.0377\n",
            "      4        0.9999  0.0374\n",
            "      5        0.9999  0.0357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        0.9999  0.0423\n",
            "      7        0.9999  0.0394\n",
            "      8        \u001b[36m0.9999\u001b[0m  0.0410\n",
            "      9        \u001b[36m0.9999\u001b[0m  0.0376\n",
            "     10        0.9999  0.0359\n",
            "     11        0.9999  0.0352\n",
            "     12        0.9999  0.0392\n",
            "     13        0.9999  0.0353\n",
            "     14        0.9999  0.0376\n",
            "     15        0.9999  0.0382\n",
            "     16        0.9999  0.0362\n",
            "     17        0.9999  0.0430\n",
            "     18        0.9999  0.0401\n",
            "     19        \u001b[36m0.9999\u001b[0m  0.0416\n",
            "     20        \u001b[36m0.9999\u001b[0m  0.0462\n",
            "     21        0.9999  0.0422\n",
            "     22        0.9999  0.0439\n",
            "     23        0.9999  0.0365\n",
            "     24        0.9999  0.0404\n",
            "     25        0.9999  0.0380\n",
            "     26        0.9999  0.0407\n",
            "     27        0.9999  0.0377\n",
            "     28        0.9999  0.0399\n",
            "     29        0.9999  0.0478\n",
            "     30        \u001b[36m0.9999\u001b[0m  0.0361\n",
            "     31        \u001b[36m0.9999\u001b[0m  0.0369\n",
            "     32        0.9999  0.0464\n",
            "     33        0.9999  0.0425\n",
            "     34        0.9999  0.0415\n",
            "     35        0.9999  0.0389\n",
            "     36        0.9999  0.0397\n",
            "     37        0.9999  0.0452\n",
            "     38        0.9999  0.0465\n",
            "     39        0.9999  0.0442\n",
            "     40        0.9999  0.0448\n",
            "     41        0.9999  0.0412\n",
            "     42        \u001b[36m0.9999\u001b[0m  0.0461\n",
            "     43        0.9999  0.0406\n",
            "     44        0.9999  0.0437\n",
            "     45        0.9999  0.0370\n",
            "     46        0.9999  0.0405\n",
            "     47        0.9999  0.0407\n",
            "     48        0.9999  0.0470\n",
            "     49        0.9999  0.0445\n",
            "     50        0.9999  0.0375\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6561\u001b[0m  0.0432\n",
            "      2        \u001b[36m0.6298\u001b[0m  0.0534\n",
            "      3        \u001b[36m0.6156\u001b[0m  0.0488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.6033\u001b[0m  0.0781\n",
            "      5        \u001b[36m0.5900\u001b[0m  0.0521\n",
            "      6        \u001b[36m0.5716\u001b[0m  0.0605\n",
            "      7        \u001b[36m0.5516\u001b[0m  0.0611\n",
            "      8        \u001b[36m0.5267\u001b[0m  0.0695\n",
            "      9        \u001b[36m0.4969\u001b[0m  0.0674\n",
            "     10        \u001b[36m0.4711\u001b[0m  0.0637\n",
            "     11        \u001b[36m0.4495\u001b[0m  0.0667\n",
            "     12        \u001b[36m0.4341\u001b[0m  0.0551\n",
            "     13        \u001b[36m0.4221\u001b[0m  0.0700\n",
            "     14        \u001b[36m0.4132\u001b[0m  0.0505\n",
            "     15        \u001b[36m0.4068\u001b[0m  0.0567\n",
            "     16        \u001b[36m0.4016\u001b[0m  0.0555\n",
            "     17        \u001b[36m0.3973\u001b[0m  0.0507\n",
            "     18        \u001b[36m0.3933\u001b[0m  0.0561\n",
            "     19        \u001b[36m0.3897\u001b[0m  0.0516\n",
            "     20        \u001b[36m0.3864\u001b[0m  0.0599\n",
            "     21        \u001b[36m0.3835\u001b[0m  0.0501\n",
            "     22        \u001b[36m0.3815\u001b[0m  0.0521\n",
            "     23        \u001b[36m0.3801\u001b[0m  0.0560\n",
            "     24        \u001b[36m0.3791\u001b[0m  0.0551\n",
            "     25        \u001b[36m0.3785\u001b[0m  0.0529\n",
            "     26        \u001b[36m0.3780\u001b[0m  0.0513\n",
            "     27        \u001b[36m0.3776\u001b[0m  0.0503\n",
            "     28        \u001b[36m0.3773\u001b[0m  0.0526\n",
            "     29        \u001b[36m0.3771\u001b[0m  0.0486\n",
            "     30        \u001b[36m0.3769\u001b[0m  0.0482\n",
            "     31        \u001b[36m0.3767\u001b[0m  0.0468\n",
            "     32        \u001b[36m0.3765\u001b[0m  0.0524\n",
            "     33        \u001b[36m0.3764\u001b[0m  0.0498\n",
            "     34        \u001b[36m0.3763\u001b[0m  0.0535\n",
            "     35        \u001b[36m0.3761\u001b[0m  0.0526\n",
            "     36        \u001b[36m0.3760\u001b[0m  0.0529\n",
            "     37        \u001b[36m0.3759\u001b[0m  0.0568\n",
            "     38        \u001b[36m0.3759\u001b[0m  0.0599\n",
            "     39        0.3764  0.0621\n",
            "     40        0.3762  0.0519\n",
            "     41        0.3761  0.0534\n",
            "     42        0.3760  0.0510\n",
            "     43        \u001b[36m0.3758\u001b[0m  0.0576\n",
            "     44        \u001b[36m0.3757\u001b[0m  0.0576\n",
            "     45        \u001b[36m0.3757\u001b[0m  0.0684\n",
            "     46        \u001b[36m0.3756\u001b[0m  0.0626\n",
            "     47        \u001b[36m0.3755\u001b[0m  0.0502\n",
            "     48        \u001b[36m0.3754\u001b[0m  0.0525\n",
            "     49        \u001b[36m0.3754\u001b[0m  0.0473\n",
            "     50        \u001b[36m0.3753\u001b[0m  0.0511\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4688\u001b[0m  0.0462\n",
            "      2        \u001b[36m0.4511\u001b[0m  0.0513\n",
            "      3        \u001b[36m0.4397\u001b[0m  0.0545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.4315\u001b[0m  0.0521\n",
            "      5        \u001b[36m0.4259\u001b[0m  0.0483\n",
            "      6        \u001b[36m0.4227\u001b[0m  0.0554\n",
            "      7        \u001b[36m0.4203\u001b[0m  0.0482\n",
            "      8        \u001b[36m0.4113\u001b[0m  0.0562\n",
            "      9        \u001b[36m0.4074\u001b[0m  0.0560\n",
            "     10        \u001b[36m0.4039\u001b[0m  0.0560\n",
            "     11        \u001b[36m0.4006\u001b[0m  0.0510\n",
            "     12        \u001b[36m0.3977\u001b[0m  0.0493\n",
            "     13        \u001b[36m0.3953\u001b[0m  0.0452\n",
            "     14        \u001b[36m0.3943\u001b[0m  0.0486\n",
            "     15        \u001b[36m0.3925\u001b[0m  0.0468\n",
            "     16        \u001b[36m0.3910\u001b[0m  0.0579\n",
            "     17        \u001b[36m0.3898\u001b[0m  0.0482\n",
            "     18        \u001b[36m0.3886\u001b[0m  0.0513\n",
            "     19        \u001b[36m0.3876\u001b[0m  0.0473\n",
            "     20        \u001b[36m0.3867\u001b[0m  0.0468\n",
            "     21        \u001b[36m0.3859\u001b[0m  0.0531\n",
            "     22        \u001b[36m0.3852\u001b[0m  0.0517\n",
            "     23        \u001b[36m0.3845\u001b[0m  0.0517\n",
            "     24        \u001b[36m0.3838\u001b[0m  0.0510\n",
            "     25        \u001b[36m0.3833\u001b[0m  0.0600\n",
            "     26        \u001b[36m0.3827\u001b[0m  0.0535\n",
            "     27        \u001b[36m0.3822\u001b[0m  0.0565\n",
            "     28        \u001b[36m0.3817\u001b[0m  0.0480\n",
            "     29        \u001b[36m0.3812\u001b[0m  0.0557\n",
            "     30        \u001b[36m0.3809\u001b[0m  0.0542\n",
            "     31        \u001b[36m0.3804\u001b[0m  0.0553\n",
            "     32        \u001b[36m0.3801\u001b[0m  0.0620\n",
            "     33        \u001b[36m0.3798\u001b[0m  0.0565\n",
            "     34        \u001b[36m0.3794\u001b[0m  0.0501\n",
            "     35        \u001b[36m0.3790\u001b[0m  0.0497\n",
            "     36        \u001b[36m0.3775\u001b[0m  0.0511\n",
            "     37        \u001b[36m0.3773\u001b[0m  0.0510\n",
            "     38        \u001b[36m0.3770\u001b[0m  0.0459\n",
            "     39        \u001b[36m0.3768\u001b[0m  0.0495\n",
            "     40        \u001b[36m0.3765\u001b[0m  0.0570\n",
            "     41        \u001b[36m0.3764\u001b[0m  0.0526\n",
            "     42        \u001b[36m0.3762\u001b[0m  0.0489\n",
            "     43        \u001b[36m0.3760\u001b[0m  0.0595\n",
            "     44        \u001b[36m0.3758\u001b[0m  0.0540\n",
            "     45        \u001b[36m0.3756\u001b[0m  0.0481\n",
            "     46        \u001b[36m0.3754\u001b[0m  0.0613\n",
            "     47        \u001b[36m0.3753\u001b[0m  0.0596\n",
            "     48        \u001b[36m0.3750\u001b[0m  0.0534\n",
            "     49        \u001b[36m0.3749\u001b[0m  0.0564\n",
            "     50        \u001b[36m0.3747\u001b[0m  0.0512\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4502\u001b[0m  0.0326\n",
            "      2        \u001b[36m0.4498\u001b[0m  0.0362\n",
            "      3        \u001b[36m0.4493\u001b[0m  0.0371\n",
            "      4        \u001b[36m0.4489\u001b[0m  0.0366\n",
            "      5        \u001b[36m0.4486\u001b[0m  0.0413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m0.4482\u001b[0m  0.0433\n",
            "      7        \u001b[36m0.4479\u001b[0m  0.0371\n",
            "      8        \u001b[36m0.4476\u001b[0m  0.0375\n",
            "      9        \u001b[36m0.4472\u001b[0m  0.0390\n",
            "     10        \u001b[36m0.4469\u001b[0m  0.0374\n",
            "     11        \u001b[36m0.4466\u001b[0m  0.0393\n",
            "     12        \u001b[36m0.4463\u001b[0m  0.0395\n",
            "     13        \u001b[36m0.4460\u001b[0m  0.0389\n",
            "     14        \u001b[36m0.4457\u001b[0m  0.0415\n",
            "     15        \u001b[36m0.4454\u001b[0m  0.0500\n",
            "     16        \u001b[36m0.4451\u001b[0m  0.0402\n",
            "     17        \u001b[36m0.4448\u001b[0m  0.0420\n",
            "     18        \u001b[36m0.4445\u001b[0m  0.0474\n",
            "     19        \u001b[36m0.4442\u001b[0m  0.0388\n",
            "     20        \u001b[36m0.4439\u001b[0m  0.0370\n",
            "     21        \u001b[36m0.4437\u001b[0m  0.0356\n",
            "     22        \u001b[36m0.4434\u001b[0m  0.0369\n",
            "     23        \u001b[36m0.4431\u001b[0m  0.0372\n",
            "     24        \u001b[36m0.4429\u001b[0m  0.0391\n",
            "     25        \u001b[36m0.4426\u001b[0m  0.0384\n",
            "     26        \u001b[36m0.4423\u001b[0m  0.0448\n",
            "     27        \u001b[36m0.4421\u001b[0m  0.0363\n",
            "     28        \u001b[36m0.4418\u001b[0m  0.0378\n",
            "     29        \u001b[36m0.4416\u001b[0m  0.0382\n",
            "     30        \u001b[36m0.4413\u001b[0m  0.0368\n",
            "     31        \u001b[36m0.4411\u001b[0m  0.0387\n",
            "     32        \u001b[36m0.4409\u001b[0m  0.0389\n",
            "     33        \u001b[36m0.4406\u001b[0m  0.0348\n",
            "     34        \u001b[36m0.4404\u001b[0m  0.0407\n",
            "     35        \u001b[36m0.4401\u001b[0m  0.0475\n",
            "     36        \u001b[36m0.4399\u001b[0m  0.0405\n",
            "     37        \u001b[36m0.4397\u001b[0m  0.0371\n",
            "     38        \u001b[36m0.4395\u001b[0m  0.0407\n",
            "     39        \u001b[36m0.4392\u001b[0m  0.0507\n",
            "     40        \u001b[36m0.4390\u001b[0m  0.0432\n",
            "     41        \u001b[36m0.4388\u001b[0m  0.0424\n",
            "     42        \u001b[36m0.4386\u001b[0m  0.0482\n",
            "     43        \u001b[36m0.4384\u001b[0m  0.0456\n",
            "     44        \u001b[36m0.4382\u001b[0m  0.0401\n",
            "     45        \u001b[36m0.4380\u001b[0m  0.0413\n",
            "     46        \u001b[36m0.4378\u001b[0m  0.0404\n",
            "     47        \u001b[36m0.4376\u001b[0m  0.0385\n",
            "     48        \u001b[36m0.4374\u001b[0m  0.0378\n",
            "     49        \u001b[36m0.4372\u001b[0m  0.0404\n",
            "     50        \u001b[36m0.4370\u001b[0m  0.0393\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8698\u001b[0m  0.0327\n",
            "      2        \u001b[36m0.8655\u001b[0m  0.0384\n",
            "      3        \u001b[36m0.8636\u001b[0m  0.0393\n",
            "      4        \u001b[36m0.8618\u001b[0m  0.0438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.8599\u001b[0m  0.0504\n",
            "      6        \u001b[36m0.8580\u001b[0m  0.0375\n",
            "      7        \u001b[36m0.8560\u001b[0m  0.0411\n",
            "      8        \u001b[36m0.8540\u001b[0m  0.0401\n",
            "      9        \u001b[36m0.8520\u001b[0m  0.0405\n",
            "     10        \u001b[36m0.8499\u001b[0m  0.0385\n",
            "     11        \u001b[36m0.8477\u001b[0m  0.0431\n",
            "     12        \u001b[36m0.8455\u001b[0m  0.0431\n",
            "     13        \u001b[36m0.8432\u001b[0m  0.0387\n",
            "     14        \u001b[36m0.8409\u001b[0m  0.0408\n",
            "     15        \u001b[36m0.8386\u001b[0m  0.0405\n",
            "     16        \u001b[36m0.8361\u001b[0m  0.0406\n",
            "     17        \u001b[36m0.8336\u001b[0m  0.0452\n",
            "     18        \u001b[36m0.8311\u001b[0m  0.0378\n",
            "     19        \u001b[36m0.8285\u001b[0m  0.0390\n",
            "     20        \u001b[36m0.8258\u001b[0m  0.0355\n",
            "     21        \u001b[36m0.8230\u001b[0m  0.0388\n",
            "     22        \u001b[36m0.8202\u001b[0m  0.0362\n",
            "     23        \u001b[36m0.8173\u001b[0m  0.0394\n",
            "     24        \u001b[36m0.8144\u001b[0m  0.0422\n",
            "     25        \u001b[36m0.8113\u001b[0m  0.0430\n",
            "     26        \u001b[36m0.8082\u001b[0m  0.0389\n",
            "     27        \u001b[36m0.8050\u001b[0m  0.0363\n",
            "     28        \u001b[36m0.8017\u001b[0m  0.0355\n",
            "     29        \u001b[36m0.7984\u001b[0m  0.0371\n",
            "     30        \u001b[36m0.7950\u001b[0m  0.0409\n",
            "     31        \u001b[36m0.7915\u001b[0m  0.0498\n",
            "     32        \u001b[36m0.7879\u001b[0m  0.0361\n",
            "     33        \u001b[36m0.7842\u001b[0m  0.0371\n",
            "     34        \u001b[36m0.7805\u001b[0m  0.0382\n",
            "     35        \u001b[36m0.7767\u001b[0m  0.0398\n",
            "     36        \u001b[36m0.7728\u001b[0m  0.0519\n",
            "     37        \u001b[36m0.7688\u001b[0m  0.0407\n",
            "     38        \u001b[36m0.7648\u001b[0m  0.0423\n",
            "     39        \u001b[36m0.7607\u001b[0m  0.0428\n",
            "     40        \u001b[36m0.7566\u001b[0m  0.0399\n",
            "     41        \u001b[36m0.7524\u001b[0m  0.0408\n",
            "     42        \u001b[36m0.7482\u001b[0m  0.0399\n",
            "     43        \u001b[36m0.7439\u001b[0m  0.0395\n",
            "     44        \u001b[36m0.7396\u001b[0m  0.0357\n",
            "     45        \u001b[36m0.7352\u001b[0m  0.0404\n",
            "     46        \u001b[36m0.7308\u001b[0m  0.0370\n",
            "     47        \u001b[36m0.7264\u001b[0m  0.0375\n",
            "     48        \u001b[36m0.7220\u001b[0m  0.0370\n",
            "     49        \u001b[36m0.7176\u001b[0m  0.0375\n",
            "     50        \u001b[36m0.7131\u001b[0m  0.0366\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6338\u001b[0m  0.0421\n",
            "      2        \u001b[36m0.5730\u001b[0m  0.0530\n",
            "      3        \u001b[36m0.5386\u001b[0m  0.0495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.5175\u001b[0m  0.0492\n",
            "      5        \u001b[36m0.5053\u001b[0m  0.0543\n",
            "      6        \u001b[36m0.4984\u001b[0m  0.0460\n",
            "      7        \u001b[36m0.4907\u001b[0m  0.0502\n",
            "      8        \u001b[36m0.4848\u001b[0m  0.0610\n",
            "      9        \u001b[36m0.4787\u001b[0m  0.0573\n",
            "     10        \u001b[36m0.4717\u001b[0m  0.0547\n",
            "     11        \u001b[36m0.4659\u001b[0m  0.0545\n",
            "     12        \u001b[36m0.4602\u001b[0m  0.0491\n",
            "     13        \u001b[36m0.4543\u001b[0m  0.0631\n",
            "     14        \u001b[36m0.4503\u001b[0m  0.0436\n",
            "     15        \u001b[36m0.4435\u001b[0m  0.0491\n",
            "     16        \u001b[36m0.4399\u001b[0m  0.0482\n",
            "     17        \u001b[36m0.4335\u001b[0m  0.0532\n",
            "     18        \u001b[36m0.4286\u001b[0m  0.0505\n",
            "     19        \u001b[36m0.4242\u001b[0m  0.0473\n",
            "     20        \u001b[36m0.4182\u001b[0m  0.0537\n",
            "     21        \u001b[36m0.4114\u001b[0m  0.0498\n",
            "     22        \u001b[36m0.4085\u001b[0m  0.0500\n",
            "     23        \u001b[36m0.4033\u001b[0m  0.0526\n",
            "     24        \u001b[36m0.3995\u001b[0m  0.0572\n",
            "     25        \u001b[36m0.3959\u001b[0m  0.0571\n",
            "     26        \u001b[36m0.3932\u001b[0m  0.0665\n",
            "     27        \u001b[36m0.3907\u001b[0m  0.0508\n",
            "     28        \u001b[36m0.3893\u001b[0m  0.0481\n",
            "     29        \u001b[36m0.3872\u001b[0m  0.0503\n",
            "     30        \u001b[36m0.3854\u001b[0m  0.0542\n",
            "     31        \u001b[36m0.3841\u001b[0m  0.0555\n",
            "     32        \u001b[36m0.3830\u001b[0m  0.0533\n",
            "     33        \u001b[36m0.3821\u001b[0m  0.0476\n",
            "     34        \u001b[36m0.3813\u001b[0m  0.0461\n",
            "     35        0.3818  0.0518\n",
            "     36        \u001b[36m0.3812\u001b[0m  0.0498\n",
            "     37        \u001b[36m0.3807\u001b[0m  0.0494\n",
            "     38        \u001b[36m0.3803\u001b[0m  0.0504\n",
            "     39        \u001b[36m0.3798\u001b[0m  0.0500\n",
            "     40        \u001b[36m0.3794\u001b[0m  0.0547\n",
            "     41        \u001b[36m0.3786\u001b[0m  0.0505\n",
            "     42        \u001b[36m0.3783\u001b[0m  0.0506\n",
            "     43        \u001b[36m0.3780\u001b[0m  0.0480\n",
            "     44        \u001b[36m0.3777\u001b[0m  0.0471\n",
            "     45        \u001b[36m0.3775\u001b[0m  0.0637\n",
            "     46        \u001b[36m0.3773\u001b[0m  0.0517\n",
            "     47        \u001b[36m0.3771\u001b[0m  0.0550\n",
            "     48        \u001b[36m0.3770\u001b[0m  0.0539\n",
            "     49        \u001b[36m0.3767\u001b[0m  0.0575\n",
            "     50        \u001b[36m0.3766\u001b[0m  0.0583\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9345\u001b[0m  0.0449\n",
            "      2        \u001b[36m0.9162\u001b[0m  0.0481\n",
            "      3        \u001b[36m0.8940\u001b[0m  0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.8603\u001b[0m  0.0531\n",
            "      5        \u001b[36m0.8061\u001b[0m  0.0489\n",
            "      6        \u001b[36m0.7435\u001b[0m  0.0511\n",
            "      7        \u001b[36m0.6677\u001b[0m  0.0656\n",
            "      8        \u001b[36m0.5823\u001b[0m  0.0559\n",
            "      9        \u001b[36m0.4627\u001b[0m  0.0521\n",
            "     10        \u001b[36m0.4306\u001b[0m  0.0483\n",
            "     11        \u001b[36m0.4175\u001b[0m  0.0650\n",
            "     12        \u001b[36m0.4085\u001b[0m  0.0665\n",
            "     13        \u001b[36m0.4005\u001b[0m  0.0557\n",
            "     14        \u001b[36m0.3960\u001b[0m  0.0569\n",
            "     15        \u001b[36m0.3905\u001b[0m  0.0502\n",
            "     16        \u001b[36m0.3884\u001b[0m  0.0537\n",
            "     17        \u001b[36m0.3863\u001b[0m  0.0597\n",
            "     18        \u001b[36m0.3849\u001b[0m  0.0528\n",
            "     19        0.3855  0.0512\n",
            "     20        \u001b[36m0.3838\u001b[0m  0.0554\n",
            "     21        0.3839  0.0534\n",
            "     22        \u001b[36m0.3824\u001b[0m  0.0590\n",
            "     23        \u001b[36m0.3812\u001b[0m  0.0455\n",
            "     24        \u001b[36m0.3802\u001b[0m  0.0542\n",
            "     25        \u001b[36m0.3794\u001b[0m  0.0488\n",
            "     26        \u001b[36m0.3787\u001b[0m  0.0480\n",
            "     27        \u001b[36m0.3782\u001b[0m  0.0566\n",
            "     28        \u001b[36m0.3777\u001b[0m  0.0510\n",
            "     29        \u001b[36m0.3772\u001b[0m  0.0570\n",
            "     30        \u001b[36m0.3770\u001b[0m  0.0705\n",
            "     31        \u001b[36m0.3764\u001b[0m  0.0537\n",
            "     32        \u001b[36m0.3761\u001b[0m  0.0515\n",
            "     33        \u001b[36m0.3759\u001b[0m  0.0585\n",
            "     34        \u001b[36m0.3756\u001b[0m  0.0513\n",
            "     35        \u001b[36m0.3754\u001b[0m  0.0555\n",
            "     36        \u001b[36m0.3752\u001b[0m  0.0586\n",
            "     37        \u001b[36m0.3749\u001b[0m  0.0571\n",
            "     38        \u001b[36m0.3747\u001b[0m  0.0552\n",
            "     39        \u001b[36m0.3746\u001b[0m  0.0489\n",
            "     40        \u001b[36m0.3745\u001b[0m  0.0549\n",
            "     41        0.3750  0.0572\n",
            "     42        0.3748  0.0600\n",
            "     43        \u001b[36m0.3743\u001b[0m  0.0585\n",
            "     44        \u001b[36m0.3741\u001b[0m  0.0655\n",
            "     45        \u001b[36m0.3737\u001b[0m  0.0548\n",
            "     46        \u001b[36m0.3737\u001b[0m  0.0551\n",
            "     47        \u001b[36m0.3736\u001b[0m  0.0732\n",
            "     48        \u001b[36m0.3735\u001b[0m  0.0497\n",
            "     49        \u001b[36m0.3734\u001b[0m  0.0480\n",
            "     50        \u001b[36m0.3733\u001b[0m  0.0515\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7818\u001b[0m  0.0336\n",
            "      2        \u001b[36m0.7724\u001b[0m  0.0390\n",
            "      3        \u001b[36m0.7667\u001b[0m  0.0506\n",
            "      4        \u001b[36m0.7604\u001b[0m  0.0370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.7536\u001b[0m  0.0478\n",
            "      6        \u001b[36m0.7463\u001b[0m  0.0478\n",
            "      7        \u001b[36m0.7233\u001b[0m  0.0401\n",
            "      8        \u001b[36m0.7187\u001b[0m  0.0423\n",
            "      9        \u001b[36m0.7144\u001b[0m  0.0456\n",
            "     10        \u001b[36m0.7103\u001b[0m  0.0397\n",
            "     11        \u001b[36m0.7065\u001b[0m  0.0428\n",
            "     12        \u001b[36m0.7029\u001b[0m  0.0461\n",
            "     13        \u001b[36m0.6997\u001b[0m  0.0410\n",
            "     14        \u001b[36m0.6968\u001b[0m  0.0395\n",
            "     15        \u001b[36m0.6941\u001b[0m  0.0386\n",
            "     16        \u001b[36m0.6917\u001b[0m  0.0411\n",
            "     17        \u001b[36m0.6894\u001b[0m  0.0363\n",
            "     18        \u001b[36m0.6873\u001b[0m  0.0394\n",
            "     19        \u001b[36m0.6854\u001b[0m  0.0506\n",
            "     20        \u001b[36m0.6836\u001b[0m  0.0416\n",
            "     21        \u001b[36m0.6819\u001b[0m  0.0403\n",
            "     22        \u001b[36m0.6803\u001b[0m  0.0392\n",
            "     23        \u001b[36m0.6787\u001b[0m  0.0489\n",
            "     24        \u001b[36m0.6773\u001b[0m  0.0397\n",
            "     25        \u001b[36m0.6759\u001b[0m  0.0388\n",
            "     26        \u001b[36m0.6746\u001b[0m  0.0388\n",
            "     27        \u001b[36m0.6733\u001b[0m  0.0400\n",
            "     28        \u001b[36m0.6720\u001b[0m  0.0406\n",
            "     29        \u001b[36m0.6708\u001b[0m  0.0446\n",
            "     30        \u001b[36m0.6697\u001b[0m  0.0418\n",
            "     31        \u001b[36m0.6685\u001b[0m  0.0427\n",
            "     32        \u001b[36m0.6674\u001b[0m  0.0403\n",
            "     33        \u001b[36m0.6663\u001b[0m  0.0367\n",
            "     34        \u001b[36m0.6653\u001b[0m  0.0400\n",
            "     35        \u001b[36m0.6642\u001b[0m  0.0453\n",
            "     36        \u001b[36m0.6632\u001b[0m  0.0434\n",
            "     37        \u001b[36m0.6622\u001b[0m  0.0452\n",
            "     38        \u001b[36m0.6612\u001b[0m  0.0383\n",
            "     39        \u001b[36m0.6603\u001b[0m  0.0384\n",
            "     40        \u001b[36m0.6593\u001b[0m  0.0384\n",
            "     41        \u001b[36m0.6584\u001b[0m  0.0470\n",
            "     42        \u001b[36m0.6574\u001b[0m  0.0492\n",
            "     43        \u001b[36m0.6565\u001b[0m  0.0478\n",
            "     44        \u001b[36m0.6556\u001b[0m  0.0382\n",
            "     45        \u001b[36m0.6547\u001b[0m  0.0390\n",
            "     46        \u001b[36m0.6538\u001b[0m  0.0390\n",
            "     47        \u001b[36m0.6529\u001b[0m  0.0414\n",
            "     48        \u001b[36m0.6520\u001b[0m  0.0404\n",
            "     49        \u001b[36m0.6511\u001b[0m  0.0377\n",
            "     50        \u001b[36m0.6502\u001b[0m  0.0432\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7283\u001b[0m  0.0350\n",
            "      2        \u001b[36m0.7226\u001b[0m  0.0454\n",
            "      3        \u001b[36m0.7188\u001b[0m  0.0386\n",
            "      4        \u001b[36m0.7151\u001b[0m  0.0442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.7115\u001b[0m  0.0438\n",
            "      6        \u001b[36m0.7079\u001b[0m  0.0385\n",
            "      7        \u001b[36m0.7045\u001b[0m  0.0407\n",
            "      8        \u001b[36m0.7011\u001b[0m  0.0401\n",
            "      9        \u001b[36m0.6979\u001b[0m  0.0385\n",
            "     10        \u001b[36m0.6948\u001b[0m  0.0411\n",
            "     11        \u001b[36m0.6917\u001b[0m  0.0451\n",
            "     12        \u001b[36m0.6888\u001b[0m  0.0442\n",
            "     13        \u001b[36m0.6859\u001b[0m  0.0416\n",
            "     14        \u001b[36m0.6832\u001b[0m  0.0454\n",
            "     15        \u001b[36m0.6805\u001b[0m  0.0431\n",
            "     16        \u001b[36m0.6779\u001b[0m  0.0397\n",
            "     17        \u001b[36m0.6754\u001b[0m  0.0378\n",
            "     18        \u001b[36m0.6730\u001b[0m  0.0381\n",
            "     19        \u001b[36m0.6707\u001b[0m  0.0441\n",
            "     20        \u001b[36m0.6684\u001b[0m  0.0500\n",
            "     21        \u001b[36m0.6662\u001b[0m  0.0385\n",
            "     22        \u001b[36m0.6641\u001b[0m  0.0382\n",
            "     23        \u001b[36m0.6620\u001b[0m  0.0390\n",
            "     24        \u001b[36m0.6600\u001b[0m  0.0382\n",
            "     25        \u001b[36m0.6581\u001b[0m  0.0444\n",
            "     26        \u001b[36m0.6562\u001b[0m  0.0399\n",
            "     27        \u001b[36m0.6544\u001b[0m  0.0437\n",
            "     28        \u001b[36m0.6526\u001b[0m  0.0408\n",
            "     29        \u001b[36m0.6509\u001b[0m  0.0363\n",
            "     30        \u001b[36m0.6492\u001b[0m  0.0427\n",
            "     31        \u001b[36m0.6475\u001b[0m  0.0381\n",
            "     32        \u001b[36m0.6459\u001b[0m  0.0434\n",
            "     33        \u001b[36m0.6443\u001b[0m  0.0443\n",
            "     34        \u001b[36m0.6428\u001b[0m  0.0426\n",
            "     35        \u001b[36m0.6413\u001b[0m  0.0418\n",
            "     36        \u001b[36m0.6399\u001b[0m  0.0387\n",
            "     37        \u001b[36m0.6384\u001b[0m  0.0406\n",
            "     38        \u001b[36m0.6370\u001b[0m  0.0452\n",
            "     39        \u001b[36m0.6357\u001b[0m  0.0363\n",
            "     40        \u001b[36m0.6343\u001b[0m  0.0400\n",
            "     41        \u001b[36m0.6330\u001b[0m  0.0390\n",
            "     42        \u001b[36m0.6317\u001b[0m  0.0404\n",
            "     43        \u001b[36m0.6304\u001b[0m  0.0388\n",
            "     44        \u001b[36m0.6292\u001b[0m  0.0375\n",
            "     45        \u001b[36m0.6279\u001b[0m  0.0381\n",
            "     46        \u001b[36m0.6267\u001b[0m  0.0401\n",
            "     47        \u001b[36m0.6255\u001b[0m  0.0379\n",
            "     48        \u001b[36m0.6244\u001b[0m  0.0398\n",
            "     49        \u001b[36m0.6232\u001b[0m  0.0419\n",
            "     50        \u001b[36m0.6221\u001b[0m  0.0425\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0467\n",
            "      2        1.0000  0.0539\n",
            "      3        1.0000  0.0606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        1.0000  0.0603\n",
            "      5        1.0000  0.0522\n",
            "      6        1.0000  0.0471\n",
            "      7        1.0000  0.0507\n",
            "      8        1.0000  0.0677\n",
            "      9        1.0000  0.0512\n",
            "     10        1.0000  0.0516\n",
            "     11        1.0000  0.0501\n",
            "     12        1.0000  0.0523\n",
            "     13        1.0000  0.0488\n",
            "     14        1.0000  0.0500\n",
            "     15        1.0000  0.0556\n",
            "     16        1.0000  0.0505\n",
            "     17        1.0000  0.0576\n",
            "     18        1.0000  0.0473\n",
            "     19        1.0000  0.0559\n",
            "     20        1.0000  0.0480\n",
            "     21        1.0000  0.0503\n",
            "     22        1.0000  0.0534\n",
            "     23        1.0000  0.0560\n",
            "     24        1.0000  0.0558\n",
            "     25        1.0000  0.0629\n",
            "     26        1.0000  0.0539\n",
            "     27        1.0000  0.0506\n",
            "     28        1.0000  0.0470\n",
            "     29        1.0000  0.0480\n",
            "     30        1.0000  0.0583\n",
            "     31        1.0000  0.0493\n",
            "     32        1.0000  0.0471\n",
            "     33        1.0000  0.0491\n",
            "     34        1.0000  0.0462\n",
            "     35        1.0000  0.0494\n",
            "     36        \u001b[36m1.0000\u001b[0m  0.0493\n",
            "     37        \u001b[36m1.0000\u001b[0m  0.0589\n",
            "     38        \u001b[36m0.6435\u001b[0m  0.0516\n",
            "     39        \u001b[36m0.3732\u001b[0m  0.0589\n",
            "     40        \u001b[36m0.3732\u001b[0m  0.0498\n",
            "     41        \u001b[36m0.3732\u001b[0m  0.0574\n",
            "     42        0.3732  0.0533\n",
            "     43        0.3732  0.0517\n",
            "     44        0.3732  0.0557\n",
            "     45        0.3732  0.0511\n",
            "     46        0.3732  0.0494\n",
            "     47        0.3732  0.0509\n",
            "     48        0.3732  0.0552\n",
            "     49        0.3732  0.0520\n",
            "     50        0.3732  0.0546\n",
            "     51        0.3732  0.0511\n",
            "     52        0.3732  0.0460\n",
            "     53        0.3732  0.0560\n",
            "     54        0.3732  0.0484\n",
            "     55        0.3732  0.0597\n",
            "     56        0.3732  0.0508\n",
            "     57        0.3732  0.0492\n",
            "     58        0.3732  0.0520\n",
            "     59        0.3732  0.0591\n",
            "     60        0.3732  0.0514\n",
            "     61        0.3732  0.0578\n",
            "     62        0.3732  0.0496\n",
            "     63        0.3732  0.0527\n",
            "     64        0.3732  0.0576\n",
            "     65        0.3732  0.0474\n",
            "     66        0.3732  0.0508\n",
            "     67        0.3732  0.0481\n",
            "     68        0.3732  0.0483\n",
            "     69        0.3732  0.0517\n",
            "     70        0.3732  0.0445\n",
            "     71        0.3732  0.0480\n",
            "     72        0.3732  0.0482\n",
            "     73        0.3732  0.0499\n",
            "     74        0.3732  0.0518\n",
            "     75        0.3732  0.0501\n",
            "     76        0.3732  0.0522\n",
            "     77        0.3732  0.0520\n",
            "     78        0.3732  0.0526\n",
            "     79        0.3732  0.0512\n",
            "     80        0.3732  0.0496\n",
            "     81        0.3732  0.0483\n",
            "     82        0.3732  0.0526\n",
            "     83        0.3732  0.0534\n",
            "     84        0.3732  0.0464\n",
            "     85        0.3732  0.0471\n",
            "     86        0.3732  0.0523\n",
            "     87        0.3732  0.0448\n",
            "     88        0.3732  0.0503\n",
            "     89        0.3732  0.0478\n",
            "     90        0.3732  0.0507\n",
            "     91        0.3732  0.0493\n",
            "     92        0.3732  0.0659\n",
            "     93        0.3732  0.0581\n",
            "     94        0.3732  0.0551\n",
            "     95        0.3732  0.0520\n",
            "     96        0.3732  0.0584\n",
            "     97        0.3732  0.0558\n",
            "     98        0.3732  0.0588\n",
            "     99        0.3732  0.0563\n",
            "    100        0.3732  0.0526\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0562\n",
            "      2        1.0000  0.0504\n",
            "      3        1.0000  0.0505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        1.0000  0.0550\n",
            "      5        1.0000  0.0469\n",
            "      6        1.0000  0.0479\n",
            "      7        1.0000  0.0515\n",
            "      8        1.0000  0.0518\n",
            "      9        1.0000  0.0540\n",
            "     10        1.0000  0.0504\n",
            "     11        1.0000  0.0556\n",
            "     12        1.0000  0.0582\n",
            "     13        1.0000  0.0557\n",
            "     14        1.0000  0.0587\n",
            "     15        1.0000  0.0541\n",
            "     16        1.0000  0.0494\n",
            "     17        1.0000  0.0489\n",
            "     18        1.0000  0.0549\n",
            "     19        1.0000  0.0475\n",
            "     20        1.0000  0.0655\n",
            "     21        1.0000  0.0518\n",
            "     22        1.0000  0.0472\n",
            "     23        1.0000  0.0486\n",
            "     24        1.0000  0.0505\n",
            "     25        1.0000  0.0514\n",
            "     26        1.0000  0.0519\n",
            "     27        1.0000  0.0484\n",
            "     28        \u001b[36m0.8219\u001b[0m  0.0544\n",
            "     29        \u001b[36m0.3719\u001b[0m  0.0559\n",
            "     30        0.3719  0.0588\n",
            "     31        0.3719  0.0568\n",
            "     32        0.3719  0.0549\n",
            "     33        0.3719  0.0536\n",
            "     34        0.3719  0.0604\n",
            "     35        0.3719  0.0588\n",
            "     36        0.3719  0.0520\n",
            "     37        0.3719  0.0476\n",
            "     38        0.3719  0.0616\n",
            "     39        0.3719  0.0581\n",
            "     40        0.3719  0.0533\n",
            "     41        0.3719  0.0539\n",
            "     42        0.3719  0.0625\n",
            "     43        0.3719  0.0585\n",
            "     44        0.3719  0.0496\n",
            "     45        0.3719  0.0572\n",
            "     46        0.3719  0.0539\n",
            "     47        0.3719  0.0531\n",
            "     48        0.3719  0.0540\n",
            "     49        0.3719  0.0605\n",
            "     50        0.3719  0.0560\n",
            "     51        0.3719  0.0543\n",
            "     52        0.3719  0.0479\n",
            "     53        0.3719  0.0488\n",
            "     54        0.3719  0.0461\n",
            "     55        0.3719  0.0543\n",
            "     56        0.3719  0.0575\n",
            "     57        0.3719  0.0561\n",
            "     58        0.3719  0.0503\n",
            "     59        0.3719  0.0507\n",
            "     60        0.3719  0.0496\n",
            "     61        0.3719  0.0534\n",
            "     62        0.3719  0.0490\n",
            "     63        0.3719  0.0479\n",
            "     64        0.3719  0.0593\n",
            "     65        0.3719  0.0604\n",
            "     66        0.3719  0.0609\n",
            "     67        0.3719  0.0533\n",
            "     68        0.3719  0.0518\n",
            "     69        0.3719  0.0517\n",
            "     70        0.3719  0.0551\n",
            "     71        0.3719  0.0557\n",
            "     72        0.3719  0.0517\n",
            "     73        0.3719  0.0487\n",
            "     74        0.3719  0.0589\n",
            "     75        0.3719  0.0545\n",
            "     76        0.3719  0.0492\n",
            "     77        0.3719  0.0535\n",
            "     78        0.3719  0.0517\n",
            "     79        0.3719  0.0545\n",
            "     80        0.3719  0.0485\n",
            "     81        0.3719  0.0490\n",
            "     82        0.3719  0.0521\n",
            "     83        0.3719  0.0503\n",
            "     84        0.3719  0.0556\n",
            "     85        0.3719  0.0589\n",
            "     86        0.3719  0.0575\n",
            "     87        0.3719  0.0514\n",
            "     88        0.3719  0.0562\n",
            "     89        0.3719  0.0551\n",
            "     90        0.3719  0.0542\n",
            "     91        0.3719  0.0507\n",
            "     92        0.3719  0.0637\n",
            "     93        0.3719  0.0481\n",
            "     94        0.3719  0.0480\n",
            "     95        0.3719  0.0519\n",
            "     96        0.3719  0.0513\n",
            "     97        0.3719  0.0486\n",
            "     98        0.3719  0.0484\n",
            "     99        0.3719  0.0506\n",
            "    100        0.3719  0.0514\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0338\n",
            "      2        1.0000  0.0548\n",
            "      3        1.0000  0.0501\n",
            "      4        1.0000  0.0385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0440\n",
            "      6        1.0000  0.0371\n",
            "      7        1.0000  0.0361\n",
            "      8        1.0000  0.0372\n",
            "      9        1.0000  0.0380\n",
            "     10        1.0000  0.0356\n",
            "     11        1.0000  0.0418\n",
            "     12        1.0000  0.0438\n",
            "     13        1.0000  0.0507\n",
            "     14        1.0000  0.0356\n",
            "     15        1.0000  0.0420\n",
            "     16        1.0000  0.0356\n",
            "     17        1.0000  0.0406\n",
            "     18        1.0000  0.0361\n",
            "     19        1.0000  0.0408\n",
            "     20        1.0000  0.0387\n",
            "     21        1.0000  0.0369\n",
            "     22        1.0000  0.0432\n",
            "     23        1.0000  0.0480\n",
            "     24        1.0000  0.0427\n",
            "     25        1.0000  0.0437\n",
            "     26        1.0000  0.0390\n",
            "     27        1.0000  0.0402\n",
            "     28        1.0000  0.0402\n",
            "     29        1.0000  0.0368\n",
            "     30        1.0000  0.0377\n",
            "     31        1.0000  0.0445\n",
            "     32        1.0000  0.0382\n",
            "     33        1.0000  0.0374\n",
            "     34        1.0000  0.0388\n",
            "     35        1.0000  0.0387\n",
            "     36        1.0000  0.0400\n",
            "     37        1.0000  0.0471\n",
            "     38        1.0000  0.0356\n",
            "     39        1.0000  0.0418\n",
            "     40        1.0000  0.0376\n",
            "     41        1.0000  0.0408\n",
            "     42        1.0000  0.0385\n",
            "     43        1.0000  0.0378\n",
            "     44        1.0000  0.0428\n",
            "     45        1.0000  0.0401\n",
            "     46        1.0000  0.0406\n",
            "     47        1.0000  0.0381\n",
            "     48        1.0000  0.0400\n",
            "     49        1.0000  0.0458\n",
            "     50        1.0000  0.0414\n",
            "     51        1.0000  0.0386\n",
            "     52        1.0000  0.0425\n",
            "     53        1.0000  0.0389\n",
            "     54        1.0000  0.0393\n",
            "     55        1.0000  0.0353\n",
            "     56        1.0000  0.0387\n",
            "     57        1.0000  0.0365\n",
            "     58        1.0000  0.0419\n",
            "     59        1.0000  0.0398\n",
            "     60        1.0000  0.0373\n",
            "     61        1.0000  0.0522\n",
            "     62        1.0000  0.0382\n",
            "     63        1.0000  0.0367\n",
            "     64        1.0000  0.0412\n",
            "     65        1.0000  0.0382\n",
            "     66        1.0000  0.0370\n",
            "     67        1.0000  0.0471\n",
            "     68        1.0000  0.0419\n",
            "     69        1.0000  0.0399\n",
            "     70        1.0000  0.0412\n",
            "     71        1.0000  0.0423\n",
            "     72        1.0000  0.0420\n",
            "     73        1.0000  0.0404\n",
            "     74        1.0000  0.0442\n",
            "     75        1.0000  0.0370\n",
            "     76        1.0000  0.0350\n",
            "     77        1.0000  0.0435\n",
            "     78        1.0000  0.0541\n",
            "     79        1.0000  0.0398\n",
            "     80        1.0000  0.0443\n",
            "     81        1.0000  0.0429\n",
            "     82        1.0000  0.0367\n",
            "     83        1.0000  0.0407\n",
            "     84        1.0000  0.0421\n",
            "     85        1.0000  0.0391\n",
            "     86        1.0000  0.0363\n",
            "     87        1.0000  0.0385\n",
            "     88        1.0000  0.0397\n",
            "     89        1.0000  0.0370\n",
            "     90        1.0000  0.0364\n",
            "     91        1.0000  0.0395\n",
            "     92        1.0000  0.0447\n",
            "     93        1.0000  0.0491\n",
            "     94        1.0000  0.0461\n",
            "     95        1.0000  0.0433\n",
            "     96        1.0000  0.0403\n",
            "     97        1.0000  0.0397\n",
            "     98        1.0000  0.0345\n",
            "     99        1.0000  0.0374\n",
            "    100        1.0000  0.0390\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0366\n",
            "      2        1.0000  0.0366\n",
            "      3        1.0000  0.0388\n",
            "      4        1.0000  0.0435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0532\n",
            "      6        1.0000  0.0385\n",
            "      7        1.0000  0.0472\n",
            "      8        1.0000  0.0372\n",
            "      9        1.0000  0.0399\n",
            "     10        1.0000  0.0393\n",
            "     11        1.0000  0.0370\n",
            "     12        1.0000  0.0425\n",
            "     13        1.0000  0.0397\n",
            "     14        1.0000  0.0390\n",
            "     15        1.0000  0.0380\n",
            "     16        1.0000  0.0435\n",
            "     17        1.0000  0.0436\n",
            "     18        1.0000  0.0399\n",
            "     19        1.0000  0.0408\n",
            "     20        1.0000  0.0383\n",
            "     21        1.0000  0.0424\n",
            "     22        1.0000  0.0411\n",
            "     23        1.0000  0.0379\n",
            "     24        1.0000  0.0372\n",
            "     25        1.0000  0.0381\n",
            "     26        1.0000  0.0413\n",
            "     27        1.0000  0.0358\n",
            "     28        1.0000  0.0381\n",
            "     29        1.0000  0.0388\n",
            "     30        1.0000  0.0394\n",
            "     31        1.0000  0.0467\n",
            "     32        1.0000  0.0407\n",
            "     33        1.0000  0.0407\n",
            "     34        1.0000  0.0380\n",
            "     35        1.0000  0.0422\n",
            "     36        1.0000  0.0389\n",
            "     37        1.0000  0.0417\n",
            "     38        1.0000  0.0434\n",
            "     39        1.0000  0.0571\n",
            "     40        1.0000  0.0450\n",
            "     41        1.0000  0.0428\n",
            "     42        1.0000  0.0384\n",
            "     43        1.0000  0.0443\n",
            "     44        1.0000  0.0361\n",
            "     45        1.0000  0.0378\n",
            "     46        1.0000  0.0388\n",
            "     47        1.0000  0.0380\n",
            "     48        1.0000  0.0374\n",
            "     49        1.0000  0.0398\n",
            "     50        1.0000  0.0405\n",
            "     51        1.0000  0.0374\n",
            "     52        1.0000  0.0375\n",
            "     53        1.0000  0.0369\n",
            "     54        1.0000  0.0560\n",
            "     55        1.0000  0.0448\n",
            "     56        1.0000  0.0412\n",
            "     57        1.0000  0.0408\n",
            "     58        1.0000  0.0404\n",
            "     59        1.0000  0.0340\n",
            "     60        1.0000  0.0389\n",
            "     61        1.0000  0.0377\n",
            "     62        1.0000  0.0479\n",
            "     63        1.0000  0.0447\n",
            "     64        1.0000  0.0413\n",
            "     65        1.0000  0.0407\n",
            "     66        1.0000  0.0402\n",
            "     67        1.0000  0.0387\n",
            "     68        1.0000  0.0407\n",
            "     69        1.0000  0.0375\n",
            "     70        1.0000  0.0457\n",
            "     71        1.0000  0.0365\n",
            "     72        1.0000  0.0390\n",
            "     73        1.0000  0.0383\n",
            "     74        1.0000  0.0373\n",
            "     75        1.0000  0.0364\n",
            "     76        1.0000  0.0373\n",
            "     77        1.0000  0.0375\n",
            "     78        1.0000  0.0463\n",
            "     79        1.0000  0.0391\n",
            "     80        1.0000  0.0417\n",
            "     81        1.0000  0.0394\n",
            "     82        1.0000  0.0394\n",
            "     83        1.0000  0.0385\n",
            "     84        1.0000  0.0575\n",
            "     85        1.0000  0.0451\n",
            "     86        1.0000  0.0439\n",
            "     87        1.0000  0.0399\n",
            "     88        1.0000  0.0432\n",
            "     89        1.0000  0.0349\n",
            "     90        1.0000  0.0399\n",
            "     91        1.0000  0.0405\n",
            "     92        1.0000  0.0386\n",
            "     93        1.0000  0.0387\n",
            "     94        1.0000  0.0386\n",
            "     95        1.0000  0.0418\n",
            "     96        1.0000  0.0399\n",
            "     97        1.0000  0.0362\n",
            "     98        1.0000  0.0373\n",
            "     99        1.0000  0.0392\n",
            "    100        1.0000  0.0427\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0520\n",
            "      2        1.0000  0.0484\n",
            "      3        1.0000  0.0567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        1.0000  0.0671\n",
            "      5        1.0000  0.0548\n",
            "      6        1.0000  0.0557\n",
            "      7        1.0000  0.0574\n",
            "      8        1.0000  0.0560\n",
            "      9        1.0000  0.0603\n",
            "     10        1.0000  0.0574\n",
            "     11        1.0000  0.0516\n",
            "     12        1.0000  0.0529\n",
            "     13        1.0000  0.0491\n",
            "     14        1.0000  0.0539\n",
            "     15        1.0000  0.0507\n",
            "     16        1.0000  0.0496\n",
            "     17        1.0000  0.0467\n",
            "     18        1.0000  0.0523\n",
            "     19        1.0000  0.0582\n",
            "     20        1.0000  0.0538\n",
            "     21        1.0000  0.0502\n",
            "     22        1.0000  0.0521\n",
            "     23        1.0000  0.0507\n",
            "     24        1.0000  0.0619\n",
            "     25        1.0000  0.0573\n",
            "     26        1.0000  0.0587\n",
            "     27        1.0000  0.0467\n",
            "     28        1.0000  0.0492\n",
            "     29        1.0000  0.0539\n",
            "     30        1.0000  0.0548\n",
            "     31        1.0000  0.0591\n",
            "     32        1.0000  0.0560\n",
            "     33        1.0000  0.0539\n",
            "     34        1.0000  0.0519\n",
            "     35        1.0000  0.0492\n",
            "     36        1.0000  0.0478\n",
            "     37        1.0000  0.0720\n",
            "     38        1.0000  0.0577\n",
            "     39        1.0000  0.0559\n",
            "     40        1.0000  0.0553\n",
            "     41        1.0000  0.0625\n",
            "     42        \u001b[36m1.0000\u001b[0m  0.0547\n",
            "     43        \u001b[36m0.7369\u001b[0m  0.0551\n",
            "     44        \u001b[36m0.3732\u001b[0m  0.0529\n",
            "     45        \u001b[36m0.3732\u001b[0m  0.0560\n",
            "     46        0.3732  0.0524\n",
            "     47        0.3732  0.0531\n",
            "     48        0.3732  0.0503\n",
            "     49        0.3732  0.0535\n",
            "     50        0.3732  0.0530\n",
            "     51        0.3732  0.0543\n",
            "     52        0.3732  0.0535\n",
            "     53        0.3732  0.0598\n",
            "     54        0.3732  0.0518\n",
            "     55        0.3732  0.0546\n",
            "     56        0.3732  0.0488\n",
            "     57        0.3732  0.0503\n",
            "     58        0.3732  0.0596\n",
            "     59        0.3732  0.0694\n",
            "     60        0.3732  0.0678\n",
            "     61        0.3732  0.0562\n",
            "     62        0.3732  0.0531\n",
            "     63        0.3732  0.0554\n",
            "     64        0.3732  0.0562\n",
            "     65        0.3732  0.0488\n",
            "     66        0.3732  0.0599\n",
            "     67        0.3732  0.0478\n",
            "     68        0.3732  0.0477\n",
            "     69        0.3732  0.0545\n",
            "     70        0.3732  0.0501\n",
            "     71        0.3732  0.0531\n",
            "     72        0.3732  0.0504\n",
            "     73        0.3732  0.0634\n",
            "     74        0.3732  0.0509\n",
            "     75        0.3732  0.0537\n",
            "     76        0.3732  0.0629\n",
            "     77        0.3732  0.0575\n",
            "     78        0.3732  0.0511\n",
            "     79        0.3732  0.0485\n",
            "     80        0.3732  0.0484\n",
            "     81        0.3732  0.0527\n",
            "     82        0.3732  0.0549\n",
            "     83        0.3732  0.0587\n",
            "     84        0.3732  0.0549\n",
            "     85        0.3732  0.0501\n",
            "     86        0.3732  0.0520\n",
            "     87        0.3732  0.0519\n",
            "     88        0.3732  0.0491\n",
            "     89        0.3732  0.0548\n",
            "     90        0.3732  0.0641\n",
            "     91        0.3732  0.0676\n",
            "     92        0.3732  0.0674\n",
            "     93        0.3732  0.0589\n",
            "     94        0.3732  0.0594\n",
            "     95        0.3732  0.0594\n",
            "     96        0.3732  0.0482\n",
            "     97        0.3732  0.0587\n",
            "     98        0.3732  0.0492\n",
            "     99        0.3732  0.0487\n",
            "    100        0.3732  0.0560\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0442\n",
            "      2        1.0000  0.0609\n",
            "      3        1.0000  0.0534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        1.0000  0.0630\n",
            "      5        1.0000  0.0586\n",
            "      6        1.0000  0.0527\n",
            "      7        1.0000  0.0563\n",
            "      8        1.0000  0.0572\n",
            "      9        1.0000  0.0605\n",
            "     10        1.0000  0.0510\n",
            "     11        1.0000  0.0561\n",
            "     12        1.0000  0.0602\n",
            "     13        1.0000  0.0471\n",
            "     14        1.0000  0.0530\n",
            "     15        1.0000  0.0477\n",
            "     16        1.0000  0.0453\n",
            "     17        1.0000  0.0484\n",
            "     18        1.0000  0.0586\n",
            "     19        1.0000  0.0514\n",
            "     20        1.0000  0.0470\n",
            "     21        1.0000  0.0493\n",
            "     22        1.0000  0.0554\n",
            "     23        1.0000  0.0483\n",
            "     24        1.0000  0.0545\n",
            "     25        1.0000  0.0575\n",
            "     26        1.0000  0.0653\n",
            "     27        1.0000  0.0533\n",
            "     28        1.0000  0.0564\n",
            "     29        1.0000  0.0521\n",
            "     30        1.0000  0.0515\n",
            "     31        1.0000  0.0456\n",
            "     32        1.0000  0.0500\n",
            "     33        1.0000  0.0527\n",
            "     34        1.0000  0.0590\n",
            "     35        1.0000  0.0503\n",
            "     36        1.0000  0.0511\n",
            "     37        1.0000  0.0550\n",
            "     38        \u001b[36m0.7887\u001b[0m  0.0520\n",
            "     39        \u001b[36m0.3719\u001b[0m  0.0622\n",
            "     40        0.3719  0.0569\n",
            "     41        0.3719  0.0564\n",
            "     42        0.3719  0.0501\n",
            "     43        0.3719  0.0523\n",
            "     44        0.3719  0.0656\n",
            "     45        0.3719  0.0512\n",
            "     46        0.3719  0.0503\n",
            "     47        0.3719  0.0530\n",
            "     48        0.3719  0.0491\n",
            "     49        0.3719  0.0516\n",
            "     50        0.3719  0.0609\n",
            "     51        0.3719  0.0486\n",
            "     52        0.3719  0.0490\n",
            "     53        0.3719  0.0474\n",
            "     54        0.3719  0.0478\n",
            "     55        0.3719  0.0483\n",
            "     56        0.3719  0.0502\n",
            "     57        0.3719  0.0504\n",
            "     58        0.3719  0.0507\n",
            "     59        0.3719  0.0487\n",
            "     60        0.3719  0.0504\n",
            "     61        0.3719  0.0499\n",
            "     62        0.3719  0.0519\n",
            "     63        0.3719  0.0654\n",
            "     64        0.3719  0.0568\n",
            "     65        0.3719  0.0530\n",
            "     66        0.3719  0.0560\n",
            "     67        0.3719  0.0519\n",
            "     68        0.3719  0.0480\n",
            "     69        0.3719  0.0519\n",
            "     70        0.3719  0.0462\n",
            "     71        0.3719  0.0483\n",
            "     72        0.3719  0.0513\n",
            "     73        0.3719  0.0480\n",
            "     74        0.3719  0.0497\n",
            "     75        0.3719  0.0468\n",
            "     76        0.3719  0.0530\n",
            "     77        0.3719  0.0496\n",
            "     78        0.3719  0.0512\n",
            "     79        0.3719  0.0505\n",
            "     80        0.3719  0.0528\n",
            "     81        0.3719  0.0601\n",
            "     82        0.3719  0.0615\n",
            "     83        0.3719  0.0496\n",
            "     84        0.3719  0.0492\n",
            "     85        0.3719  0.0481\n",
            "     86        0.3719  0.0475\n",
            "     87        0.3719  0.0478\n",
            "     88        0.3719  0.0485\n",
            "     89        0.3719  0.0580\n",
            "     90        0.3719  0.0498\n",
            "     91        0.3719  0.0511\n",
            "     92        0.3719  0.0506\n",
            "     93        0.3719  0.0490\n",
            "     94        0.3719  0.0494\n",
            "     95        0.3719  0.0505\n",
            "     96        0.3719  0.0510\n",
            "     97        0.3719  0.0561\n",
            "     98        0.3719  0.0467\n",
            "     99        0.3719  0.0513\n",
            "    100        0.3719  0.0603\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0367\n",
            "      2        1.0000  0.0482\n",
            "      3        1.0000  0.0426\n",
            "      4        1.0000  0.0378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0460\n",
            "      6        1.0000  0.0386\n",
            "      7        1.0000  0.0415\n",
            "      8        1.0000  0.0372\n",
            "      9        1.0000  0.0397\n",
            "     10        1.0000  0.0400\n",
            "     11        1.0000  0.0369\n",
            "     12        1.0000  0.0373\n",
            "     13        1.0000  0.0363\n",
            "     14        1.0000  0.0472\n",
            "     15        1.0000  0.0387\n",
            "     16        1.0000  0.0435\n",
            "     17        1.0000  0.0369\n",
            "     18        1.0000  0.0389\n",
            "     19        1.0000  0.0474\n",
            "     20        1.0000  0.0456\n",
            "     21        1.0000  0.0431\n",
            "     22        1.0000  0.0424\n",
            "     23        1.0000  0.0415\n",
            "     24        1.0000  0.0500\n",
            "     25        1.0000  0.0408\n",
            "     26        1.0000  0.0405\n",
            "     27        1.0000  0.0442\n",
            "     28        1.0000  0.0356\n",
            "     29        1.0000  0.0394\n",
            "     30        1.0000  0.0371\n",
            "     31        1.0000  0.0376\n",
            "     32        1.0000  0.0372\n",
            "     33        1.0000  0.0425\n",
            "     34        1.0000  0.0399\n",
            "     35        1.0000  0.0399\n",
            "     36        1.0000  0.0370\n",
            "     37        1.0000  0.0366\n",
            "     38        1.0000  0.0475\n",
            "     39        1.0000  0.0409\n",
            "     40        1.0000  0.0398\n",
            "     41        1.0000  0.0411\n",
            "     42        1.0000  0.0397\n",
            "     43        1.0000  0.0381\n",
            "     44        1.0000  0.0416\n",
            "     45        1.0000  0.0433\n",
            "     46        1.0000  0.0439\n",
            "     47        1.0000  0.0391\n",
            "     48        1.0000  0.0481\n",
            "     49        1.0000  0.0385\n",
            "     50        1.0000  0.0354\n",
            "     51        1.0000  0.0425\n",
            "     52        1.0000  0.0417\n",
            "     53        1.0000  0.0407\n",
            "     54        1.0000  0.0400\n",
            "     55        1.0000  0.0525\n",
            "     56        1.0000  0.0442\n",
            "     57        1.0000  0.0436\n",
            "     58        1.0000  0.0433\n",
            "     59        1.0000  0.0392\n",
            "     60        1.0000  0.0494\n",
            "     61        1.0000  0.0405\n",
            "     62        1.0000  0.0402\n",
            "     63        1.0000  0.0460\n",
            "     64        1.0000  0.0519\n",
            "     65        1.0000  0.0377\n",
            "     66        1.0000  0.0390\n",
            "     67        1.0000  0.0397\n",
            "     68        1.0000  0.0408\n",
            "     69        1.0000  0.0400\n",
            "     70        1.0000  0.0448\n",
            "     71        1.0000  0.0435\n",
            "     72        1.0000  0.0361\n",
            "     73        1.0000  0.0368\n",
            "     74        1.0000  0.0396\n",
            "     75        1.0000  0.0468\n",
            "     76        1.0000  0.0388\n",
            "     77        1.0000  0.0392\n",
            "     78        1.0000  0.0403\n",
            "     79        1.0000  0.0463\n",
            "     80        1.0000  0.0393\n",
            "     81        1.0000  0.0421\n",
            "     82        1.0000  0.0413\n",
            "     83        1.0000  0.0398\n",
            "     84        1.0000  0.0384\n",
            "     85        1.0000  0.0441\n",
            "     86        1.0000  0.0473\n",
            "     87        1.0000  0.0490\n",
            "     88        1.0000  0.0479\n",
            "     89        1.0000  0.0424\n",
            "     90        1.0000  0.0388\n",
            "     91        1.0000  0.0381\n",
            "     92        1.0000  0.0378\n",
            "     93        1.0000  0.0387\n",
            "     94        1.0000  0.0555\n",
            "     95        1.0000  0.0442\n",
            "     96        1.0000  0.0385\n",
            "     97        1.0000  0.0426\n",
            "     98        1.0000  0.0415\n",
            "     99        1.0000  0.0425\n",
            "    100        1.0000  0.0464\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0329\n",
            "      2        1.0000  0.0511\n",
            "      3        1.0000  0.0513\n",
            "      4        1.0000  0.0432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        1.0000  0.0477\n",
            "      6        1.0000  0.0427\n",
            "      7        1.0000  0.0398\n",
            "      8        1.0000  0.0423\n",
            "      9        1.0000  0.0403\n",
            "     10        1.0000  0.0432\n",
            "     11        1.0000  0.0454\n",
            "     12        1.0000  0.0453\n",
            "     13        1.0000  0.0418\n",
            "     14        1.0000  0.0400\n",
            "     15        1.0000  0.0364\n",
            "     16        1.0000  0.0496\n",
            "     17        1.0000  0.0472\n",
            "     18        1.0000  0.0436\n",
            "     19        1.0000  0.0421\n",
            "     20        1.0000  0.0443\n",
            "     21        1.0000  0.0388\n",
            "     22        1.0000  0.0406\n",
            "     23        1.0000  0.0397\n",
            "     24        1.0000  0.0369\n",
            "     25        1.0000  0.0396\n",
            "     26        1.0000  0.0409\n",
            "     27        1.0000  0.0425\n",
            "     28        1.0000  0.0386\n",
            "     29        1.0000  0.0429\n",
            "     30        1.0000  0.0406\n",
            "     31        1.0000  0.0393\n",
            "     32        1.0000  0.0433\n",
            "     33        1.0000  0.0461\n",
            "     34        1.0000  0.0408\n",
            "     35        1.0000  0.0447\n",
            "     36        1.0000  0.0389\n",
            "     37        1.0000  0.0386\n",
            "     38        1.0000  0.0393\n",
            "     39        1.0000  0.0472\n",
            "     40        1.0000  0.0369\n",
            "     41        1.0000  0.0430\n",
            "     42        1.0000  0.0399\n",
            "     43        1.0000  0.0395\n",
            "     44        1.0000  0.0405\n",
            "     45        1.0000  0.0448\n",
            "     46        1.0000  0.0370\n",
            "     47        1.0000  0.0388\n",
            "     48        1.0000  0.0438\n",
            "     49        1.0000  0.0425\n",
            "     50        1.0000  0.0414\n",
            "     51        1.0000  0.0443\n",
            "     52        1.0000  0.0400\n",
            "     53        1.0000  0.0418\n",
            "     54        1.0000  0.0418\n",
            "     55        1.0000  0.0429\n",
            "     56        1.0000  0.0455\n",
            "     57        1.0000  0.0408\n",
            "     58        1.0000  0.0416\n",
            "     59        1.0000  0.0403\n",
            "     60        1.0000  0.0393\n",
            "     61        1.0000  0.0483\n",
            "     62        1.0000  0.0517\n",
            "     63        1.0000  0.0402\n",
            "     64        1.0000  0.0397\n",
            "     65        1.0000  0.0376\n",
            "     66        1.0000  0.0381\n",
            "     67        1.0000  0.0409\n",
            "     68        1.0000  0.0413\n",
            "     69        1.0000  0.0393\n",
            "     70        1.0000  0.0390\n",
            "     71        1.0000  0.0441\n",
            "     72        1.0000  0.0394\n",
            "     73        1.0000  0.0420\n",
            "     74        1.0000  0.0385\n",
            "     75        1.0000  0.0472\n",
            "     76        1.0000  0.0334\n",
            "     77        1.0000  0.0451\n",
            "     78        1.0000  0.0501\n",
            "     79        1.0000  0.0446\n",
            "     80        1.0000  0.0412\n",
            "     81        1.0000  0.0403\n",
            "     82        1.0000  0.0389\n",
            "     83        1.0000  0.0380\n",
            "     84        1.0000  0.0385\n",
            "     85        1.0000  0.0517\n",
            "     86        1.0000  0.0440\n",
            "     87        1.0000  0.0398\n",
            "     88        1.0000  0.0420\n",
            "     89        1.0000  0.0464\n",
            "     90        1.0000  0.0410\n",
            "     91        1.0000  0.0386\n",
            "     92        1.0000  0.0389\n",
            "     93        1.0000  0.0423\n",
            "     94        1.0000  0.0436\n",
            "     95        1.0000  0.0413\n",
            "     96        1.0000  0.0424\n",
            "     97        1.0000  0.0518\n",
            "     98        1.0000  0.0422\n",
            "     99        1.0000  0.0392\n",
            "    100        1.0000  0.0395\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4437\u001b[0m  0.0462\n",
            "      2        \u001b[36m0.4401\u001b[0m  0.0509\n",
            "      3        0.4437  0.0488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        0.4472  0.0526\n",
            "      5        0.4507  0.0504\n",
            "      6        0.4613  0.0657\n",
            "      7        0.4613  0.0472\n",
            "      8        0.4824  0.0485\n",
            "      9        0.4542  0.0500\n",
            "     10        0.4577  0.0501\n",
            "     11        0.4472  0.0542\n",
            "     12        0.4472  0.0499\n",
            "     13        0.4472  0.0483\n",
            "     14        0.4472  0.0538\n",
            "     15        0.4507  0.0541\n",
            "     16        0.4507  0.0620\n",
            "     17        0.4507  0.0640\n",
            "     18        0.4507  0.0552\n",
            "     19        0.4507  0.0494\n",
            "     20        0.4507  0.0486\n",
            "     21        0.4507  0.0555\n",
            "     22        0.4507  0.0531\n",
            "     23        0.4507  0.0500\n",
            "     24        0.4507  0.0444\n",
            "     25        0.4507  0.0634\n",
            "     26        0.4507  0.0534\n",
            "     27        0.4507  0.0495\n",
            "     28        0.4507  0.0503\n",
            "     29        0.4473  0.0541\n",
            "     30        0.4401  0.0501\n",
            "     31        0.4401  0.0516\n",
            "     32        0.4401  0.0514\n",
            "     33        0.4401  0.0575\n",
            "     34        0.4401  0.0527\n",
            "     35        0.4401  0.0551\n",
            "     36        0.4401  0.0614\n",
            "     37        0.4401  0.0644\n",
            "     38        0.4401  0.0554\n",
            "     39        0.4401  0.0465\n",
            "     40        0.4401  0.0477\n",
            "     41        0.4401  0.0481\n",
            "     42        0.4401  0.0461\n",
            "     43        0.4401  0.0636\n",
            "     44        0.4401  0.0561\n",
            "     45        0.4401  0.0475\n",
            "     46        0.4401  0.0533\n",
            "     47        0.4401  0.0491\n",
            "     48        0.4401  0.0502\n",
            "     49        0.4401  0.0524\n",
            "     50        0.4401  0.0570\n",
            "     51        0.4401  0.0537\n",
            "     52        0.4401  0.0617\n",
            "     53        0.4401  0.0560\n",
            "     54        0.4401  0.0529\n",
            "     55        0.4401  0.0507\n",
            "     56        \u001b[36m0.4371\u001b[0m  0.0535\n",
            "     57        \u001b[36m0.4331\u001b[0m  0.0504\n",
            "     58        0.4331  0.0471\n",
            "     59        0.4331  0.0489\n",
            "     60        0.4331  0.0588\n",
            "     61        0.4331  0.0558\n",
            "     62        0.4331  0.0483\n",
            "     63        0.4331  0.0535\n",
            "     64        0.4331  0.0501\n",
            "     65        0.4331  0.0587\n",
            "     66        0.4331  0.0555\n",
            "     67        0.4331  0.0560\n",
            "     68        0.4331  0.0520\n",
            "     69        0.4331  0.0518\n",
            "     70        0.4331  0.0506\n",
            "     71        0.4331  0.0674\n",
            "     72        0.4331  0.0550\n",
            "     73        0.4331  0.0536\n",
            "     74        0.4331  0.0535\n",
            "     75        0.4331  0.0502\n",
            "     76        0.4331  0.0508\n",
            "     77        0.4331  0.0501\n",
            "     78        0.4331  0.0499\n",
            "     79        0.4331  0.0475\n",
            "     80        0.4331  0.0558\n",
            "     81        0.4331  0.0519\n",
            "     82        0.4331  0.0439\n",
            "     83        0.4331  0.0486\n",
            "     84        0.4331  0.0506\n",
            "     85        0.4331  0.0510\n",
            "     86        0.4331  0.0489\n",
            "     87        0.4331  0.0598\n",
            "     88        0.4331  0.0572\n",
            "     89        0.4331  0.0637\n",
            "     90        0.4331  0.0523\n",
            "     91        0.4331  0.0594\n",
            "     92        0.4331  0.0513\n",
            "     93        0.4331  0.0508\n",
            "     94        0.4331  0.0514\n",
            "     95        0.4331  0.0541\n",
            "     96        0.4331  0.0514\n",
            "     97        0.4331  0.0508\n",
            "     98        0.4331  0.0626\n",
            "     99        0.4331  0.0477\n",
            "    100        0.4331  0.0507\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4561\u001b[0m  0.0433\n",
            "      2        \u001b[36m0.4491\u001b[0m  0.0487\n",
            "      3        0.4491  0.0486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        0.4491  0.0548\n",
            "      5        \u001b[36m0.4485\u001b[0m  0.0565\n",
            "      6        \u001b[36m0.4421\u001b[0m  0.0542\n",
            "      7        \u001b[36m0.4386\u001b[0m  0.0538\n",
            "      8        0.4386  0.0575\n",
            "      9        0.4418  0.0561\n",
            "     10        0.4386  0.0491\n",
            "     11        0.4386  0.0479\n",
            "     12        0.4386  0.0498\n",
            "     13        0.4386  0.0469\n",
            "     14        0.4386  0.0551\n",
            "     15        0.4386  0.0461\n",
            "     16        0.4386  0.0577\n",
            "     17        0.4386  0.0474\n",
            "     18        0.4386  0.0492\n",
            "     19        0.4386  0.0499\n",
            "     20        0.4386  0.0529\n",
            "     21        0.4386  0.0466\n",
            "     22        0.4386  0.0501\n",
            "     23        0.4386  0.0513\n",
            "     24        0.4386  0.0579\n",
            "     25        0.4386  0.0633\n",
            "     26        0.4386  0.0571\n",
            "     27        0.4386  0.0545\n",
            "     28        0.4386  0.0496\n",
            "     29        0.4386  0.0517\n",
            "     30        0.4386  0.0614\n",
            "     31        0.4386  0.0504\n",
            "     32        0.4386  0.0565\n",
            "     33        0.4386  0.0502\n",
            "     34        0.4386  0.0555\n",
            "     35        0.4386  0.0528\n",
            "     36        0.4386  0.0471\n",
            "     37        0.4386  0.0575\n",
            "     38        0.4387  0.0501\n",
            "     39        0.4386  0.0599\n",
            "     40        0.4386  0.0518\n",
            "     41        0.4386  0.0533\n",
            "     42        0.4386  0.0626\n",
            "     43        0.4386  0.0572\n",
            "     44        0.4386  0.0557\n",
            "     45        0.4386  0.0494\n",
            "     46        0.4386  0.0662\n",
            "     47        0.4386  0.0578\n",
            "     48        0.4386  0.0506\n",
            "     49        0.4386  0.0537\n",
            "     50        0.4386  0.0497\n",
            "     51        0.4386  0.0487\n",
            "     52        0.4386  0.0533\n",
            "     53        0.4386  0.0609\n",
            "     54        0.4386  0.0520\n",
            "     55        0.4386  0.0491\n",
            "     56        0.4386  0.0490\n",
            "     57        0.4386  0.0491\n",
            "     58        0.4386  0.0544\n",
            "     59        0.4386  0.0603\n",
            "     60        0.4386  0.0591\n",
            "     61        0.4386  0.0598\n",
            "     62        0.4386  0.0611\n",
            "     63        0.4386  0.0583\n",
            "     64        0.4386  0.0553\n",
            "     65        0.4386  0.0481\n",
            "     66        0.4386  0.0519\n",
            "     67        0.4386  0.0527\n",
            "     68        0.4386  0.0517\n",
            "     69        \u001b[36m0.4386\u001b[0m  0.0530\n",
            "     70        \u001b[36m0.4349\u001b[0m  0.0481\n",
            "     71        \u001b[36m0.4281\u001b[0m  0.0548\n",
            "     72        0.4281  0.0519\n",
            "     73        0.4281  0.0492\n",
            "     74        0.4281  0.0508\n",
            "     75        0.4281  0.0632\n",
            "     76        0.4281  0.0591\n",
            "     77        0.4281  0.0544\n",
            "     78        0.4281  0.0529\n",
            "     79        0.4281  0.0534\n",
            "     80        0.4281  0.0550\n",
            "     81        0.4281  0.0559\n",
            "     82        0.4281  0.0565\n",
            "     83        0.4281  0.0572\n",
            "     84        0.4281  0.0504\n",
            "     85        0.4281  0.0469\n",
            "     86        0.4281  0.0457\n",
            "     87        0.4281  0.0510\n",
            "     88        0.4281  0.0495\n",
            "     89        0.4281  0.0579\n",
            "     90        0.4281  0.0558\n",
            "     91        0.4281  0.0469\n",
            "     92        0.4281  0.0541\n",
            "     93        0.4281  0.0502\n",
            "     94        0.4281  0.0514\n",
            "     95        0.4281  0.0536\n",
            "     96        0.4281  0.0520\n",
            "     97        0.4281  0.0487\n",
            "     98        0.4281  0.0505\n",
            "     99        0.4281  0.0519\n",
            "    100        0.4281  0.0516\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4296\u001b[0m  0.0317\n",
            "      2        0.4296  0.0424\n",
            "      3        0.4296  0.0364\n",
            "      4        0.4296  0.0363\n",
            "      5        0.4296  0.0382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        0.4296  0.0469\n",
            "      7        0.4296  0.0374\n",
            "      8        0.4296  0.0372\n",
            "      9        0.4296  0.0451\n",
            "     10        0.4296  0.0388\n",
            "     11        0.4296  0.0404\n",
            "     12        0.4296  0.0422\n",
            "     13        0.4296  0.0389\n",
            "     14        0.4296  0.0417\n",
            "     15        0.4296  0.0448\n",
            "     16        0.4296  0.0359\n",
            "     17        0.4296  0.0402\n",
            "     18        0.4296  0.0410\n",
            "     19        0.4296  0.0401\n",
            "     20        0.4296  0.0390\n",
            "     21        0.4296  0.0369\n",
            "     22        0.4296  0.0359\n",
            "     23        0.4296  0.0382\n",
            "     24        0.4296  0.0360\n",
            "     25        0.4296  0.0369\n",
            "     26        0.4296  0.0384\n",
            "     27        0.4296  0.0446\n",
            "     28        0.4296  0.0398\n",
            "     29        0.4296  0.0407\n",
            "     30        0.4296  0.0372\n",
            "     31        0.4296  0.0382\n",
            "     32        0.4296  0.0383\n",
            "     33        0.4296  0.0433\n",
            "     34        0.4296  0.0494\n",
            "     35        0.4296  0.0384\n",
            "     36        0.4296  0.0389\n",
            "     37        0.4296  0.0431\n",
            "     38        0.4296  0.0444\n",
            "     39        0.4296  0.0488\n",
            "     40        0.4296  0.0416\n",
            "     41        0.4296  0.0450\n",
            "     42        0.4296  0.0469\n",
            "     43        0.4296  0.0388\n",
            "     44        0.4296  0.0364\n",
            "     45        0.4296  0.0370\n",
            "     46        0.4296  0.0395\n",
            "     47        0.4296  0.0377\n",
            "     48        0.4296  0.0407\n",
            "     49        0.4296  0.0464\n",
            "     50        0.4296  0.0371\n",
            "     51        0.4296  0.0395\n",
            "     52        0.4296  0.0369\n",
            "     53        0.4296  0.0382\n",
            "     54        0.4296  0.0397\n",
            "     55        0.4296  0.0385\n",
            "     56        0.4296  0.0402\n",
            "     57        0.4296  0.0472\n",
            "     58        0.4296  0.0432\n",
            "     59        0.4296  0.0366\n",
            "     60        0.4296  0.0398\n",
            "     61        0.4296  0.0429\n",
            "     62        0.4296  0.0421\n",
            "     63        0.4296  0.0416\n",
            "     64        0.4296  0.0449\n",
            "     65        0.4296  0.0432\n",
            "     66        0.4296  0.0411\n",
            "     67        0.4296  0.0402\n",
            "     68        0.4296  0.0432\n",
            "     69        0.4296  0.0421\n",
            "     70        0.4296  0.0392\n",
            "     71        0.4296  0.0410\n",
            "     72        0.4296  0.0403\n",
            "     73        0.4296  0.0380\n",
            "     74        0.4296  0.0381\n",
            "     75        0.4296  0.0403\n",
            "     76        0.4296  0.0371\n",
            "     77        0.4296  0.0401\n",
            "     78        0.4296  0.0384\n",
            "     79        0.4296  0.0411\n",
            "     80        0.4296  0.0403\n",
            "     81        0.4296  0.0513\n",
            "     82        0.4296  0.0460\n",
            "     83        0.4296  0.0420\n",
            "     84        0.4296  0.0412\n",
            "     85        0.4296  0.0417\n",
            "     86        0.4296  0.0409\n",
            "     87        0.4296  0.0407\n",
            "     88        0.4296  0.0409\n",
            "     89        0.4296  0.0413\n",
            "     90        0.4296  0.0389\n",
            "     91        0.4296  0.0428\n",
            "     92        0.4296  0.0385\n",
            "     93        0.4296  0.0380\n",
            "     94        0.4296  0.0411\n",
            "     95        0.4296  0.0365\n",
            "     96        0.4296  0.0458\n",
            "     97        0.4296  0.0375\n",
            "     98        0.4296  0.0415\n",
            "     99        0.4296  0.0412\n",
            "    100        0.4296  0.0395\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.3965\u001b[0m  0.0321\n",
            "      2        0.3965  0.0405\n",
            "      3        0.3965  0.0430\n",
            "      4        0.3965  0.0558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        0.3965  0.0536\n",
            "      6        0.3965  0.0445\n",
            "      7        0.3965  0.0413\n",
            "      8        0.3965  0.0423\n",
            "      9        0.3965  0.0432\n",
            "     10        0.3965  0.0399\n",
            "     11        0.3965  0.0386\n",
            "     12        0.3965  0.0370\n",
            "     13        0.3965  0.0366\n",
            "     14        0.3965  0.0403\n",
            "     15        0.3965  0.0358\n",
            "     16        0.3965  0.0406\n",
            "     17        0.3965  0.0431\n",
            "     18        0.3965  0.0380\n",
            "     19        0.3965  0.0374\n",
            "     20        0.3965  0.0355\n",
            "     21        0.3965  0.0425\n",
            "     22        0.3965  0.0347\n",
            "     23        0.3965  0.0394\n",
            "     24        0.3965  0.0373\n",
            "     25        0.3965  0.0360\n",
            "     26        0.3965  0.0384\n",
            "     27        0.3965  0.0457\n",
            "     28        0.3965  0.0462\n",
            "     29        0.3965  0.0398\n",
            "     30        0.3965  0.0429\n",
            "     31        0.3965  0.0397\n",
            "     32        0.3965  0.0404\n",
            "     33        0.3965  0.0389\n",
            "     34        0.3965  0.0402\n",
            "     35        0.3965  0.0394\n",
            "     36        0.3965  0.0349\n",
            "     37        0.3965  0.0374\n",
            "     38        0.3965  0.0466\n",
            "     39        0.3965  0.0390\n",
            "     40        0.3965  0.0367\n",
            "     41        0.3965  0.0374\n",
            "     42        0.3965  0.0405\n",
            "     43        0.3965  0.0382\n",
            "     44        0.3965  0.0395\n",
            "     45        0.3965  0.0392\n",
            "     46        0.3965  0.0389\n",
            "     47        0.3965  0.0376\n",
            "     48        0.3965  0.0417\n",
            "     49        0.3965  0.0390\n",
            "     50        0.3965  0.0434\n",
            "     51        0.3965  0.0337\n",
            "     52        0.3965  0.0492\n",
            "     53        0.3965  0.0413\n",
            "     54        0.3965  0.0388\n",
            "     55        0.3965  0.0407\n",
            "     56        0.3965  0.0405\n",
            "     57        0.3965  0.0409\n",
            "     58        0.3965  0.0377\n",
            "     59        0.3965  0.0377\n",
            "     60        0.3965  0.0358\n",
            "     61        0.3965  0.0398\n",
            "     62        0.3965  0.0376\n",
            "     63        0.3965  0.0376\n",
            "     64        0.3965  0.0381\n",
            "     65        0.3965  0.0411\n",
            "     66        0.3965  0.0441\n",
            "     67        0.3965  0.0395\n",
            "     68        0.3965  0.0411\n",
            "     69        0.3965  0.0390\n",
            "     70        0.3965  0.0365\n",
            "     71        0.3965  0.0402\n",
            "     72        0.3965  0.0454\n",
            "     73        0.3965  0.0396\n",
            "     74        0.3965  0.0371\n",
            "     75        0.3965  0.0385\n",
            "     76        0.3965  0.0506\n",
            "     77        0.3965  0.0391\n",
            "     78        0.3965  0.0379\n",
            "     79        0.3965  0.0388\n",
            "     80        0.3965  0.0405\n",
            "     81        0.3965  0.0430\n",
            "     82        0.3965  0.0365\n",
            "     83        0.3965  0.0403\n",
            "     84        0.3965  0.0424\n",
            "     85        0.3965  0.0349\n",
            "     86        0.3965  0.0365\n",
            "     87        0.3965  0.0388\n",
            "     88        0.3965  0.0426\n",
            "     89        0.3965  0.0396\n",
            "     90        0.3965  0.0431\n",
            "     91        0.3965  0.0398\n",
            "     92        0.3965  0.0394\n",
            "     93        0.3965  0.0399\n",
            "     94        0.3965  0.0367\n",
            "     95        0.3965  0.0389\n",
            "     96        0.3965  0.0380\n",
            "     97        0.3965  0.0376\n",
            "     98        0.3965  0.0426\n",
            "     99        0.3965  0.0463\n",
            "    100        0.3965  0.0511\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9859\u001b[0m  0.0471\n",
            "      2        0.9859  0.0592\n",
            "      3        0.9859  0.0543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        0.9859  0.0555\n",
            "      5        0.9859  0.0530\n",
            "      6        0.9859  0.0566\n",
            "      7        0.9859  0.0485\n",
            "      8        0.9859  0.0496\n",
            "      9        0.9859  0.0495\n",
            "     10        0.9859  0.0459\n",
            "     11        0.9859  0.0493\n",
            "     12        0.9859  0.0499\n",
            "     13        0.9859  0.0527\n",
            "     14        0.9859  0.0593\n",
            "     15        0.9859  0.0473\n",
            "     16        0.9859  0.0500\n",
            "     17        0.9859  0.0649\n",
            "     18        0.9859  0.0674\n",
            "     19        \u001b[36m0.9824\u001b[0m  0.0600\n",
            "     20        0.9824  0.0568\n",
            "     21        0.9824  0.0606\n",
            "     22        0.9824  0.0569\n",
            "     23        0.9824  0.0509\n",
            "     24        0.9824  0.0504\n",
            "     25        \u001b[36m0.9819\u001b[0m  0.0476\n",
            "     26        \u001b[36m0.9648\u001b[0m  0.0529\n",
            "     27        \u001b[36m0.9366\u001b[0m  0.0492\n",
            "     28        \u001b[36m0.7187\u001b[0m  0.0591\n",
            "     29        \u001b[36m0.4824\u001b[0m  0.0539\n",
            "     30        \u001b[36m0.4613\u001b[0m  0.0542\n",
            "     31        \u001b[36m0.4577\u001b[0m  0.0525\n",
            "     32        \u001b[36m0.4472\u001b[0m  0.0505\n",
            "     33        \u001b[36m0.4331\u001b[0m  0.0534\n",
            "     34        \u001b[36m0.4261\u001b[0m  0.0570\n",
            "     35        0.4261  0.0517\n",
            "     36        0.4261  0.0666\n",
            "     37        0.4261  0.0613\n",
            "     38        0.4261  0.0582\n",
            "     39        \u001b[36m0.4257\u001b[0m  0.0507\n",
            "     40        \u001b[36m0.3908\u001b[0m  0.0499\n",
            "     41        0.3908  0.0484\n",
            "     42        0.3908  0.0550\n",
            "     43        0.3908  0.0594\n",
            "     44        0.3908  0.0504\n",
            "     45        0.3908  0.0527\n",
            "     46        0.3908  0.0489\n",
            "     47        0.3908  0.0508\n",
            "     48        0.3908  0.0501\n",
            "     49        0.3908  0.0517\n",
            "     50        0.3908  0.0515\n",
            "     51        0.3908  0.0592\n",
            "     52        0.3908  0.0626\n",
            "     53        0.3908  0.0556\n",
            "     54        \u001b[36m0.3908\u001b[0m  0.0652\n",
            "     55        \u001b[36m0.3908\u001b[0m  0.0610\n",
            "     56        \u001b[36m0.3873\u001b[0m  0.0522\n",
            "     57        0.3873  0.0493\n",
            "     58        0.3873  0.0532\n",
            "     59        0.3873  0.0582\n",
            "     60        0.3873  0.0505\n",
            "     61        0.3873  0.0505\n",
            "     62        0.3873  0.0499\n",
            "     63        0.3873  0.0528\n",
            "     64        0.3873  0.0559\n",
            "     65        0.3873  0.0540\n",
            "     66        0.3873  0.0609\n",
            "     67        0.3873  0.0510\n",
            "     68        0.3873  0.0552\n",
            "     69        0.3873  0.0544\n",
            "     70        0.3873  0.0540\n",
            "     71        0.3873  0.0576\n",
            "     72        0.3873  0.0689\n",
            "     73        0.3873  0.0493\n",
            "     74        0.3873  0.0497\n",
            "     75        0.3873  0.0590\n",
            "     76        0.3873  0.0506\n",
            "     77        0.3873  0.0508\n",
            "     78        0.3873  0.0546\n",
            "     79        0.3873  0.0546\n",
            "     80        0.3873  0.1724\n",
            "     81        0.3873  0.0528\n",
            "     82        0.3873  0.0559\n",
            "     83        0.3873  0.0526\n",
            "     84        0.3873  0.0499\n",
            "     85        0.3873  0.0594\n",
            "     86        0.3873  0.0596\n",
            "     87        0.3873  0.0772\n",
            "     88        0.3873  0.0544\n",
            "     89        0.3873  0.0512\n",
            "     90        0.3873  0.0496\n",
            "     91        0.3873  0.0477\n",
            "     92        0.3873  0.0523\n",
            "     93        0.3873  0.0503\n",
            "     94        0.3873  0.0501\n",
            "     95        0.3873  0.0578\n",
            "     96        0.3873  0.0522\n",
            "     97        0.3873  0.0523\n",
            "     98        0.3873  0.0546\n",
            "     99        0.3873  0.0599\n",
            "    100        0.3873  0.0535\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6667\u001b[0m  0.0466\n",
            "      2        0.6772  0.0545\n",
            "      3        0.6737  0.0620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        0.6772  0.0685\n",
            "      5        0.6842  0.0634\n",
            "      6        0.6912  0.0547\n",
            "      7        0.6877  0.0605\n",
            "      8        0.6877  0.0530\n",
            "      9        0.6982  0.0506\n",
            "     10        0.7053  0.0549\n",
            "     11        0.7053  0.0591\n",
            "     12        0.7088  0.0674\n",
            "     13        0.7123  0.0558\n",
            "     14        0.7029  0.0525\n",
            "     15        0.6912  0.0540\n",
            "     16        0.6912  0.0500\n",
            "     17        0.6912  0.0498\n",
            "     18        0.6912  0.0567\n",
            "     19        0.6700  0.0541\n",
            "     20        \u001b[36m0.6491\u001b[0m  0.0516\n",
            "     21        \u001b[36m0.6386\u001b[0m  0.0602\n",
            "     22        0.6386  0.0619\n",
            "     23        0.6386  0.0487\n",
            "     24        0.6386  0.0508\n",
            "     25        0.6386  0.0473\n",
            "     26        0.6386  0.0643\n",
            "     27        0.6386  0.0455\n",
            "     28        0.6386  0.0500\n",
            "     29        0.6386  0.0484\n",
            "     30        \u001b[36m0.6386\u001b[0m  0.0449\n",
            "     31        \u001b[36m0.6351\u001b[0m  0.0583\n",
            "     32        0.6351  0.0493\n",
            "     33        0.6351  0.0452\n",
            "     34        0.6351  0.0498\n",
            "     35        0.6351  0.0477\n",
            "     36        0.6351  0.0515\n",
            "     37        0.6351  0.0538\n",
            "     38        0.6351  0.0580\n",
            "     39        0.6351  0.0595\n",
            "     40        0.6351  0.0564\n",
            "     41        0.6351  0.0601\n",
            "     42        0.6351  0.0565\n",
            "     43        \u001b[36m0.6316\u001b[0m  0.0526\n",
            "     44        0.6316  0.0505\n",
            "     45        0.6316  0.0506\n",
            "     46        \u001b[36m0.6281\u001b[0m  0.0487\n",
            "     47        \u001b[36m0.6281\u001b[0m  0.0493\n",
            "     48        0.6281  0.0495\n",
            "     49        0.6281  0.0486\n",
            "     50        0.6281  0.0447\n",
            "     51        0.6281  0.0546\n",
            "     52        0.6281  0.0534\n",
            "     53        0.6281  0.0505\n",
            "     54        \u001b[36m0.6210\u001b[0m  0.0534\n",
            "     55        \u001b[36m0.6175\u001b[0m  0.0528\n",
            "     56        0.6175  0.0586\n",
            "     57        0.6175  0.0533\n",
            "     58        0.6175  0.0535\n",
            "     59        0.6175  0.0605\n",
            "     60        0.6175  0.0468\n",
            "     61        0.6175  0.0506\n",
            "     62        0.6175  0.0555\n",
            "     63        0.6175  0.0500\n",
            "     64        0.6175  0.0507\n",
            "     65        0.6175  0.0507\n",
            "     66        0.6175  0.0505\n",
            "     67        0.6175  0.0511\n",
            "     68        0.6175  0.0498\n",
            "     69        0.6175  0.0520\n",
            "     70        0.6175  0.0538\n",
            "     71        0.6175  0.0524\n",
            "     72        0.6175  0.0526\n",
            "     73        0.6175  0.0540\n",
            "     74        0.6175  0.0519\n",
            "     75        0.6175  0.0634\n",
            "     76        0.6175  0.0533\n",
            "     77        0.6175  0.0531\n",
            "     78        0.6175  0.0588\n",
            "     79        0.6175  0.0486\n",
            "     80        0.6175  0.0565\n",
            "     81        0.6175  0.0511\n",
            "     82        0.6175  0.0562\n",
            "     83        0.6175  0.0495\n",
            "     84        0.6175  0.0489\n",
            "     85        0.6175  0.0513\n",
            "     86        0.6175  0.0494\n",
            "     87        0.6175  0.0516\n",
            "     88        0.6175  0.0574\n",
            "     89        0.6175  0.0518\n",
            "     90        0.6175  0.0504\n",
            "     91        0.6175  0.0488\n",
            "     92        0.6175  0.0557\n",
            "     93        0.6175  0.0492\n",
            "     94        0.6175  0.0537\n",
            "     95        0.6175  0.0561\n",
            "     96        0.6175  0.0662\n",
            "     97        0.6246  0.0583\n",
            "     98        0.6175  0.0463\n",
            "     99        0.6175  0.0482\n",
            "    100        0.6175  0.0534\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4472\u001b[0m  0.0332\n",
            "      2        0.4472  0.0396\n",
            "      3        0.4472  0.0396\n",
            "      4        0.4472  0.0451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        0.4472  0.0513\n",
            "      6        0.4472  0.0416\n",
            "      7        0.4472  0.0354\n",
            "      8        0.4472  0.0370\n",
            "      9        0.4472  0.0405\n",
            "     10        0.4472  0.0415\n",
            "     11        0.4472  0.0429\n",
            "     12        0.4472  0.0418\n",
            "     13        0.4472  0.0479\n",
            "     14        0.4472  0.0480\n",
            "     15        0.4472  0.0416\n",
            "     16        0.4472  0.0394\n",
            "     17        0.4472  0.0387\n",
            "     18        0.4472  0.0469\n",
            "     19        0.4472  0.0371\n",
            "     20        0.4472  0.0447\n",
            "     21        0.4472  0.0374\n",
            "     22        0.4472  0.0361\n",
            "     23        0.4472  0.0424\n",
            "     24        0.4472  0.0398\n",
            "     25        0.4472  0.0379\n",
            "     26        0.4472  0.0377\n",
            "     27        0.4472  0.0415\n",
            "     28        0.4472  0.0385\n",
            "     29        0.4472  0.0421\n",
            "     30        0.4472  0.0360\n",
            "     31        0.4472  0.0455\n",
            "     32        0.4472  0.0380\n",
            "     33        0.4472  0.0422\n",
            "     34        0.4472  0.0415\n",
            "     35        0.4472  0.0394\n",
            "     36        0.4472  0.0421\n",
            "     37        0.4472  0.0402\n",
            "     38        0.4472  0.0405\n",
            "     39        0.4472  0.0385\n",
            "     40        0.4472  0.0383\n",
            "     41        0.4472  0.0376\n",
            "     42        0.4472  0.0467\n",
            "     43        0.4472  0.0369\n",
            "     44        0.4472  0.0412\n",
            "     45        0.4472  0.0408\n",
            "     46        0.4472  0.0405\n",
            "     47        0.4472  0.0377\n",
            "     48        0.4472  0.0383\n",
            "     49        0.4472  0.0380\n",
            "     50        0.4472  0.0393\n",
            "     51        0.4472  0.0403\n",
            "     52        0.4472  0.0398\n",
            "     53        0.4472  0.0450\n",
            "     54        0.4472  0.0407\n",
            "     55        0.4472  0.0440\n",
            "     56        0.4472  0.0522\n",
            "     57        0.4472  0.0408\n",
            "     58        0.4472  0.0432\n",
            "     59        0.4472  0.0421\n",
            "     60        0.4472  0.0429\n",
            "     61        0.4472  0.0399\n",
            "     62        0.4472  0.0380\n",
            "     63        0.4472  0.0443\n",
            "     64        0.4472  0.0427\n",
            "     65        0.4472  0.0551\n",
            "     66        0.4472  0.0356\n",
            "     67        0.4472  0.0545\n",
            "     68        0.4472  0.0408\n",
            "     69        0.4472  0.0394\n",
            "     70        0.4472  0.0398\n",
            "     71        0.4472  0.0398\n",
            "     72        0.4472  0.0427\n",
            "     73        0.4472  0.0436\n",
            "     74        0.4472  0.0444\n",
            "     75        0.4472  0.0435\n",
            "     76        0.4472  0.0419\n",
            "     77        0.4472  0.0398\n",
            "     78        0.4472  0.0380\n",
            "     79        0.4472  0.0463\n",
            "     80        0.4472  0.0419\n",
            "     81        0.4472  0.0429\n",
            "     82        0.4472  0.0411\n",
            "     83        0.4472  0.0494\n",
            "     84        0.4472  0.0494\n",
            "     85        0.4472  0.0519\n",
            "     86        0.4472  0.0382\n",
            "     87        0.4472  0.0471\n",
            "     88        0.4472  0.0372\n",
            "     89        0.4472  0.0398\n",
            "     90        0.4472  0.0393\n",
            "     91        0.4472  0.0458\n",
            "     92        0.4472  0.0392\n",
            "     93        0.4472  0.0428\n",
            "     94        0.4472  0.0406\n",
            "     95        0.4472  0.0410\n",
            "     96        0.4472  0.0405\n",
            "     97        0.4472  0.0376\n",
            "     98        0.4472  0.0380\n",
            "     99        0.4472  0.0450\n",
            "    100        0.4472  0.0443\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7017\u001b[0m  0.0340\n",
            "      2        \u001b[36m0.6982\u001b[0m  0.0410\n",
            "      3        0.6982  0.0525\n",
            "      4        0.6982  0.0449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        0.6982  0.0484\n",
            "      6        0.6982  0.0470\n",
            "      7        0.6982  0.0429\n",
            "      8        0.6982  0.0460\n",
            "      9        0.6982  0.0472\n",
            "     10        0.6982  0.0489\n",
            "     11        0.6982  0.0444\n",
            "     12        0.6982  0.0372\n",
            "     13        0.6982  0.0384\n",
            "     14        0.6982  0.0412\n",
            "     15        0.6982  0.0352\n",
            "     16        0.6982  0.0459\n",
            "     17        0.6982  0.0427\n",
            "     18        0.6982  0.0447\n",
            "     19        0.6982  0.0443\n",
            "     20        0.6982  0.0460\n",
            "     21        0.6982  0.0424\n",
            "     22        0.6982  0.0496\n",
            "     23        0.6982  0.0519\n",
            "     24        0.6982  0.0462\n",
            "     25        0.6982  0.0491\n",
            "     26        0.6982  0.0444\n",
            "     27        0.6982  0.0386\n",
            "     28        0.6982  0.0389\n",
            "     29        0.6982  0.0525\n",
            "     30        0.6982  0.0384\n",
            "     31        0.6982  0.0436\n",
            "     32        0.6982  0.0420\n",
            "     33        0.6982  0.0397\n",
            "     34        0.6982  0.0380\n",
            "     35        0.6982  0.0392\n",
            "     36        0.6982  0.0386\n",
            "     37        0.6982  0.0402\n",
            "     38        0.6982  0.0382\n",
            "     39        0.6982  0.0397\n",
            "     40        0.6982  0.0425\n",
            "     41        0.6982  0.0390\n",
            "     42        0.6982  0.0454\n",
            "     43        0.6982  0.0426\n",
            "     44        0.6982  0.0460\n",
            "     45        0.6982  0.0434\n",
            "     46        0.6982  0.0451\n",
            "     47        0.6982  0.0419\n",
            "     48        0.6982  0.0408\n",
            "     49        0.6982  0.0378\n",
            "     50        0.6982  0.0534\n",
            "     51        0.6982  0.0416\n",
            "     52        0.6982  0.0410\n",
            "     53        0.6982  0.0416\n",
            "     54        0.6982  0.0470\n",
            "     55        0.6982  0.0402\n",
            "     56        0.6982  0.0437\n",
            "     57        0.6982  0.0389\n",
            "     58        0.6982  0.0398\n",
            "     59        0.6982  0.0412\n",
            "     60        0.6982  0.0359\n",
            "     61        0.6982  0.0423\n",
            "     62        0.6982  0.0388\n",
            "     63        0.6982  0.0428\n",
            "     64        0.6982  0.0467\n",
            "     65        0.6982  0.0466\n",
            "     66        0.6982  0.0417\n",
            "     67        0.6982  0.0462\n",
            "     68        0.6982  0.0465\n",
            "     69        0.6982  0.0376\n",
            "     70        0.6982  0.0402\n",
            "     71        0.6982  0.0401\n",
            "     72        0.6982  0.0372\n",
            "     73        0.6982  0.0379\n",
            "     74        0.6982  0.0479\n",
            "     75        0.6982  0.0422\n",
            "     76        0.6982  0.0440\n",
            "     77        0.6982  0.0356\n",
            "     78        0.6982  0.0512\n",
            "     79        0.6982  0.0406\n",
            "     80        0.6982  0.0407\n",
            "     81        0.6982  0.0387\n",
            "     82        0.6982  0.0401\n",
            "     83        0.6982  0.0411\n",
            "     84        0.6982  0.0417\n",
            "     85        0.6982  0.0367\n",
            "     86        0.6982  0.0399\n",
            "     87        0.6982  0.0374\n",
            "     88        0.6982  0.0469\n",
            "     89        0.6982  0.0457\n",
            "     90        0.6982  0.0445\n",
            "     91        0.6982  0.0447\n",
            "     92        0.6982  0.0409\n",
            "     93        0.6982  0.0402\n",
            "     94        0.6982  0.0411\n",
            "     95        0.6982  0.0543\n",
            "     96        0.6982  0.0415\n",
            "     97        0.6982  0.0451\n",
            "     98        0.6982  0.0442\n",
            "     99        0.6982  0.0377\n",
            "    100        0.6982  0.0372\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9897\u001b[0m  0.0431\n",
            "      2        \u001b[36m0.9867\u001b[0m  0.0530\n",
            "      3        \u001b[36m0.9824\u001b[0m  0.0532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9754\u001b[0m  0.0577\n",
            "      5        \u001b[36m0.9624\u001b[0m  0.0493\n",
            "      6        \u001b[36m0.9323\u001b[0m  0.0493\n",
            "      7        \u001b[36m0.8573\u001b[0m  0.0536\n",
            "      8        \u001b[36m0.7336\u001b[0m  0.0604\n",
            "      9        \u001b[36m0.6142\u001b[0m  0.0551\n",
            "     10        \u001b[36m0.5291\u001b[0m  0.0594\n",
            "     11        \u001b[36m0.4725\u001b[0m  0.0527\n",
            "     12        \u001b[36m0.4395\u001b[0m  0.0542\n",
            "     13        \u001b[36m0.4232\u001b[0m  0.0552\n",
            "     14        \u001b[36m0.4144\u001b[0m  0.0513\n",
            "     15        \u001b[36m0.4087\u001b[0m  0.0494\n",
            "     16        \u001b[36m0.4044\u001b[0m  0.0491\n",
            "     17        \u001b[36m0.4010\u001b[0m  0.0464\n",
            "     18        \u001b[36m0.3983\u001b[0m  0.0619\n",
            "     19        \u001b[36m0.3960\u001b[0m  0.0531\n",
            "     20        \u001b[36m0.3941\u001b[0m  0.0513\n",
            "     21        \u001b[36m0.3924\u001b[0m  0.0498\n",
            "     22        \u001b[36m0.3909\u001b[0m  0.0547\n",
            "     23        \u001b[36m0.3896\u001b[0m  0.0543\n",
            "     24        \u001b[36m0.3885\u001b[0m  0.0663\n",
            "     25        \u001b[36m0.3875\u001b[0m  0.0550\n",
            "     26        \u001b[36m0.3866\u001b[0m  0.0548\n",
            "     27        \u001b[36m0.3858\u001b[0m  0.0622\n",
            "     28        \u001b[36m0.3850\u001b[0m  0.0527\n",
            "     29        \u001b[36m0.3843\u001b[0m  0.0458\n",
            "     30        \u001b[36m0.3837\u001b[0m  0.0504\n",
            "     31        \u001b[36m0.3832\u001b[0m  0.0484\n",
            "     32        \u001b[36m0.3827\u001b[0m  0.0509\n",
            "     33        \u001b[36m0.3822\u001b[0m  0.0463\n",
            "     34        \u001b[36m0.3817\u001b[0m  0.0546\n",
            "     35        \u001b[36m0.3813\u001b[0m  0.0505\n",
            "     36        \u001b[36m0.3810\u001b[0m  0.0511\n",
            "     37        \u001b[36m0.3806\u001b[0m  0.0566\n",
            "     38        \u001b[36m0.3803\u001b[0m  0.0475\n",
            "     39        \u001b[36m0.3800\u001b[0m  0.0547\n",
            "     40        \u001b[36m0.3797\u001b[0m  0.0579\n",
            "     41        \u001b[36m0.3794\u001b[0m  0.0549\n",
            "     42        \u001b[36m0.3792\u001b[0m  0.0612\n",
            "     43        \u001b[36m0.3789\u001b[0m  0.0476\n",
            "     44        \u001b[36m0.3787\u001b[0m  0.0538\n",
            "     45        \u001b[36m0.3785\u001b[0m  0.0544\n",
            "     46        \u001b[36m0.3783\u001b[0m  0.0573\n",
            "     47        \u001b[36m0.3781\u001b[0m  0.0505\n",
            "     48        \u001b[36m0.3780\u001b[0m  0.0493\n",
            "     49        \u001b[36m0.3778\u001b[0m  0.0471\n",
            "     50        \u001b[36m0.3776\u001b[0m  0.0488\n",
            "     51        \u001b[36m0.3775\u001b[0m  0.0512\n",
            "     52        \u001b[36m0.3773\u001b[0m  0.0478\n",
            "     53        \u001b[36m0.3772\u001b[0m  0.0477\n",
            "     54        \u001b[36m0.3771\u001b[0m  0.0488\n",
            "     55        \u001b[36m0.3770\u001b[0m  0.0513\n",
            "     56        \u001b[36m0.3768\u001b[0m  0.0491\n",
            "     57        \u001b[36m0.3767\u001b[0m  0.0503\n",
            "     58        \u001b[36m0.3766\u001b[0m  0.0522\n",
            "     59        \u001b[36m0.3765\u001b[0m  0.0481\n",
            "     60        \u001b[36m0.3764\u001b[0m  0.0483\n",
            "     61        \u001b[36m0.3763\u001b[0m  0.0518\n",
            "     62        \u001b[36m0.3762\u001b[0m  0.0564\n",
            "     63        \u001b[36m0.3762\u001b[0m  0.0527\n",
            "     64        \u001b[36m0.3761\u001b[0m  0.0560\n",
            "     65        \u001b[36m0.3760\u001b[0m  0.0543\n",
            "     66        \u001b[36m0.3759\u001b[0m  0.0519\n",
            "     67        \u001b[36m0.3758\u001b[0m  0.0558\n",
            "     68        \u001b[36m0.3758\u001b[0m  0.0510\n",
            "     69        \u001b[36m0.3757\u001b[0m  0.0502\n",
            "     70        \u001b[36m0.3756\u001b[0m  0.0477\n",
            "     71        \u001b[36m0.3756\u001b[0m  0.0524\n",
            "     72        \u001b[36m0.3755\u001b[0m  0.0480\n",
            "     73        \u001b[36m0.3755\u001b[0m  0.0484\n",
            "     74        \u001b[36m0.3754\u001b[0m  0.0559\n",
            "     75        \u001b[36m0.3754\u001b[0m  0.0537\n",
            "     76        \u001b[36m0.3753\u001b[0m  0.0529\n",
            "     77        \u001b[36m0.3753\u001b[0m  0.0525\n",
            "     78        \u001b[36m0.3752\u001b[0m  0.0552\n",
            "     79        \u001b[36m0.3752\u001b[0m  0.0596\n",
            "     80        \u001b[36m0.3751\u001b[0m  0.0576\n",
            "     81        \u001b[36m0.3751\u001b[0m  0.0616\n",
            "     82        \u001b[36m0.3750\u001b[0m  0.0559\n",
            "     83        \u001b[36m0.3750\u001b[0m  0.0519\n",
            "     84        \u001b[36m0.3749\u001b[0m  0.0568\n",
            "     85        \u001b[36m0.3749\u001b[0m  0.0471\n",
            "     86        \u001b[36m0.3749\u001b[0m  0.0485\n",
            "     87        \u001b[36m0.3748\u001b[0m  0.0502\n",
            "     88        \u001b[36m0.3748\u001b[0m  0.0440\n",
            "     89        \u001b[36m0.3748\u001b[0m  0.0566\n",
            "     90        \u001b[36m0.3747\u001b[0m  0.0444\n",
            "     91        \u001b[36m0.3747\u001b[0m  0.0496\n",
            "     92        \u001b[36m0.3747\u001b[0m  0.0496\n",
            "     93        \u001b[36m0.3746\u001b[0m  0.0710\n",
            "     94        \u001b[36m0.3746\u001b[0m  0.0535\n",
            "     95        \u001b[36m0.3746\u001b[0m  0.0511\n",
            "     96        \u001b[36m0.3746\u001b[0m  0.0569\n",
            "     97        \u001b[36m0.3745\u001b[0m  0.0533\n",
            "     98        \u001b[36m0.3745\u001b[0m  0.0583\n",
            "     99        \u001b[36m0.3745\u001b[0m  0.0601\n",
            "    100        \u001b[36m0.3745\u001b[0m  0.0534\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9879\u001b[0m  0.0476\n",
            "      2        \u001b[36m0.9843\u001b[0m  0.0504\n",
            "      3        \u001b[36m0.9791\u001b[0m  0.0476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9712\u001b[0m  0.0631\n",
            "      5        \u001b[36m0.9573\u001b[0m  0.0515\n",
            "      6        \u001b[36m0.9269\u001b[0m  0.0519\n",
            "      7        \u001b[36m0.8420\u001b[0m  0.0517\n",
            "      8        \u001b[36m0.6621\u001b[0m  0.0497\n",
            "      9        \u001b[36m0.5114\u001b[0m  0.0493\n",
            "     10        \u001b[36m0.4525\u001b[0m  0.0498\n",
            "     11        \u001b[36m0.4294\u001b[0m  0.0620\n",
            "     12        \u001b[36m0.4173\u001b[0m  0.0498\n",
            "     13        \u001b[36m0.4096\u001b[0m  0.0543\n",
            "     14        \u001b[36m0.4041\u001b[0m  0.0479\n",
            "     15        \u001b[36m0.3999\u001b[0m  0.0492\n",
            "     16        \u001b[36m0.3967\u001b[0m  0.0540\n",
            "     17        \u001b[36m0.3940\u001b[0m  0.0604\n",
            "     18        \u001b[36m0.3917\u001b[0m  0.0556\n",
            "     19        \u001b[36m0.3899\u001b[0m  0.0566\n",
            "     20        \u001b[36m0.3882\u001b[0m  0.0543\n",
            "     21        \u001b[36m0.3868\u001b[0m  0.0478\n",
            "     22        \u001b[36m0.3856\u001b[0m  0.0512\n",
            "     23        \u001b[36m0.3846\u001b[0m  0.0529\n",
            "     24        \u001b[36m0.3836\u001b[0m  0.0496\n",
            "     25        \u001b[36m0.3828\u001b[0m  0.0490\n",
            "     26        \u001b[36m0.3821\u001b[0m  0.0562\n",
            "     27        \u001b[36m0.3814\u001b[0m  0.0564\n",
            "     28        \u001b[36m0.3808\u001b[0m  0.0487\n",
            "     29        \u001b[36m0.3803\u001b[0m  0.0747\n",
            "     30        \u001b[36m0.3798\u001b[0m  0.0519\n",
            "     31        \u001b[36m0.3794\u001b[0m  0.0523\n",
            "     32        \u001b[36m0.3790\u001b[0m  0.0554\n",
            "     33        \u001b[36m0.3786\u001b[0m  0.0557\n",
            "     34        \u001b[36m0.3782\u001b[0m  0.0558\n",
            "     35        \u001b[36m0.3779\u001b[0m  0.0631\n",
            "     36        \u001b[36m0.3776\u001b[0m  0.0519\n",
            "     37        \u001b[36m0.3774\u001b[0m  0.0480\n",
            "     38        \u001b[36m0.3771\u001b[0m  0.0477\n",
            "     39        \u001b[36m0.3769\u001b[0m  0.0484\n",
            "     40        \u001b[36m0.3767\u001b[0m  0.0499\n",
            "     41        \u001b[36m0.3765\u001b[0m  0.0512\n",
            "     42        \u001b[36m0.3763\u001b[0m  0.0581\n",
            "     43        \u001b[36m0.3761\u001b[0m  0.0476\n",
            "     44        \u001b[36m0.3760\u001b[0m  0.0486\n",
            "     45        \u001b[36m0.3758\u001b[0m  0.0500\n",
            "     46        \u001b[36m0.3757\u001b[0m  0.0539\n",
            "     47        \u001b[36m0.3755\u001b[0m  0.0616\n",
            "     48        \u001b[36m0.3754\u001b[0m  0.0557\n",
            "     49        \u001b[36m0.3753\u001b[0m  0.0580\n",
            "     50        \u001b[36m0.3752\u001b[0m  0.0650\n",
            "     51        \u001b[36m0.3751\u001b[0m  0.0630\n",
            "     52        \u001b[36m0.3749\u001b[0m  0.0635\n",
            "     53        \u001b[36m0.3748\u001b[0m  0.0599\n",
            "     54        \u001b[36m0.3747\u001b[0m  0.0668\n",
            "     55        \u001b[36m0.3746\u001b[0m  0.0505\n",
            "     56        \u001b[36m0.3746\u001b[0m  0.0506\n",
            "     57        \u001b[36m0.3745\u001b[0m  0.0602\n",
            "     58        \u001b[36m0.3744\u001b[0m  0.0541\n",
            "     59        \u001b[36m0.3743\u001b[0m  0.0536\n",
            "     60        \u001b[36m0.3743\u001b[0m  0.0498\n",
            "     61        \u001b[36m0.3742\u001b[0m  0.0529\n",
            "     62        \u001b[36m0.3741\u001b[0m  0.0558\n",
            "     63        \u001b[36m0.3741\u001b[0m  0.0602\n",
            "     64        \u001b[36m0.3740\u001b[0m  0.0660\n",
            "     65        \u001b[36m0.3739\u001b[0m  0.0552\n",
            "     66        \u001b[36m0.3739\u001b[0m  0.0562\n",
            "     67        \u001b[36m0.3738\u001b[0m  0.0584\n",
            "     68        \u001b[36m0.3738\u001b[0m  0.0654\n",
            "     69        \u001b[36m0.3737\u001b[0m  0.0574\n",
            "     70        \u001b[36m0.3737\u001b[0m  0.0633\n",
            "     71        \u001b[36m0.3736\u001b[0m  0.0664\n",
            "     72        \u001b[36m0.3736\u001b[0m  0.0614\n",
            "     73        \u001b[36m0.3736\u001b[0m  0.0555\n",
            "     74        \u001b[36m0.3735\u001b[0m  0.0494\n",
            "     75        \u001b[36m0.3735\u001b[0m  0.0501\n",
            "     76        \u001b[36m0.3734\u001b[0m  0.0496\n",
            "     77        \u001b[36m0.3734\u001b[0m  0.0761\n",
            "     78        \u001b[36m0.3734\u001b[0m  0.0464\n",
            "     79        \u001b[36m0.3733\u001b[0m  0.0556\n",
            "     80        \u001b[36m0.3733\u001b[0m  0.0522\n",
            "     81        \u001b[36m0.3733\u001b[0m  0.0610\n",
            "     82        \u001b[36m0.3732\u001b[0m  0.0605\n",
            "     83        \u001b[36m0.3732\u001b[0m  0.0622\n",
            "     84        \u001b[36m0.3732\u001b[0m  0.0705\n",
            "     85        \u001b[36m0.3732\u001b[0m  0.0672\n",
            "     86        \u001b[36m0.3731\u001b[0m  0.0549\n",
            "     87        \u001b[36m0.3731\u001b[0m  0.0558\n",
            "     88        \u001b[36m0.3731\u001b[0m  0.0529\n",
            "     89        \u001b[36m0.3731\u001b[0m  0.0555\n",
            "     90        \u001b[36m0.3730\u001b[0m  0.0472\n",
            "     91        \u001b[36m0.3730\u001b[0m  0.0523\n",
            "     92        \u001b[36m0.3730\u001b[0m  0.0570\n",
            "     93        \u001b[36m0.3730\u001b[0m  0.0543\n",
            "     94        \u001b[36m0.3729\u001b[0m  0.0482\n",
            "     95        \u001b[36m0.3729\u001b[0m  0.0527\n",
            "     96        \u001b[36m0.3729\u001b[0m  0.0524\n",
            "     97        \u001b[36m0.3729\u001b[0m  0.0633\n",
            "     98        \u001b[36m0.3729\u001b[0m  0.0665\n",
            "     99        \u001b[36m0.3728\u001b[0m  0.0562\n",
            "    100        \u001b[36m0.3728\u001b[0m  0.0633\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9952\u001b[0m  0.0443\n",
            "      2        \u001b[36m0.9952\u001b[0m  0.0524\n",
            "      3        \u001b[36m0.9952\u001b[0m  0.0509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9952\u001b[0m  0.0527\n",
            "      5        \u001b[36m0.9952\u001b[0m  0.0413\n",
            "      6        \u001b[36m0.9952\u001b[0m  0.0373\n",
            "      7        \u001b[36m0.9952\u001b[0m  0.0377\n",
            "      8        \u001b[36m0.9952\u001b[0m  0.0419\n",
            "      9        \u001b[36m0.9952\u001b[0m  0.0406\n",
            "     10        \u001b[36m0.9952\u001b[0m  0.0416\n",
            "     11        \u001b[36m0.9952\u001b[0m  0.0393\n",
            "     12        \u001b[36m0.9952\u001b[0m  0.0415\n",
            "     13        \u001b[36m0.9952\u001b[0m  0.0364\n",
            "     14        \u001b[36m0.9952\u001b[0m  0.0373\n",
            "     15        \u001b[36m0.9952\u001b[0m  0.0380\n",
            "     16        \u001b[36m0.9952\u001b[0m  0.0396\n",
            "     17        \u001b[36m0.9951\u001b[0m  0.0399\n",
            "     18        \u001b[36m0.9951\u001b[0m  0.0369\n",
            "     19        \u001b[36m0.9951\u001b[0m  0.0535\n",
            "     20        \u001b[36m0.9951\u001b[0m  0.0431\n",
            "     21        \u001b[36m0.9951\u001b[0m  0.0437\n",
            "     22        \u001b[36m0.9951\u001b[0m  0.0456\n",
            "     23        \u001b[36m0.9951\u001b[0m  0.0472\n",
            "     24        \u001b[36m0.9951\u001b[0m  0.0424\n",
            "     25        \u001b[36m0.9951\u001b[0m  0.0456\n",
            "     26        \u001b[36m0.9951\u001b[0m  0.0394\n",
            "     27        \u001b[36m0.9951\u001b[0m  0.0399\n",
            "     28        \u001b[36m0.9951\u001b[0m  0.0450\n",
            "     29        \u001b[36m0.9951\u001b[0m  0.0422\n",
            "     30        \u001b[36m0.9951\u001b[0m  0.0362\n",
            "     31        \u001b[36m0.9951\u001b[0m  0.0386\n",
            "     32        \u001b[36m0.9951\u001b[0m  0.0477\n",
            "     33        \u001b[36m0.9951\u001b[0m  0.0414\n",
            "     34        \u001b[36m0.9950\u001b[0m  0.0345\n",
            "     35        \u001b[36m0.9950\u001b[0m  0.0399\n",
            "     36        \u001b[36m0.9950\u001b[0m  0.0435\n",
            "     37        \u001b[36m0.9950\u001b[0m  0.0408\n",
            "     38        \u001b[36m0.9950\u001b[0m  0.0474\n",
            "     39        \u001b[36m0.9950\u001b[0m  0.0410\n",
            "     40        \u001b[36m0.9950\u001b[0m  0.0470\n",
            "     41        \u001b[36m0.9950\u001b[0m  0.0468\n",
            "     42        \u001b[36m0.9950\u001b[0m  0.0447\n",
            "     43        \u001b[36m0.9950\u001b[0m  0.0408\n",
            "     44        \u001b[36m0.9950\u001b[0m  0.0409\n",
            "     45        \u001b[36m0.9950\u001b[0m  0.0388\n",
            "     46        \u001b[36m0.9950\u001b[0m  0.0460\n",
            "     47        \u001b[36m0.9950\u001b[0m  0.0424\n",
            "     48        \u001b[36m0.9950\u001b[0m  0.0392\n",
            "     49        \u001b[36m0.9949\u001b[0m  0.0377\n",
            "     50        \u001b[36m0.9949\u001b[0m  0.0408\n",
            "     51        \u001b[36m0.9949\u001b[0m  0.0373\n",
            "     52        \u001b[36m0.9949\u001b[0m  0.0388\n",
            "     53        \u001b[36m0.9949\u001b[0m  0.0482\n",
            "     54        \u001b[36m0.9949\u001b[0m  0.0394\n",
            "     55        \u001b[36m0.9949\u001b[0m  0.0399\n",
            "     56        \u001b[36m0.9949\u001b[0m  0.0446\n",
            "     57        \u001b[36m0.9949\u001b[0m  0.0388\n",
            "     58        \u001b[36m0.9949\u001b[0m  0.0384\n",
            "     59        \u001b[36m0.9949\u001b[0m  0.0391\n",
            "     60        \u001b[36m0.9949\u001b[0m  0.0440\n",
            "     61        \u001b[36m0.9949\u001b[0m  0.0410\n",
            "     62        \u001b[36m0.9949\u001b[0m  0.0433\n",
            "     63        \u001b[36m0.9949\u001b[0m  0.0420\n",
            "     64        \u001b[36m0.9948\u001b[0m  0.0423\n",
            "     65        \u001b[36m0.9948\u001b[0m  0.0503\n",
            "     66        \u001b[36m0.9948\u001b[0m  0.0430\n",
            "     67        \u001b[36m0.9948\u001b[0m  0.0467\n",
            "     68        \u001b[36m0.9948\u001b[0m  0.0438\n",
            "     69        \u001b[36m0.9948\u001b[0m  0.0461\n",
            "     70        \u001b[36m0.9948\u001b[0m  0.0400\n",
            "     71        \u001b[36m0.9948\u001b[0m  0.0378\n",
            "     72        \u001b[36m0.9948\u001b[0m  0.0378\n",
            "     73        \u001b[36m0.9948\u001b[0m  0.0365\n",
            "     74        \u001b[36m0.9948\u001b[0m  0.0471\n",
            "     75        \u001b[36m0.9948\u001b[0m  0.0385\n",
            "     76        \u001b[36m0.9948\u001b[0m  0.0396\n",
            "     77        \u001b[36m0.9948\u001b[0m  0.0430\n",
            "     78        \u001b[36m0.9948\u001b[0m  0.0410\n",
            "     79        \u001b[36m0.9947\u001b[0m  0.0413\n",
            "     80        \u001b[36m0.9947\u001b[0m  0.0430\n",
            "     81        \u001b[36m0.9947\u001b[0m  0.0394\n",
            "     82        \u001b[36m0.9947\u001b[0m  0.0390\n",
            "     83        \u001b[36m0.9947\u001b[0m  0.0412\n",
            "     84        \u001b[36m0.9947\u001b[0m  0.0426\n",
            "     85        \u001b[36m0.9947\u001b[0m  0.0436\n",
            "     86        \u001b[36m0.9947\u001b[0m  0.0451\n",
            "     87        \u001b[36m0.9947\u001b[0m  0.0504\n",
            "     88        \u001b[36m0.9947\u001b[0m  0.0442\n",
            "     89        \u001b[36m0.9947\u001b[0m  0.0415\n",
            "     90        \u001b[36m0.9947\u001b[0m  0.0402\n",
            "     91        \u001b[36m0.9947\u001b[0m  0.0411\n",
            "     92        \u001b[36m0.9947\u001b[0m  0.0399\n",
            "     93        \u001b[36m0.9946\u001b[0m  0.0469\n",
            "     94        \u001b[36m0.9946\u001b[0m  0.0425\n",
            "     95        \u001b[36m0.9946\u001b[0m  0.0364\n",
            "     96        \u001b[36m0.9946\u001b[0m  0.0429\n",
            "     97        \u001b[36m0.9946\u001b[0m  0.0390\n",
            "     98        \u001b[36m0.9946\u001b[0m  0.0432\n",
            "     99        \u001b[36m0.9946\u001b[0m  0.0393\n",
            "    100        \u001b[36m0.9946\u001b[0m  0.0381\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9935\u001b[0m  0.0339\n",
            "      2        \u001b[36m0.9935\u001b[0m  0.0387\n",
            "      3        \u001b[36m0.9935\u001b[0m  0.0409\n",
            "      4        \u001b[36m0.9934\u001b[0m  0.0438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.9934\u001b[0m  0.0459\n",
            "      6        \u001b[36m0.9934\u001b[0m  0.0380\n",
            "      7        \u001b[36m0.9934\u001b[0m  0.0397\n",
            "      8        \u001b[36m0.9934\u001b[0m  0.0402\n",
            "      9        \u001b[36m0.9934\u001b[0m  0.0424\n",
            "     10        \u001b[36m0.9934\u001b[0m  0.0486\n",
            "     11        \u001b[36m0.9934\u001b[0m  0.0513\n",
            "     12        \u001b[36m0.9934\u001b[0m  0.0451\n",
            "     13        \u001b[36m0.9933\u001b[0m  0.0380\n",
            "     14        \u001b[36m0.9933\u001b[0m  0.0380\n",
            "     15        \u001b[36m0.9933\u001b[0m  0.0372\n",
            "     16        \u001b[36m0.9933\u001b[0m  0.0371\n",
            "     17        \u001b[36m0.9933\u001b[0m  0.0400\n",
            "     18        \u001b[36m0.9933\u001b[0m  0.0376\n",
            "     19        \u001b[36m0.9933\u001b[0m  0.0377\n",
            "     20        \u001b[36m0.9933\u001b[0m  0.0376\n",
            "     21        \u001b[36m0.9933\u001b[0m  0.0414\n",
            "     22        \u001b[36m0.9932\u001b[0m  0.0381\n",
            "     23        \u001b[36m0.9932\u001b[0m  0.0466\n",
            "     24        \u001b[36m0.9932\u001b[0m  0.0394\n",
            "     25        \u001b[36m0.9932\u001b[0m  0.0415\n",
            "     26        \u001b[36m0.9932\u001b[0m  0.0371\n",
            "     27        \u001b[36m0.9932\u001b[0m  0.0388\n",
            "     28        \u001b[36m0.9932\u001b[0m  0.0397\n",
            "     29        \u001b[36m0.9932\u001b[0m  0.0421\n",
            "     30        \u001b[36m0.9931\u001b[0m  0.0359\n",
            "     31        \u001b[36m0.9931\u001b[0m  0.0378\n",
            "     32        \u001b[36m0.9931\u001b[0m  0.0469\n",
            "     33        \u001b[36m0.9931\u001b[0m  0.0472\n",
            "     34        \u001b[36m0.9931\u001b[0m  0.0439\n",
            "     35        \u001b[36m0.9931\u001b[0m  0.0458\n",
            "     36        \u001b[36m0.9931\u001b[0m  0.0436\n",
            "     37        \u001b[36m0.9931\u001b[0m  0.0444\n",
            "     38        \u001b[36m0.9930\u001b[0m  0.0389\n",
            "     39        \u001b[36m0.9930\u001b[0m  0.0411\n",
            "     40        \u001b[36m0.9930\u001b[0m  0.0418\n",
            "     41        \u001b[36m0.9930\u001b[0m  0.0371\n",
            "     42        \u001b[36m0.9930\u001b[0m  0.0355\n",
            "     43        \u001b[36m0.9930\u001b[0m  0.0410\n",
            "     44        \u001b[36m0.9930\u001b[0m  0.0445\n",
            "     45        \u001b[36m0.9930\u001b[0m  0.0374\n",
            "     46        \u001b[36m0.9929\u001b[0m  0.0403\n",
            "     47        \u001b[36m0.9929\u001b[0m  0.0381\n",
            "     48        \u001b[36m0.9929\u001b[0m  0.0406\n",
            "     49        \u001b[36m0.9929\u001b[0m  0.0392\n",
            "     50        \u001b[36m0.9929\u001b[0m  0.0396\n",
            "     51        \u001b[36m0.9929\u001b[0m  0.0414\n",
            "     52        \u001b[36m0.9929\u001b[0m  0.0384\n",
            "     53        \u001b[36m0.9929\u001b[0m  0.0421\n",
            "     54        \u001b[36m0.9928\u001b[0m  0.0393\n",
            "     55        \u001b[36m0.9928\u001b[0m  0.0474\n",
            "     56        \u001b[36m0.9928\u001b[0m  0.0447\n",
            "     57        \u001b[36m0.9928\u001b[0m  0.0510\n",
            "     58        \u001b[36m0.9928\u001b[0m  0.0601\n",
            "     59        \u001b[36m0.9928\u001b[0m  0.0450\n",
            "     60        \u001b[36m0.9928\u001b[0m  0.0463\n",
            "     61        \u001b[36m0.9927\u001b[0m  0.0488\n",
            "     62        \u001b[36m0.9927\u001b[0m  0.0441\n",
            "     63        \u001b[36m0.9927\u001b[0m  0.0454\n",
            "     64        \u001b[36m0.9927\u001b[0m  0.0395\n",
            "     65        \u001b[36m0.9927\u001b[0m  0.0407\n",
            "     66        \u001b[36m0.9927\u001b[0m  0.0394\n",
            "     67        \u001b[36m0.9927\u001b[0m  0.0399\n",
            "     68        \u001b[36m0.9927\u001b[0m  0.0395\n",
            "     69        \u001b[36m0.9926\u001b[0m  0.0444\n",
            "     70        \u001b[36m0.9926\u001b[0m  0.0386\n",
            "     71        \u001b[36m0.9926\u001b[0m  0.0384\n",
            "     72        \u001b[36m0.9926\u001b[0m  0.0406\n",
            "     73        \u001b[36m0.9926\u001b[0m  0.0351\n",
            "     74        \u001b[36m0.9926\u001b[0m  0.0410\n",
            "     75        \u001b[36m0.9926\u001b[0m  0.0393\n",
            "     76        \u001b[36m0.9925\u001b[0m  0.0402\n",
            "     77        \u001b[36m0.9925\u001b[0m  0.0450\n",
            "     78        \u001b[36m0.9925\u001b[0m  0.0461\n",
            "     79        \u001b[36m0.9925\u001b[0m  0.0413\n",
            "     80        \u001b[36m0.9925\u001b[0m  0.0518\n",
            "     81        \u001b[36m0.9925\u001b[0m  0.0407\n",
            "     82        \u001b[36m0.9925\u001b[0m  0.0445\n",
            "     83        \u001b[36m0.9924\u001b[0m  0.0390\n",
            "     84        \u001b[36m0.9924\u001b[0m  0.0417\n",
            "     85        \u001b[36m0.9924\u001b[0m  0.0408\n",
            "     86        \u001b[36m0.9924\u001b[0m  0.0383\n",
            "     87        \u001b[36m0.9924\u001b[0m  0.0407\n",
            "     88        \u001b[36m0.9924\u001b[0m  0.0417\n",
            "     89        \u001b[36m0.9923\u001b[0m  0.0370\n",
            "     90        \u001b[36m0.9923\u001b[0m  0.0378\n",
            "     91        \u001b[36m0.9923\u001b[0m  0.0420\n",
            "     92        \u001b[36m0.9923\u001b[0m  0.0400\n",
            "     93        \u001b[36m0.9923\u001b[0m  0.0354\n",
            "     94        \u001b[36m0.9923\u001b[0m  0.0393\n",
            "     95        \u001b[36m0.9923\u001b[0m  0.0403\n",
            "     96        \u001b[36m0.9922\u001b[0m  0.0413\n",
            "     97        \u001b[36m0.9922\u001b[0m  0.0406\n",
            "     98        \u001b[36m0.9922\u001b[0m  0.0417\n",
            "     99        \u001b[36m0.9922\u001b[0m  0.0409\n",
            "    100        \u001b[36m0.9922\u001b[0m  0.0414\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9993\u001b[0m  0.0490\n",
            "      2        \u001b[36m0.9989\u001b[0m  0.0504\n",
            "      3        \u001b[36m0.9980\u001b[0m  0.0577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9961\u001b[0m  0.0658\n",
            "      5        \u001b[36m0.9909\u001b[0m  0.0523\n",
            "      6        \u001b[36m0.9766\u001b[0m  0.0512\n",
            "      7        \u001b[36m0.9355\u001b[0m  0.0550\n",
            "      8        \u001b[36m0.8337\u001b[0m  0.0537\n",
            "      9        \u001b[36m0.6659\u001b[0m  0.0483\n",
            "     10        \u001b[36m0.5203\u001b[0m  0.0522\n",
            "     11        \u001b[36m0.4251\u001b[0m  0.0536\n",
            "     12        \u001b[36m0.3892\u001b[0m  0.0533\n",
            "     13        \u001b[36m0.3846\u001b[0m  0.0614\n",
            "     14        \u001b[36m0.3832\u001b[0m  0.0536\n",
            "     15        \u001b[36m0.3823\u001b[0m  0.0515\n",
            "     16        \u001b[36m0.3816\u001b[0m  0.0562\n",
            "     17        \u001b[36m0.3810\u001b[0m  0.0587\n",
            "     18        \u001b[36m0.3805\u001b[0m  0.0579\n",
            "     19        \u001b[36m0.3796\u001b[0m  0.0538\n",
            "     20        \u001b[36m0.3783\u001b[0m  0.0682\n",
            "     21        \u001b[36m0.3779\u001b[0m  0.0625\n",
            "     22        \u001b[36m0.3771\u001b[0m  0.0572\n",
            "     23        \u001b[36m0.3765\u001b[0m  0.0571\n",
            "     24        \u001b[36m0.3764\u001b[0m  0.0600\n",
            "     25        \u001b[36m0.3763\u001b[0m  0.0560\n",
            "     26        \u001b[36m0.3762\u001b[0m  0.0510\n",
            "     27        \u001b[36m0.3761\u001b[0m  0.0501\n",
            "     28        \u001b[36m0.3760\u001b[0m  0.0480\n",
            "     29        \u001b[36m0.3759\u001b[0m  0.0529\n",
            "     30        \u001b[36m0.3758\u001b[0m  0.0555\n",
            "     31        \u001b[36m0.3758\u001b[0m  0.0591\n",
            "     32        \u001b[36m0.3757\u001b[0m  0.0510\n",
            "     33        \u001b[36m0.3756\u001b[0m  0.0565\n",
            "     34        \u001b[36m0.3756\u001b[0m  0.0655\n",
            "     35        \u001b[36m0.3755\u001b[0m  0.0574\n",
            "     36        \u001b[36m0.3754\u001b[0m  0.0563\n",
            "     37        \u001b[36m0.3754\u001b[0m  0.0639\n",
            "     38        \u001b[36m0.3753\u001b[0m  0.0488\n",
            "     39        \u001b[36m0.3753\u001b[0m  0.0597\n",
            "     40        \u001b[36m0.3752\u001b[0m  0.0502\n",
            "     41        \u001b[36m0.3752\u001b[0m  0.0537\n",
            "     42        \u001b[36m0.3751\u001b[0m  0.0495\n",
            "     43        \u001b[36m0.3751\u001b[0m  0.0508\n",
            "     44        \u001b[36m0.3750\u001b[0m  0.0531\n",
            "     45        \u001b[36m0.3750\u001b[0m  0.0481\n",
            "     46        \u001b[36m0.3749\u001b[0m  0.0508\n",
            "     47        \u001b[36m0.3749\u001b[0m  0.0532\n",
            "     48        \u001b[36m0.3748\u001b[0m  0.0558\n",
            "     49        \u001b[36m0.3748\u001b[0m  0.0537\n",
            "     50        \u001b[36m0.3748\u001b[0m  0.0571\n",
            "     51        \u001b[36m0.3747\u001b[0m  0.0654\n",
            "     52        \u001b[36m0.3747\u001b[0m  0.0603\n",
            "     53        \u001b[36m0.3747\u001b[0m  0.0555\n",
            "     54        \u001b[36m0.3746\u001b[0m  0.0534\n",
            "     55        \u001b[36m0.3746\u001b[0m  0.0721\n",
            "     56        \u001b[36m0.3746\u001b[0m  0.0540\n",
            "     57        \u001b[36m0.3745\u001b[0m  0.0486\n",
            "     58        \u001b[36m0.3745\u001b[0m  0.0544\n",
            "     59        \u001b[36m0.3745\u001b[0m  0.0504\n",
            "     60        \u001b[36m0.3744\u001b[0m  0.0531\n",
            "     61        \u001b[36m0.3744\u001b[0m  0.0565\n",
            "     62        \u001b[36m0.3744\u001b[0m  0.0608\n",
            "     63        \u001b[36m0.3744\u001b[0m  0.0550\n",
            "     64        \u001b[36m0.3743\u001b[0m  0.0571\n",
            "     65        \u001b[36m0.3743\u001b[0m  0.0543\n",
            "     66        \u001b[36m0.3743\u001b[0m  0.0602\n",
            "     67        \u001b[36m0.3743\u001b[0m  0.0556\n",
            "     68        \u001b[36m0.3742\u001b[0m  0.0525\n",
            "     69        \u001b[36m0.3742\u001b[0m  0.0595\n",
            "     70        \u001b[36m0.3742\u001b[0m  0.0550\n",
            "     71        \u001b[36m0.3742\u001b[0m  0.0541\n",
            "     72        \u001b[36m0.3742\u001b[0m  0.0532\n",
            "     73        \u001b[36m0.3741\u001b[0m  0.0722\n",
            "     74        \u001b[36m0.3741\u001b[0m  0.0518\n",
            "     75        \u001b[36m0.3741\u001b[0m  0.0603\n",
            "     76        \u001b[36m0.3741\u001b[0m  0.0584\n",
            "     77        \u001b[36m0.3741\u001b[0m  0.0497\n",
            "     78        \u001b[36m0.3741\u001b[0m  0.0514\n",
            "     79        \u001b[36m0.3740\u001b[0m  0.0532\n",
            "     80        \u001b[36m0.3740\u001b[0m  0.0571\n",
            "     81        \u001b[36m0.3740\u001b[0m  0.0607\n",
            "     82        \u001b[36m0.3740\u001b[0m  0.0534\n",
            "     83        \u001b[36m0.3740\u001b[0m  0.0601\n",
            "     84        \u001b[36m0.3740\u001b[0m  0.0571\n",
            "     85        \u001b[36m0.3740\u001b[0m  0.0542\n",
            "     86        \u001b[36m0.3739\u001b[0m  0.0593\n",
            "     87        \u001b[36m0.3739\u001b[0m  0.0626\n",
            "     88        \u001b[36m0.3739\u001b[0m  0.0541\n",
            "     89        \u001b[36m0.3739\u001b[0m  0.0495\n",
            "     90        \u001b[36m0.3739\u001b[0m  0.0617\n",
            "     91        \u001b[36m0.3739\u001b[0m  0.0535\n",
            "     92        \u001b[36m0.3739\u001b[0m  0.0498\n",
            "     93        \u001b[36m0.3739\u001b[0m  0.0534\n",
            "     94        \u001b[36m0.3738\u001b[0m  0.0504\n",
            "     95        \u001b[36m0.3738\u001b[0m  0.0482\n",
            "     96        \u001b[36m0.3738\u001b[0m  0.0517\n",
            "     97        \u001b[36m0.3738\u001b[0m  0.0563\n",
            "     98        \u001b[36m0.3738\u001b[0m  0.0568\n",
            "     99        \u001b[36m0.3738\u001b[0m  0.0559\n",
            "    100        \u001b[36m0.3738\u001b[0m  0.0504\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9993\u001b[0m  0.0466\n",
            "      2        \u001b[36m0.9988\u001b[0m  0.0609\n",
            "      3        \u001b[36m0.9979\u001b[0m  0.0597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.9956\u001b[0m  0.0770\n",
            "      5        \u001b[36m0.9897\u001b[0m  0.0582\n",
            "      6        \u001b[36m0.9729\u001b[0m  0.0515\n",
            "      7        \u001b[36m0.9218\u001b[0m  0.0636\n",
            "      8        \u001b[36m0.7468\u001b[0m  0.0521\n",
            "      9        \u001b[36m0.4999\u001b[0m  0.0500\n",
            "     10        \u001b[36m0.4336\u001b[0m  0.0516\n",
            "     11        \u001b[36m0.4137\u001b[0m  0.0578\n",
            "     12        \u001b[36m0.4034\u001b[0m  0.0480\n",
            "     13        \u001b[36m0.3955\u001b[0m  0.0484\n",
            "     14        \u001b[36m0.3841\u001b[0m  0.0514\n",
            "     15        \u001b[36m0.3803\u001b[0m  0.0472\n",
            "     16        \u001b[36m0.3795\u001b[0m  0.0573\n",
            "     17        \u001b[36m0.3789\u001b[0m  0.0543\n",
            "     18        \u001b[36m0.3784\u001b[0m  0.0570\n",
            "     19        \u001b[36m0.3769\u001b[0m  0.0515\n",
            "     20        \u001b[36m0.3750\u001b[0m  0.0577\n",
            "     21        \u001b[36m0.3747\u001b[0m  0.0596\n",
            "     22        \u001b[36m0.3743\u001b[0m  0.0539\n",
            "     23        \u001b[36m0.3742\u001b[0m  0.0542\n",
            "     24        \u001b[36m0.3741\u001b[0m  0.0545\n",
            "     25        \u001b[36m0.3740\u001b[0m  0.0553\n",
            "     26        \u001b[36m0.3739\u001b[0m  0.0562\n",
            "     27        \u001b[36m0.3736\u001b[0m  0.0513\n",
            "     28        \u001b[36m0.3734\u001b[0m  0.0513\n",
            "     29        \u001b[36m0.3733\u001b[0m  0.0490\n",
            "     30        \u001b[36m0.3733\u001b[0m  0.0500\n",
            "     31        \u001b[36m0.3733\u001b[0m  0.0548\n",
            "     32        \u001b[36m0.3733\u001b[0m  0.0527\n",
            "     33        \u001b[36m0.3732\u001b[0m  0.0550\n",
            "     34        \u001b[36m0.3732\u001b[0m  0.0611\n",
            "     35        \u001b[36m0.3732\u001b[0m  0.0559\n",
            "     36        \u001b[36m0.3732\u001b[0m  0.0512\n",
            "     37        \u001b[36m0.3732\u001b[0m  0.0505\n",
            "     38        \u001b[36m0.3731\u001b[0m  0.0557\n",
            "     39        \u001b[36m0.3731\u001b[0m  0.0537\n",
            "     40        \u001b[36m0.3731\u001b[0m  0.0594\n",
            "     41        \u001b[36m0.3731\u001b[0m  0.0592\n",
            "     42        \u001b[36m0.3730\u001b[0m  0.0544\n",
            "     43        \u001b[36m0.3730\u001b[0m  0.0681\n",
            "     44        \u001b[36m0.3730\u001b[0m  0.0577\n",
            "     45        \u001b[36m0.3730\u001b[0m  0.0480\n",
            "     46        \u001b[36m0.3730\u001b[0m  0.0494\n",
            "     47        \u001b[36m0.3729\u001b[0m  0.0531\n",
            "     48        \u001b[36m0.3729\u001b[0m  0.0441\n",
            "     49        \u001b[36m0.3729\u001b[0m  0.0485\n",
            "     50        \u001b[36m0.3729\u001b[0m  0.0498\n",
            "     51        \u001b[36m0.3729\u001b[0m  0.0536\n",
            "     52        \u001b[36m0.3728\u001b[0m  0.0508\n",
            "     53        \u001b[36m0.3728\u001b[0m  0.0492\n",
            "     54        \u001b[36m0.3728\u001b[0m  0.0505\n",
            "     55        \u001b[36m0.3728\u001b[0m  0.0505\n",
            "     56        \u001b[36m0.3728\u001b[0m  0.0542\n",
            "     57        \u001b[36m0.3728\u001b[0m  0.0576\n",
            "     58        \u001b[36m0.3727\u001b[0m  0.0527\n",
            "     59        \u001b[36m0.3727\u001b[0m  0.0564\n",
            "     60        \u001b[36m0.3727\u001b[0m  0.0587\n",
            "     61        \u001b[36m0.3727\u001b[0m  0.0565\n",
            "     62        \u001b[36m0.3727\u001b[0m  0.0459\n",
            "     63        \u001b[36m0.3727\u001b[0m  0.0492\n",
            "     64        \u001b[36m0.3727\u001b[0m  0.0527\n",
            "     65        \u001b[36m0.3727\u001b[0m  0.0550\n",
            "     66        \u001b[36m0.3726\u001b[0m  0.0470\n",
            "     67        \u001b[36m0.3726\u001b[0m  0.0489\n",
            "     68        \u001b[36m0.3726\u001b[0m  0.0487\n",
            "     69        \u001b[36m0.3726\u001b[0m  0.0549\n",
            "     70        \u001b[36m0.3726\u001b[0m  0.0510\n",
            "     71        \u001b[36m0.3726\u001b[0m  0.0506\n",
            "     72        \u001b[36m0.3726\u001b[0m  0.0559\n",
            "     73        \u001b[36m0.3726\u001b[0m  0.0525\n",
            "     74        \u001b[36m0.3726\u001b[0m  0.0582\n",
            "     75        \u001b[36m0.3725\u001b[0m  0.0561\n",
            "     76        \u001b[36m0.3725\u001b[0m  0.0569\n",
            "     77        \u001b[36m0.3725\u001b[0m  0.0495\n",
            "     78        \u001b[36m0.3725\u001b[0m  0.0465\n",
            "     79        \u001b[36m0.3725\u001b[0m  0.0500\n",
            "     80        \u001b[36m0.3725\u001b[0m  0.0619\n",
            "     81        \u001b[36m0.3725\u001b[0m  0.0554\n",
            "     82        \u001b[36m0.3725\u001b[0m  0.0533\n",
            "     83        \u001b[36m0.3725\u001b[0m  0.0493\n",
            "     84        \u001b[36m0.3725\u001b[0m  0.0464\n",
            "     85        \u001b[36m0.3725\u001b[0m  0.0510\n",
            "     86        \u001b[36m0.3724\u001b[0m  0.0494\n",
            "     87        \u001b[36m0.3724\u001b[0m  0.0571\n",
            "     88        \u001b[36m0.3724\u001b[0m  0.0643\n",
            "     89        \u001b[36m0.3724\u001b[0m  0.0543\n",
            "     90        \u001b[36m0.3724\u001b[0m  0.0533\n",
            "     91        \u001b[36m0.3724\u001b[0m  0.0570\n",
            "     92        \u001b[36m0.3724\u001b[0m  0.0554\n",
            "     93        \u001b[36m0.3724\u001b[0m  0.0555\n",
            "     94        \u001b[36m0.3724\u001b[0m  0.0494\n",
            "     95        \u001b[36m0.3724\u001b[0m  0.0540\n",
            "     96        \u001b[36m0.3724\u001b[0m  0.0519\n",
            "     97        \u001b[36m0.3724\u001b[0m  0.0563\n",
            "     98        \u001b[36m0.3724\u001b[0m  0.0601\n",
            "     99        \u001b[36m0.3724\u001b[0m  0.0499\n",
            "    100        \u001b[36m0.3723\u001b[0m  0.0503\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9999\u001b[0m  0.0322\n",
            "      2        0.9999  0.0476\n",
            "      3        0.9999  0.0400\n",
            "      4        0.9999  0.0452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        0.9999  0.0477\n",
            "      6        0.9999  0.0360\n",
            "      7        0.9999  0.0415\n",
            "      8        0.9999  0.0446\n",
            "      9        0.9999  0.0419\n",
            "     10        0.9999  0.0375\n",
            "     11        0.9999  0.0449\n",
            "     12        \u001b[36m0.9999\u001b[0m  0.0419\n",
            "     13        \u001b[36m0.9999\u001b[0m  0.0407\n",
            "     14        0.9999  0.0402\n",
            "     15        0.9999  0.0408\n",
            "     16        0.9999  0.0387\n",
            "     17        0.9999  0.0384\n",
            "     18        0.9999  0.0393\n",
            "     19        0.9999  0.0398\n",
            "     20        0.9999  0.0476\n",
            "     21        0.9999  0.0360\n",
            "     22        0.9999  0.0418\n",
            "     23        0.9999  0.0525\n",
            "     24        0.9999  0.0423\n",
            "     25        \u001b[36m0.9999\u001b[0m  0.0429\n",
            "     26        \u001b[36m0.9999\u001b[0m  0.0385\n",
            "     27        0.9999  0.0414\n",
            "     28        0.9999  0.0369\n",
            "     29        0.9999  0.0379\n",
            "     30        0.9999  0.0398\n",
            "     31        0.9999  0.0376\n",
            "     32        0.9999  0.0380\n",
            "     33        0.9999  0.0404\n",
            "     34        0.9999  0.0429\n",
            "     35        0.9999  0.0434\n",
            "     36        0.9999  0.0430\n",
            "     37        0.9999  0.0430\n",
            "     38        \u001b[36m0.9999\u001b[0m  0.0383\n",
            "     39        \u001b[36m0.9999\u001b[0m  0.0412\n",
            "     40        0.9999  0.0386\n",
            "     41        0.9999  0.0404\n",
            "     42        0.9999  0.0434\n",
            "     43        0.9999  0.0453\n",
            "     44        0.9999  0.0415\n",
            "     45        0.9999  0.0466\n",
            "     46        0.9999  0.0364\n",
            "     47        0.9999  0.0421\n",
            "     48        0.9999  0.0421\n",
            "     49        0.9999  0.0379\n",
            "     50        0.9999  0.0405\n",
            "     51        0.9999  0.0401\n",
            "     52        \u001b[36m0.9999\u001b[0m  0.0375\n",
            "     53        \u001b[36m0.9999\u001b[0m  0.0368\n",
            "     54        0.9999  0.0399\n",
            "     55        0.9999  0.0471\n",
            "     56        0.9999  0.0448\n",
            "     57        0.9999  0.0464\n",
            "     58        0.9999  0.0444\n",
            "     59        0.9999  0.0393\n",
            "     60        0.9999  0.0388\n",
            "     61        0.9999  0.0421\n",
            "     62        0.9999  0.0420\n",
            "     63        0.9999  0.0387\n",
            "     64        0.9999  0.0473\n",
            "     65        \u001b[36m0.9999\u001b[0m  0.0468\n",
            "     66        \u001b[36m0.9999\u001b[0m  0.0516\n",
            "     67        0.9999  0.0390\n",
            "     68        0.9999  0.0362\n",
            "     69        0.9999  0.0416\n",
            "     70        0.9999  0.0388\n",
            "     71        0.9999  0.0394\n",
            "     72        0.9999  0.0403\n",
            "     73        0.9999  0.0469\n",
            "     74        0.9999  0.0395\n",
            "     75        0.9999  0.0397\n",
            "     76        0.9999  0.0475\n",
            "     77        0.9999  0.0418\n",
            "     78        \u001b[36m0.9999\u001b[0m  0.0437\n",
            "     79        \u001b[36m0.9999\u001b[0m  0.0500\n",
            "     80        0.9999  0.0425\n",
            "     81        0.9999  0.0394\n",
            "     82        0.9999  0.0410\n",
            "     83        0.9999  0.0387\n",
            "     84        0.9999  0.0449\n",
            "     85        0.9999  0.0396\n",
            "     86        0.9999  0.0386\n",
            "     87        0.9999  0.0372\n",
            "     88        0.9999  0.0473\n",
            "     89        0.9999  0.0475\n",
            "     90        0.9999  0.0411\n",
            "     91        \u001b[36m0.9999\u001b[0m  0.0391\n",
            "     92        \u001b[36m0.9999\u001b[0m  0.0390\n",
            "     93        0.9999  0.0401\n",
            "     94        0.9999  0.0401\n",
            "     95        0.9999  0.0458\n",
            "     96        0.9999  0.0396\n",
            "     97        0.9999  0.0420\n",
            "     98        0.9999  0.0459\n",
            "     99        0.9999  0.0470\n",
            "    100        0.9999  0.0439\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9992\u001b[0m  0.0381\n",
            "      2        \u001b[36m0.9992\u001b[0m  0.0431\n",
            "      3        \u001b[36m0.9992\u001b[0m  0.0465\n",
            "      4        \u001b[36m0.9992\u001b[0m  0.0489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.9992\u001b[0m  0.0505\n",
            "      6        \u001b[36m0.9992\u001b[0m  0.0389\n",
            "      7        \u001b[36m0.9992\u001b[0m  0.0415\n",
            "      8        \u001b[36m0.9992\u001b[0m  0.0363\n",
            "      9        \u001b[36m0.9992\u001b[0m  0.0410\n",
            "     10        \u001b[36m0.9992\u001b[0m  0.0449\n",
            "     11        \u001b[36m0.9992\u001b[0m  0.0565\n",
            "     12        \u001b[36m0.9992\u001b[0m  0.0427\n",
            "     13        \u001b[36m0.9992\u001b[0m  0.0387\n",
            "     14        \u001b[36m0.9992\u001b[0m  0.0391\n",
            "     15        \u001b[36m0.9992\u001b[0m  0.0397\n",
            "     16        \u001b[36m0.9992\u001b[0m  0.0455\n",
            "     17        \u001b[36m0.9992\u001b[0m  0.0405\n",
            "     18        \u001b[36m0.9992\u001b[0m  0.0424\n",
            "     19        \u001b[36m0.9992\u001b[0m  0.0439\n",
            "     20        \u001b[36m0.9992\u001b[0m  0.0412\n",
            "     21        \u001b[36m0.9992\u001b[0m  0.0444\n",
            "     22        \u001b[36m0.9992\u001b[0m  0.0443\n",
            "     23        \u001b[36m0.9992\u001b[0m  0.0490\n",
            "     24        \u001b[36m0.9992\u001b[0m  0.0398\n",
            "     25        \u001b[36m0.9992\u001b[0m  0.0472\n",
            "     26        \u001b[36m0.9992\u001b[0m  0.0419\n",
            "     27        \u001b[36m0.9992\u001b[0m  0.0410\n",
            "     28        \u001b[36m0.9992\u001b[0m  0.0384\n",
            "     29        \u001b[36m0.9992\u001b[0m  0.0400\n",
            "     30        \u001b[36m0.9992\u001b[0m  0.0484\n",
            "     31        \u001b[36m0.9992\u001b[0m  0.0392\n",
            "     32        \u001b[36m0.9992\u001b[0m  0.0423\n",
            "     33        \u001b[36m0.9992\u001b[0m  0.0410\n",
            "     34        \u001b[36m0.9992\u001b[0m  0.0402\n",
            "     35        \u001b[36m0.9992\u001b[0m  0.0415\n",
            "     36        \u001b[36m0.9992\u001b[0m  0.0378\n",
            "     37        \u001b[36m0.9992\u001b[0m  0.0413\n",
            "     38        \u001b[36m0.9992\u001b[0m  0.0432\n",
            "     39        \u001b[36m0.9992\u001b[0m  0.0422\n",
            "     40        \u001b[36m0.9992\u001b[0m  0.0387\n",
            "     41        \u001b[36m0.9992\u001b[0m  0.0398\n",
            "     42        \u001b[36m0.9992\u001b[0m  0.0454\n",
            "     43        \u001b[36m0.9992\u001b[0m  0.0482\n",
            "     44        \u001b[36m0.9992\u001b[0m  0.0472\n",
            "     45        \u001b[36m0.9992\u001b[0m  0.0445\n",
            "     46        \u001b[36m0.9992\u001b[0m  0.0461\n",
            "     47        \u001b[36m0.9992\u001b[0m  0.0517\n",
            "     48        \u001b[36m0.9992\u001b[0m  0.0471\n",
            "     49        \u001b[36m0.9992\u001b[0m  0.0467\n",
            "     50        \u001b[36m0.9992\u001b[0m  0.0419\n",
            "     51        \u001b[36m0.9992\u001b[0m  0.0399\n",
            "     52        \u001b[36m0.9992\u001b[0m  0.0372\n",
            "     53        \u001b[36m0.9992\u001b[0m  0.0420\n",
            "     54        \u001b[36m0.9992\u001b[0m  0.0387\n",
            "     55        \u001b[36m0.9992\u001b[0m  0.0393\n",
            "     56        \u001b[36m0.9992\u001b[0m  0.0488\n",
            "     57        \u001b[36m0.9992\u001b[0m  0.0418\n",
            "     58        \u001b[36m0.9992\u001b[0m  0.0379\n",
            "     59        \u001b[36m0.9992\u001b[0m  0.0393\n",
            "     60        \u001b[36m0.9992\u001b[0m  0.0394\n",
            "     61        \u001b[36m0.9992\u001b[0m  0.0394\n",
            "     62        \u001b[36m0.9992\u001b[0m  0.0407\n",
            "     63        \u001b[36m0.9992\u001b[0m  0.0410\n",
            "     64        \u001b[36m0.9992\u001b[0m  0.0439\n",
            "     65        \u001b[36m0.9992\u001b[0m  0.0437\n",
            "     66        \u001b[36m0.9992\u001b[0m  0.0483\n",
            "     67        \u001b[36m0.9992\u001b[0m  0.0446\n",
            "     68        \u001b[36m0.9992\u001b[0m  0.0411\n",
            "     69        \u001b[36m0.9992\u001b[0m  0.0381\n",
            "     70        \u001b[36m0.9992\u001b[0m  0.0450\n",
            "     71        \u001b[36m0.9992\u001b[0m  0.0484\n",
            "     72        \u001b[36m0.9992\u001b[0m  0.0433\n",
            "     73        \u001b[36m0.9992\u001b[0m  0.0440\n",
            "     74        \u001b[36m0.9992\u001b[0m  0.0366\n",
            "     75        \u001b[36m0.9992\u001b[0m  0.0399\n",
            "     76        \u001b[36m0.9992\u001b[0m  0.0486\n",
            "     77        \u001b[36m0.9992\u001b[0m  0.0349\n",
            "     78        \u001b[36m0.9992\u001b[0m  0.0445\n",
            "     79        \u001b[36m0.9992\u001b[0m  0.0375\n",
            "     80        \u001b[36m0.9992\u001b[0m  0.0393\n",
            "     81        \u001b[36m0.9992\u001b[0m  0.0394\n",
            "     82        \u001b[36m0.9992\u001b[0m  0.0446\n",
            "     83        \u001b[36m0.9992\u001b[0m  0.0409\n",
            "     84        \u001b[36m0.9992\u001b[0m  0.0424\n",
            "     85        \u001b[36m0.9992\u001b[0m  0.0516\n",
            "     86        \u001b[36m0.9992\u001b[0m  0.0426\n",
            "     87        \u001b[36m0.9992\u001b[0m  0.0459\n",
            "     88        \u001b[36m0.9992\u001b[0m  0.0397\n",
            "     89        \u001b[36m0.9992\u001b[0m  0.0459\n",
            "     90        \u001b[36m0.9992\u001b[0m  0.0415\n",
            "     91        \u001b[36m0.9992\u001b[0m  0.0401\n",
            "     92        \u001b[36m0.9992\u001b[0m  0.0405\n",
            "     93        \u001b[36m0.9992\u001b[0m  0.0445\n",
            "     94        \u001b[36m0.9992\u001b[0m  0.0425\n",
            "     95        \u001b[36m0.9992\u001b[0m  0.0407\n",
            "     96        \u001b[36m0.9992\u001b[0m  0.0443\n",
            "     97        \u001b[36m0.9992\u001b[0m  0.0433\n",
            "     98        \u001b[36m0.9992\u001b[0m  0.0379\n",
            "     99        \u001b[36m0.9992\u001b[0m  0.0395\n",
            "    100        \u001b[36m0.9992\u001b[0m  0.0402\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7114\u001b[0m  0.0526\n",
            "      2        \u001b[36m0.6378\u001b[0m  0.0496\n",
            "      3        \u001b[36m0.5826\u001b[0m  0.0501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.5461\u001b[0m  0.0607\n",
            "      5        \u001b[36m0.5189\u001b[0m  0.0573\n",
            "      6        \u001b[36m0.5001\u001b[0m  0.0577\n",
            "      7        \u001b[36m0.4843\u001b[0m  0.0548\n",
            "      8        \u001b[36m0.4722\u001b[0m  0.0593\n",
            "      9        \u001b[36m0.4635\u001b[0m  0.0578\n",
            "     10        \u001b[36m0.4560\u001b[0m  0.0506\n",
            "     11        \u001b[36m0.4476\u001b[0m  0.0568\n",
            "     12        \u001b[36m0.4408\u001b[0m  0.0578\n",
            "     13        \u001b[36m0.4370\u001b[0m  0.0465\n",
            "     14        \u001b[36m0.4289\u001b[0m  0.0495\n",
            "     15        \u001b[36m0.4243\u001b[0m  0.0506\n",
            "     16        \u001b[36m0.4208\u001b[0m  0.0474\n",
            "     17        \u001b[36m0.4178\u001b[0m  0.0506\n",
            "     18        \u001b[36m0.4138\u001b[0m  0.0534\n",
            "     19        \u001b[36m0.4116\u001b[0m  0.0595\n",
            "     20        \u001b[36m0.4079\u001b[0m  0.0545\n",
            "     21        \u001b[36m0.4061\u001b[0m  0.0532\n",
            "     22        \u001b[36m0.4046\u001b[0m  0.0530\n",
            "     23        \u001b[36m0.4031\u001b[0m  0.0520\n",
            "     24        \u001b[36m0.4018\u001b[0m  0.0548\n",
            "     25        \u001b[36m0.4005\u001b[0m  0.0574\n",
            "     26        \u001b[36m0.3993\u001b[0m  0.0538\n",
            "     27        \u001b[36m0.3981\u001b[0m  0.0552\n",
            "     28        \u001b[36m0.3972\u001b[0m  0.0514\n",
            "     29        \u001b[36m0.3962\u001b[0m  0.0484\n",
            "     30        \u001b[36m0.3954\u001b[0m  0.0540\n",
            "     31        \u001b[36m0.3946\u001b[0m  0.0504\n",
            "     32        \u001b[36m0.3938\u001b[0m  0.0491\n",
            "     33        \u001b[36m0.3917\u001b[0m  0.0519\n",
            "     34        \u001b[36m0.3911\u001b[0m  0.0558\n",
            "     35        \u001b[36m0.3905\u001b[0m  0.0477\n",
            "     36        \u001b[36m0.3899\u001b[0m  0.0483\n",
            "     37        \u001b[36m0.3893\u001b[0m  0.0535\n",
            "     38        \u001b[36m0.3888\u001b[0m  0.0543\n",
            "     39        \u001b[36m0.3883\u001b[0m  0.0538\n",
            "     40        \u001b[36m0.3878\u001b[0m  0.0458\n",
            "     41        \u001b[36m0.3873\u001b[0m  0.0521\n",
            "     42        \u001b[36m0.3869\u001b[0m  0.0549\n",
            "     43        \u001b[36m0.3865\u001b[0m  0.0565\n",
            "     44        \u001b[36m0.3861\u001b[0m  0.0526\n",
            "     45        \u001b[36m0.3857\u001b[0m  0.0560\n",
            "     46        \u001b[36m0.3853\u001b[0m  0.0517\n",
            "     47        \u001b[36m0.3849\u001b[0m  0.0472\n",
            "     48        \u001b[36m0.3846\u001b[0m  0.0508\n",
            "     49        \u001b[36m0.3842\u001b[0m  0.0493\n",
            "     50        \u001b[36m0.3839\u001b[0m  0.0475\n",
            "     51        \u001b[36m0.3836\u001b[0m  0.0493\n",
            "     52        \u001b[36m0.3832\u001b[0m  0.0557\n",
            "     53        \u001b[36m0.3829\u001b[0m  0.0486\n",
            "     54        \u001b[36m0.3826\u001b[0m  0.0483\n",
            "     55        \u001b[36m0.3823\u001b[0m  0.0489\n",
            "     56        \u001b[36m0.3819\u001b[0m  0.0529\n",
            "     57        \u001b[36m0.3816\u001b[0m  0.0582\n",
            "     58        \u001b[36m0.3813\u001b[0m  0.0516\n",
            "     59        \u001b[36m0.3810\u001b[0m  0.0548\n",
            "     60        \u001b[36m0.3808\u001b[0m  0.0562\n",
            "     61        \u001b[36m0.3805\u001b[0m  0.0561\n",
            "     62        \u001b[36m0.3802\u001b[0m  0.0562\n",
            "     63        \u001b[36m0.3799\u001b[0m  0.0588\n",
            "     64        \u001b[36m0.3799\u001b[0m  0.0533\n",
            "     65        \u001b[36m0.3796\u001b[0m  0.0561\n",
            "     66        \u001b[36m0.3793\u001b[0m  0.0516\n",
            "     67        \u001b[36m0.3791\u001b[0m  0.0550\n",
            "     68        \u001b[36m0.3788\u001b[0m  0.0525\n",
            "     69        \u001b[36m0.3786\u001b[0m  0.0500\n",
            "     70        \u001b[36m0.3784\u001b[0m  0.0499\n",
            "     71        \u001b[36m0.3781\u001b[0m  0.0522\n",
            "     72        \u001b[36m0.3779\u001b[0m  0.0509\n",
            "     73        \u001b[36m0.3777\u001b[0m  0.0501\n",
            "     74        \u001b[36m0.3774\u001b[0m  0.0534\n",
            "     75        \u001b[36m0.3769\u001b[0m  0.0610\n",
            "     76        \u001b[36m0.3767\u001b[0m  0.0515\n",
            "     77        \u001b[36m0.3766\u001b[0m  0.0552\n",
            "     78        \u001b[36m0.3765\u001b[0m  0.0659\n",
            "     79        \u001b[36m0.3763\u001b[0m  0.0642\n",
            "     80        \u001b[36m0.3762\u001b[0m  0.0592\n",
            "     81        \u001b[36m0.3761\u001b[0m  0.0521\n",
            "     82        \u001b[36m0.3760\u001b[0m  0.0489\n",
            "     83        \u001b[36m0.3759\u001b[0m  0.0504\n",
            "     84        \u001b[36m0.3758\u001b[0m  0.0487\n",
            "     85        \u001b[36m0.3757\u001b[0m  0.0500\n",
            "     86        \u001b[36m0.3757\u001b[0m  0.0499\n",
            "     87        \u001b[36m0.3756\u001b[0m  0.0498\n",
            "     88        \u001b[36m0.3755\u001b[0m  0.0544\n",
            "     89        \u001b[36m0.3754\u001b[0m  0.0519\n",
            "     90        \u001b[36m0.3754\u001b[0m  0.0538\n",
            "     91        \u001b[36m0.3753\u001b[0m  0.0483\n",
            "     92        \u001b[36m0.3753\u001b[0m  0.0560\n",
            "     93        \u001b[36m0.3752\u001b[0m  0.0653\n",
            "     94        \u001b[36m0.3751\u001b[0m  0.0574\n",
            "     95        \u001b[36m0.3751\u001b[0m  0.0592\n",
            "     96        \u001b[36m0.3750\u001b[0m  0.0567\n",
            "     97        \u001b[36m0.3750\u001b[0m  0.0596\n",
            "     98        \u001b[36m0.3749\u001b[0m  0.0619\n",
            "     99        \u001b[36m0.3749\u001b[0m  0.0603\n",
            "    100        \u001b[36m0.3749\u001b[0m  0.0605\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6082\u001b[0m  0.0434\n",
            "      2        \u001b[36m0.5395\u001b[0m  0.0500\n",
            "      3        \u001b[36m0.4895\u001b[0m  0.0525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.4565\u001b[0m  0.0588\n",
            "      5        \u001b[36m0.4372\u001b[0m  0.0515\n",
            "      6        \u001b[36m0.4259\u001b[0m  0.0494\n",
            "      7        \u001b[36m0.4175\u001b[0m  0.0507\n",
            "      8        \u001b[36m0.4113\u001b[0m  0.0503\n",
            "      9        \u001b[36m0.4065\u001b[0m  0.0540\n",
            "     10        \u001b[36m0.4026\u001b[0m  0.0620\n",
            "     11        \u001b[36m0.3970\u001b[0m  0.0503\n",
            "     12        \u001b[36m0.3946\u001b[0m  0.0580\n",
            "     13        \u001b[36m0.3927\u001b[0m  0.0538\n",
            "     14        \u001b[36m0.3909\u001b[0m  0.0567\n",
            "     15        \u001b[36m0.3895\u001b[0m  0.0575\n",
            "     16        \u001b[36m0.3881\u001b[0m  0.0505\n",
            "     17        \u001b[36m0.3870\u001b[0m  0.0496\n",
            "     18        \u001b[36m0.3860\u001b[0m  0.0497\n",
            "     19        \u001b[36m0.3851\u001b[0m  0.0506\n",
            "     20        \u001b[36m0.3844\u001b[0m  0.0582\n",
            "     21        \u001b[36m0.3834\u001b[0m  0.0486\n",
            "     22        \u001b[36m0.3827\u001b[0m  0.0505\n",
            "     23        \u001b[36m0.3821\u001b[0m  0.0530\n",
            "     24        \u001b[36m0.3816\u001b[0m  0.0489\n",
            "     25        \u001b[36m0.3811\u001b[0m  0.0498\n",
            "     26        \u001b[36m0.3807\u001b[0m  0.0480\n",
            "     27        \u001b[36m0.3803\u001b[0m  0.0551\n",
            "     28        \u001b[36m0.3797\u001b[0m  0.0558\n",
            "     29        \u001b[36m0.3791\u001b[0m  0.0643\n",
            "     30        \u001b[36m0.3790\u001b[0m  0.0600\n",
            "     31        \u001b[36m0.3786\u001b[0m  0.0551\n",
            "     32        \u001b[36m0.3783\u001b[0m  0.0539\n",
            "     33        \u001b[36m0.3780\u001b[0m  0.0577\n",
            "     34        \u001b[36m0.3778\u001b[0m  0.0612\n",
            "     35        \u001b[36m0.3776\u001b[0m  0.0483\n",
            "     36        \u001b[36m0.3773\u001b[0m  0.0501\n",
            "     37        \u001b[36m0.3771\u001b[0m  0.0565\n",
            "     38        \u001b[36m0.3769\u001b[0m  0.0615\n",
            "     39        \u001b[36m0.3767\u001b[0m  0.0610\n",
            "     40        \u001b[36m0.3765\u001b[0m  0.0474\n",
            "     41        \u001b[36m0.3763\u001b[0m  0.0510\n",
            "     42        \u001b[36m0.3762\u001b[0m  0.0507\n",
            "     43        \u001b[36m0.3760\u001b[0m  0.0511\n",
            "     44        0.3770  0.0502\n",
            "     45        \u001b[36m0.3757\u001b[0m  0.0529\n",
            "     46        \u001b[36m0.3756\u001b[0m  0.0651\n",
            "     47        \u001b[36m0.3754\u001b[0m  0.0507\n",
            "     48        \u001b[36m0.3754\u001b[0m  0.0635\n",
            "     49        \u001b[36m0.3752\u001b[0m  0.0617\n",
            "     50        \u001b[36m0.3751\u001b[0m  0.0591\n",
            "     51        \u001b[36m0.3750\u001b[0m  0.0547\n",
            "     52        \u001b[36m0.3749\u001b[0m  0.0550\n",
            "     53        \u001b[36m0.3748\u001b[0m  0.0504\n",
            "     54        \u001b[36m0.3747\u001b[0m  0.0517\n",
            "     55        \u001b[36m0.3746\u001b[0m  0.0536\n",
            "     56        \u001b[36m0.3746\u001b[0m  0.0551\n",
            "     57        \u001b[36m0.3745\u001b[0m  0.0497\n",
            "     58        \u001b[36m0.3744\u001b[0m  0.0481\n",
            "     59        \u001b[36m0.3743\u001b[0m  0.0524\n",
            "     60        \u001b[36m0.3742\u001b[0m  0.0509\n",
            "     61        \u001b[36m0.3742\u001b[0m  0.0488\n",
            "     62        \u001b[36m0.3741\u001b[0m  0.0553\n",
            "     63        \u001b[36m0.3741\u001b[0m  0.0498\n",
            "     64        \u001b[36m0.3740\u001b[0m  0.0693\n",
            "     65        \u001b[36m0.3739\u001b[0m  0.0642\n",
            "     66        \u001b[36m0.3739\u001b[0m  0.0580\n",
            "     67        \u001b[36m0.3738\u001b[0m  0.0649\n",
            "     68        \u001b[36m0.3738\u001b[0m  0.0593\n",
            "     69        \u001b[36m0.3737\u001b[0m  0.0706\n",
            "     70        \u001b[36m0.3737\u001b[0m  0.0510\n",
            "     71        \u001b[36m0.3736\u001b[0m  0.0484\n",
            "     72        \u001b[36m0.3736\u001b[0m  0.0545\n",
            "     73        \u001b[36m0.3735\u001b[0m  0.0507\n",
            "     74        \u001b[36m0.3734\u001b[0m  0.0509\n",
            "     75        \u001b[36m0.3734\u001b[0m  0.0510\n",
            "     76        \u001b[36m0.3733\u001b[0m  0.0558\n",
            "     77        \u001b[36m0.3733\u001b[0m  0.0490\n",
            "     78        \u001b[36m0.3732\u001b[0m  0.0486\n",
            "     79        \u001b[36m0.3732\u001b[0m  0.0538\n",
            "     80        \u001b[36m0.3732\u001b[0m  0.0547\n",
            "     81        \u001b[36m0.3731\u001b[0m  0.0692\n",
            "     82        \u001b[36m0.3731\u001b[0m  0.0538\n",
            "     83        \u001b[36m0.3731\u001b[0m  0.0649\n",
            "     84        \u001b[36m0.3731\u001b[0m  0.0621\n",
            "     85        \u001b[36m0.3730\u001b[0m  0.0528\n",
            "     86        \u001b[36m0.3730\u001b[0m  0.0543\n",
            "     87        \u001b[36m0.3730\u001b[0m  0.0571\n",
            "     88        \u001b[36m0.3730\u001b[0m  0.0508\n",
            "     89        \u001b[36m0.3729\u001b[0m  0.0564\n",
            "     90        \u001b[36m0.3729\u001b[0m  0.0507\n",
            "     91        \u001b[36m0.3729\u001b[0m  0.0505\n",
            "     92        \u001b[36m0.3729\u001b[0m  0.0508\n",
            "     93        0.3730  0.0499\n",
            "     94        0.3730  0.0498\n",
            "     95        0.3729  0.0541\n",
            "     96        0.3729  0.0500\n",
            "     97        0.3729  0.0659\n",
            "     98        \u001b[36m0.3729\u001b[0m  0.0642\n",
            "     99        \u001b[36m0.3728\u001b[0m  0.0745\n",
            "    100        \u001b[36m0.3728\u001b[0m  0.0672\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6782\u001b[0m  0.0389\n",
            "      2        \u001b[36m0.6755\u001b[0m  0.0473\n",
            "      3        \u001b[36m0.6728\u001b[0m  0.0407\n",
            "      4        \u001b[36m0.6701\u001b[0m  0.0533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.6674\u001b[0m  0.0503\n",
            "      6        \u001b[36m0.6647\u001b[0m  0.0382\n",
            "      7        \u001b[36m0.6619\u001b[0m  0.0368\n",
            "      8        \u001b[36m0.6592\u001b[0m  0.0504\n",
            "      9        \u001b[36m0.6564\u001b[0m  0.0399\n",
            "     10        \u001b[36m0.6536\u001b[0m  0.0399\n",
            "     11        \u001b[36m0.6509\u001b[0m  0.0436\n",
            "     12        \u001b[36m0.6481\u001b[0m  0.0416\n",
            "     13        \u001b[36m0.6453\u001b[0m  0.0376\n",
            "     14        \u001b[36m0.6425\u001b[0m  0.0390\n",
            "     15        \u001b[36m0.6398\u001b[0m  0.0490\n",
            "     16        \u001b[36m0.6370\u001b[0m  0.0389\n",
            "     17        \u001b[36m0.6342\u001b[0m  0.0389\n",
            "     18        \u001b[36m0.6315\u001b[0m  0.0451\n",
            "     19        \u001b[36m0.6288\u001b[0m  0.0388\n",
            "     20        \u001b[36m0.6260\u001b[0m  0.0482\n",
            "     21        \u001b[36m0.6233\u001b[0m  0.0428\n",
            "     22        \u001b[36m0.6206\u001b[0m  0.0431\n",
            "     23        \u001b[36m0.6180\u001b[0m  0.0423\n",
            "     24        \u001b[36m0.6153\u001b[0m  0.0404\n",
            "     25        \u001b[36m0.6127\u001b[0m  0.0416\n",
            "     26        \u001b[36m0.6101\u001b[0m  0.0428\n",
            "     27        \u001b[36m0.6075\u001b[0m  0.0381\n",
            "     28        \u001b[36m0.6049\u001b[0m  0.0380\n",
            "     29        \u001b[36m0.6024\u001b[0m  0.0473\n",
            "     30        \u001b[36m0.5999\u001b[0m  0.0387\n",
            "     31        \u001b[36m0.5974\u001b[0m  0.0474\n",
            "     32        \u001b[36m0.5950\u001b[0m  0.0429\n",
            "     33        \u001b[36m0.5925\u001b[0m  0.0449\n",
            "     34        \u001b[36m0.5902\u001b[0m  0.0386\n",
            "     35        \u001b[36m0.5878\u001b[0m  0.0394\n",
            "     36        \u001b[36m0.5855\u001b[0m  0.0372\n",
            "     37        \u001b[36m0.5832\u001b[0m  0.0425\n",
            "     38        \u001b[36m0.5810\u001b[0m  0.0400\n",
            "     39        \u001b[36m0.5788\u001b[0m  0.0376\n",
            "     40        \u001b[36m0.5766\u001b[0m  0.0412\n",
            "     41        \u001b[36m0.5745\u001b[0m  0.0499\n",
            "     42        \u001b[36m0.5724\u001b[0m  0.0413\n",
            "     43        \u001b[36m0.5703\u001b[0m  0.0562\n",
            "     44        \u001b[36m0.5683\u001b[0m  0.0424\n",
            "     45        \u001b[36m0.5663\u001b[0m  0.0409\n",
            "     46        \u001b[36m0.5643\u001b[0m  0.0464\n",
            "     47        \u001b[36m0.5613\u001b[0m  0.0452\n",
            "     48        \u001b[36m0.5590\u001b[0m  0.0440\n",
            "     49        \u001b[36m0.5569\u001b[0m  0.0474\n",
            "     50        \u001b[36m0.5551\u001b[0m  0.0413\n",
            "     51        \u001b[36m0.5534\u001b[0m  0.0408\n",
            "     52        \u001b[36m0.5517\u001b[0m  0.0432\n",
            "     53        \u001b[36m0.5500\u001b[0m  0.0399\n",
            "     54        \u001b[36m0.5484\u001b[0m  0.0408\n",
            "     55        \u001b[36m0.5468\u001b[0m  0.0432\n",
            "     56        \u001b[36m0.5452\u001b[0m  0.0399\n",
            "     57        \u001b[36m0.5436\u001b[0m  0.0411\n",
            "     58        \u001b[36m0.5421\u001b[0m  0.0395\n",
            "     59        \u001b[36m0.5406\u001b[0m  0.0460\n",
            "     60        \u001b[36m0.5392\u001b[0m  0.0401\n",
            "     61        \u001b[36m0.5378\u001b[0m  0.0383\n",
            "     62        \u001b[36m0.5364\u001b[0m  0.0391\n",
            "     63        \u001b[36m0.5350\u001b[0m  0.0391\n",
            "     64        \u001b[36m0.5336\u001b[0m  0.0430\n",
            "     65        \u001b[36m0.5323\u001b[0m  0.0420\n",
            "     66        \u001b[36m0.5310\u001b[0m  0.0460\n",
            "     67        \u001b[36m0.5298\u001b[0m  0.0447\n",
            "     68        \u001b[36m0.5285\u001b[0m  0.0424\n",
            "     69        \u001b[36m0.5273\u001b[0m  0.0490\n",
            "     70        \u001b[36m0.5261\u001b[0m  0.0335\n",
            "     71        \u001b[36m0.5250\u001b[0m  0.0361\n",
            "     72        \u001b[36m0.5238\u001b[0m  0.0397\n",
            "     73        \u001b[36m0.5227\u001b[0m  0.0392\n",
            "     74        \u001b[36m0.5216\u001b[0m  0.0436\n",
            "     75        \u001b[36m0.5205\u001b[0m  0.0435\n",
            "     76        \u001b[36m0.5194\u001b[0m  0.0442\n",
            "     77        \u001b[36m0.5184\u001b[0m  0.0375\n",
            "     78        \u001b[36m0.5174\u001b[0m  0.0410\n",
            "     79        \u001b[36m0.5164\u001b[0m  0.0450\n",
            "     80        \u001b[36m0.5154\u001b[0m  0.0404\n",
            "     81        \u001b[36m0.5144\u001b[0m  0.0422\n",
            "     82        \u001b[36m0.5135\u001b[0m  0.0420\n",
            "     83        \u001b[36m0.5126\u001b[0m  0.0384\n",
            "     84        \u001b[36m0.5117\u001b[0m  0.0415\n",
            "     85        \u001b[36m0.5108\u001b[0m  0.0492\n",
            "     86        \u001b[36m0.5099\u001b[0m  0.0425\n",
            "     87        \u001b[36m0.5090\u001b[0m  0.0434\n",
            "     88        \u001b[36m0.5082\u001b[0m  0.0656\n",
            "     89        \u001b[36m0.5073\u001b[0m  0.0503\n",
            "     90        \u001b[36m0.5065\u001b[0m  0.0433\n",
            "     91        \u001b[36m0.5057\u001b[0m  0.0398\n",
            "     92        \u001b[36m0.5049\u001b[0m  0.0413\n",
            "     93        \u001b[36m0.5042\u001b[0m  0.0364\n",
            "     94        \u001b[36m0.5034\u001b[0m  0.0434\n",
            "     95        \u001b[36m0.5026\u001b[0m  0.0456\n",
            "     96        \u001b[36m0.5019\u001b[0m  0.0391\n",
            "     97        \u001b[36m0.5012\u001b[0m  0.0396\n",
            "     98        \u001b[36m0.5005\u001b[0m  0.0403\n",
            "     99        \u001b[36m0.4998\u001b[0m  0.0397\n",
            "    100        \u001b[36m0.4991\u001b[0m  0.0382\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4849\u001b[0m  0.0328\n",
            "      2        \u001b[36m0.4840\u001b[0m  0.0451\n",
            "      3        \u001b[36m0.4831\u001b[0m  0.0435\n",
            "      4        \u001b[36m0.4823\u001b[0m  0.0410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.4815\u001b[0m  0.0547\n",
            "      6        \u001b[36m0.4807\u001b[0m  0.0425\n",
            "      7        \u001b[36m0.4799\u001b[0m  0.0514\n",
            "      8        \u001b[36m0.4791\u001b[0m  0.0414\n",
            "      9        \u001b[36m0.4784\u001b[0m  0.0433\n",
            "     10        \u001b[36m0.4776\u001b[0m  0.0507\n",
            "     11        \u001b[36m0.4769\u001b[0m  0.0411\n",
            "     12        \u001b[36m0.4762\u001b[0m  0.0349\n",
            "     13        \u001b[36m0.4755\u001b[0m  0.0383\n",
            "     14        \u001b[36m0.4748\u001b[0m  0.0443\n",
            "     15        \u001b[36m0.4741\u001b[0m  0.0391\n",
            "     16        \u001b[36m0.4735\u001b[0m  0.0373\n",
            "     17        \u001b[36m0.4728\u001b[0m  0.0393\n",
            "     18        \u001b[36m0.4722\u001b[0m  0.0371\n",
            "     19        \u001b[36m0.4715\u001b[0m  0.0382\n",
            "     20        \u001b[36m0.4709\u001b[0m  0.0384\n",
            "     21        \u001b[36m0.4703\u001b[0m  0.0393\n",
            "     22        \u001b[36m0.4697\u001b[0m  0.0385\n",
            "     23        \u001b[36m0.4691\u001b[0m  0.0395\n",
            "     24        \u001b[36m0.4685\u001b[0m  0.0370\n",
            "     25        \u001b[36m0.4679\u001b[0m  0.0436\n",
            "     26        \u001b[36m0.4674\u001b[0m  0.0396\n",
            "     27        \u001b[36m0.4668\u001b[0m  0.0397\n",
            "     28        \u001b[36m0.4662\u001b[0m  0.0425\n",
            "     29        \u001b[36m0.4657\u001b[0m  0.0425\n",
            "     30        \u001b[36m0.4652\u001b[0m  0.0460\n",
            "     31        \u001b[36m0.4646\u001b[0m  0.0439\n",
            "     32        \u001b[36m0.4641\u001b[0m  0.0425\n",
            "     33        \u001b[36m0.4636\u001b[0m  0.0385\n",
            "     34        \u001b[36m0.4631\u001b[0m  0.0445\n",
            "     35        \u001b[36m0.4626\u001b[0m  0.0394\n",
            "     36        \u001b[36m0.4621\u001b[0m  0.0439\n",
            "     37        \u001b[36m0.4616\u001b[0m  0.0380\n",
            "     38        \u001b[36m0.4611\u001b[0m  0.0392\n",
            "     39        \u001b[36m0.4607\u001b[0m  0.0389\n",
            "     40        \u001b[36m0.4602\u001b[0m  0.0408\n",
            "     41        \u001b[36m0.4597\u001b[0m  0.0403\n",
            "     42        \u001b[36m0.4593\u001b[0m  0.0432\n",
            "     43        \u001b[36m0.4588\u001b[0m  0.0500\n",
            "     44        \u001b[36m0.4584\u001b[0m  0.0373\n",
            "     45        \u001b[36m0.4579\u001b[0m  0.0404\n",
            "     46        \u001b[36m0.4575\u001b[0m  0.0380\n",
            "     47        \u001b[36m0.4570\u001b[0m  0.0388\n",
            "     48        \u001b[36m0.4566\u001b[0m  0.0437\n",
            "     49        \u001b[36m0.4562\u001b[0m  0.0457\n",
            "     50        \u001b[36m0.4558\u001b[0m  0.0440\n",
            "     51        \u001b[36m0.4554\u001b[0m  0.0376\n",
            "     52        \u001b[36m0.4550\u001b[0m  0.0397\n",
            "     53        \u001b[36m0.4546\u001b[0m  0.0413\n",
            "     54        \u001b[36m0.4542\u001b[0m  0.0455\n",
            "     55        \u001b[36m0.4538\u001b[0m  0.0508\n",
            "     56        \u001b[36m0.4534\u001b[0m  0.0440\n",
            "     57        \u001b[36m0.4530\u001b[0m  0.0514\n",
            "     58        \u001b[36m0.4526\u001b[0m  0.0404\n",
            "     59        \u001b[36m0.4522\u001b[0m  0.0383\n",
            "     60        \u001b[36m0.4519\u001b[0m  0.0374\n",
            "     61        \u001b[36m0.4515\u001b[0m  0.0408\n",
            "     62        \u001b[36m0.4511\u001b[0m  0.0400\n",
            "     63        \u001b[36m0.4508\u001b[0m  0.0385\n",
            "     64        \u001b[36m0.4504\u001b[0m  0.0380\n",
            "     65        \u001b[36m0.4501\u001b[0m  0.0386\n",
            "     66        \u001b[36m0.4497\u001b[0m  0.0378\n",
            "     67        \u001b[36m0.4494\u001b[0m  0.0365\n",
            "     68        \u001b[36m0.4490\u001b[0m  0.0400\n",
            "     69        \u001b[36m0.4487\u001b[0m  0.0356\n",
            "     70        \u001b[36m0.4483\u001b[0m  0.0400\n",
            "     71        \u001b[36m0.4480\u001b[0m  0.0386\n",
            "     72        \u001b[36m0.4477\u001b[0m  0.0380\n",
            "     73        \u001b[36m0.4474\u001b[0m  0.0373\n",
            "     74        \u001b[36m0.4470\u001b[0m  0.0412\n",
            "     75        \u001b[36m0.4467\u001b[0m  0.0386\n",
            "     76        \u001b[36m0.4464\u001b[0m  0.0460\n",
            "     77        \u001b[36m0.4461\u001b[0m  0.0454\n",
            "     78        \u001b[36m0.4458\u001b[0m  0.0546\n",
            "     79        \u001b[36m0.4455\u001b[0m  0.0419\n",
            "     80        \u001b[36m0.4452\u001b[0m  0.0458\n",
            "     81        \u001b[36m0.4449\u001b[0m  0.0440\n",
            "     82        \u001b[36m0.4446\u001b[0m  0.0373\n",
            "     83        \u001b[36m0.4443\u001b[0m  0.0434\n",
            "     84        \u001b[36m0.4440\u001b[0m  0.0447\n",
            "     85        \u001b[36m0.4437\u001b[0m  0.0377\n",
            "     86        \u001b[36m0.4434\u001b[0m  0.0402\n",
            "     87        \u001b[36m0.4431\u001b[0m  0.0380\n",
            "     88        \u001b[36m0.4428\u001b[0m  0.0400\n",
            "     89        \u001b[36m0.4426\u001b[0m  0.0382\n",
            "     90        \u001b[36m0.4423\u001b[0m  0.0389\n",
            "     91        \u001b[36m0.4420\u001b[0m  0.0415\n",
            "     92        \u001b[36m0.4417\u001b[0m  0.0392\n",
            "     93        \u001b[36m0.4415\u001b[0m  0.0515\n",
            "     94        \u001b[36m0.4412\u001b[0m  0.0419\n",
            "     95        \u001b[36m0.4409\u001b[0m  0.0389\n",
            "     96        \u001b[36m0.4407\u001b[0m  0.0423\n",
            "     97        \u001b[36m0.4404\u001b[0m  0.0456\n",
            "     98        \u001b[36m0.4402\u001b[0m  0.0516\n",
            "     99        \u001b[36m0.4399\u001b[0m  0.0409\n",
            "    100        \u001b[36m0.4396\u001b[0m  0.0561\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7028\u001b[0m  0.0470\n",
            "      2        \u001b[36m0.6010\u001b[0m  0.0665\n",
            "      3        \u001b[36m0.5362\u001b[0m  0.0499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.4901\u001b[0m  0.0581\n",
            "      5        \u001b[36m0.4627\u001b[0m  0.0512\n",
            "      6        \u001b[36m0.4464\u001b[0m  0.0507\n",
            "      7        \u001b[36m0.4304\u001b[0m  0.0575\n",
            "      8        \u001b[36m0.4176\u001b[0m  0.0596\n",
            "      9        \u001b[36m0.4091\u001b[0m  0.0573\n",
            "     10        \u001b[36m0.4035\u001b[0m  0.0523\n",
            "     11        \u001b[36m0.3997\u001b[0m  0.0518\n",
            "     12        \u001b[36m0.3968\u001b[0m  0.0533\n",
            "     13        \u001b[36m0.3955\u001b[0m  0.0528\n",
            "     14        \u001b[36m0.3932\u001b[0m  0.0491\n",
            "     15        \u001b[36m0.3909\u001b[0m  0.0514\n",
            "     16        \u001b[36m0.3889\u001b[0m  0.0554\n",
            "     17        \u001b[36m0.3874\u001b[0m  0.0599\n",
            "     18        \u001b[36m0.3856\u001b[0m  0.0590\n",
            "     19        \u001b[36m0.3846\u001b[0m  0.0671\n",
            "     20        \u001b[36m0.3836\u001b[0m  0.0575\n",
            "     21        \u001b[36m0.3828\u001b[0m  0.0506\n",
            "     22        \u001b[36m0.3822\u001b[0m  0.0506\n",
            "     23        \u001b[36m0.3813\u001b[0m  0.0534\n",
            "     24        \u001b[36m0.3806\u001b[0m  0.0514\n",
            "     25        \u001b[36m0.3798\u001b[0m  0.0522\n",
            "     26        \u001b[36m0.3790\u001b[0m  0.0518\n",
            "     27        \u001b[36m0.3785\u001b[0m  0.0570\n",
            "     28        \u001b[36m0.3779\u001b[0m  0.0537\n",
            "     29        \u001b[36m0.3774\u001b[0m  0.0492\n",
            "     30        \u001b[36m0.3766\u001b[0m  0.0530\n",
            "     31        \u001b[36m0.3762\u001b[0m  0.0573\n",
            "     32        \u001b[36m0.3760\u001b[0m  0.0635\n",
            "     33        \u001b[36m0.3757\u001b[0m  0.0613\n",
            "     34        \u001b[36m0.3755\u001b[0m  0.0578\n",
            "     35        \u001b[36m0.3753\u001b[0m  0.0565\n",
            "     36        \u001b[36m0.3752\u001b[0m  0.0635\n",
            "     37        \u001b[36m0.3750\u001b[0m  0.0619\n",
            "     38        \u001b[36m0.3749\u001b[0m  0.0583\n",
            "     39        \u001b[36m0.3748\u001b[0m  0.0541\n",
            "     40        \u001b[36m0.3747\u001b[0m  0.0529\n",
            "     41        \u001b[36m0.3747\u001b[0m  0.0513\n",
            "     42        \u001b[36m0.3746\u001b[0m  0.0564\n",
            "     43        \u001b[36m0.3746\u001b[0m  0.0538\n",
            "     44        \u001b[36m0.3745\u001b[0m  0.0480\n",
            "     45        \u001b[36m0.3745\u001b[0m  0.0515\n",
            "     46        \u001b[36m0.3744\u001b[0m  0.0527\n",
            "     47        \u001b[36m0.3744\u001b[0m  0.0684\n",
            "     48        \u001b[36m0.3744\u001b[0m  0.0574\n",
            "     49        \u001b[36m0.3743\u001b[0m  0.0666\n",
            "     50        \u001b[36m0.3743\u001b[0m  0.0616\n",
            "     51        \u001b[36m0.3742\u001b[0m  0.0592\n",
            "     52        \u001b[36m0.3742\u001b[0m  0.0644\n",
            "     53        \u001b[36m0.3742\u001b[0m  0.0643\n",
            "     54        \u001b[36m0.3742\u001b[0m  0.0662\n",
            "     55        \u001b[36m0.3741\u001b[0m  0.0529\n",
            "     56        \u001b[36m0.3741\u001b[0m  0.0511\n",
            "     57        \u001b[36m0.3741\u001b[0m  0.0557\n",
            "     58        \u001b[36m0.3741\u001b[0m  0.0537\n",
            "     59        \u001b[36m0.3740\u001b[0m  0.0560\n",
            "     60        \u001b[36m0.3740\u001b[0m  0.0580\n",
            "     61        \u001b[36m0.3740\u001b[0m  0.0527\n",
            "     62        \u001b[36m0.3740\u001b[0m  0.0572\n",
            "     63        \u001b[36m0.3740\u001b[0m  0.0559\n",
            "     64        \u001b[36m0.3739\u001b[0m  0.0510\n",
            "     65        \u001b[36m0.3739\u001b[0m  0.0610\n",
            "     66        \u001b[36m0.3739\u001b[0m  0.0610\n",
            "     67        \u001b[36m0.3739\u001b[0m  0.0628\n",
            "     68        \u001b[36m0.3739\u001b[0m  0.0588\n",
            "     69        \u001b[36m0.3739\u001b[0m  0.0573\n",
            "     70        \u001b[36m0.3739\u001b[0m  0.0534\n",
            "     71        \u001b[36m0.3739\u001b[0m  0.0663\n",
            "     72        \u001b[36m0.3738\u001b[0m  0.0537\n",
            "     73        \u001b[36m0.3738\u001b[0m  0.0595\n",
            "     74        \u001b[36m0.3738\u001b[0m  0.0524\n",
            "     75        \u001b[36m0.3738\u001b[0m  0.0527\n",
            "     76        \u001b[36m0.3738\u001b[0m  0.0516\n",
            "     77        \u001b[36m0.3738\u001b[0m  0.0525\n",
            "     78        \u001b[36m0.3738\u001b[0m  0.0524\n",
            "     79        \u001b[36m0.3738\u001b[0m  0.0572\n",
            "     80        \u001b[36m0.3738\u001b[0m  0.0559\n",
            "     81        \u001b[36m0.3737\u001b[0m  0.0640\n",
            "     82        \u001b[36m0.3737\u001b[0m  0.0662\n",
            "     83        \u001b[36m0.3737\u001b[0m  0.0603\n",
            "     84        \u001b[36m0.3737\u001b[0m  0.0563\n",
            "     85        \u001b[36m0.3737\u001b[0m  0.0570\n",
            "     86        \u001b[36m0.3737\u001b[0m  0.0587\n",
            "     87        \u001b[36m0.3737\u001b[0m  0.0539\n",
            "     88        \u001b[36m0.3737\u001b[0m  0.0657\n",
            "     89        0.3737  0.0542\n",
            "     90        0.3737  0.0538\n",
            "     91        \u001b[36m0.3737\u001b[0m  0.0536\n",
            "     92        \u001b[36m0.3737\u001b[0m  0.0576\n",
            "     93        \u001b[36m0.3736\u001b[0m  0.0643\n",
            "     94        \u001b[36m0.3736\u001b[0m  0.0652\n",
            "     95        \u001b[36m0.3736\u001b[0m  0.0497\n",
            "     96        \u001b[36m0.3736\u001b[0m  0.0612\n",
            "     97        \u001b[36m0.3736\u001b[0m  0.0561\n",
            "     98        \u001b[36m0.3736\u001b[0m  0.0633\n",
            "     99        \u001b[36m0.3736\u001b[0m  0.0641\n",
            "    100        \u001b[36m0.3736\u001b[0m  0.0729\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.5137\u001b[0m  0.0583\n",
            "      2        \u001b[36m0.4746\u001b[0m  0.0743\n",
            "      3        \u001b[36m0.4474\u001b[0m  0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      4        \u001b[36m0.4268\u001b[0m  0.0679\n",
            "      5        \u001b[36m0.4060\u001b[0m  0.0470\n",
            "      6        \u001b[36m0.3938\u001b[0m  0.0527\n",
            "      7        \u001b[36m0.3888\u001b[0m  0.0482\n",
            "      8        \u001b[36m0.3879\u001b[0m  0.0497\n",
            "      9        \u001b[36m0.3847\u001b[0m  0.0495\n",
            "     10        \u001b[36m0.3838\u001b[0m  0.0544\n",
            "     11        \u001b[36m0.3825\u001b[0m  0.0505\n",
            "     12        0.3825  0.0496\n",
            "     13        \u001b[36m0.3798\u001b[0m  0.0501\n",
            "     14        \u001b[36m0.3793\u001b[0m  0.0602\n",
            "     15        0.3794  0.0570\n",
            "     16        \u001b[36m0.3792\u001b[0m  0.0558\n",
            "     17        \u001b[36m0.3788\u001b[0m  0.0644\n",
            "     18        \u001b[36m0.3771\u001b[0m  0.0523\n",
            "     19        \u001b[36m0.3767\u001b[0m  0.0568\n",
            "     20        \u001b[36m0.3764\u001b[0m  0.0538\n",
            "     21        \u001b[36m0.3760\u001b[0m  0.0532\n",
            "     22        \u001b[36m0.3757\u001b[0m  0.0610\n",
            "     23        \u001b[36m0.3754\u001b[0m  0.0608\n",
            "     24        0.3760  0.0481\n",
            "     25        \u001b[36m0.3749\u001b[0m  0.0492\n",
            "     26        \u001b[36m0.3745\u001b[0m  0.0489\n",
            "     27        \u001b[36m0.3731\u001b[0m  0.0531\n",
            "     28        \u001b[36m0.3731\u001b[0m  0.0520\n",
            "     29        \u001b[36m0.3730\u001b[0m  0.0495\n",
            "     30        \u001b[36m0.3730\u001b[0m  0.0580\n",
            "     31        \u001b[36m0.3729\u001b[0m  0.0587\n",
            "     32        \u001b[36m0.3728\u001b[0m  0.0501\n",
            "     33        \u001b[36m0.3727\u001b[0m  0.0601\n",
            "     34        \u001b[36m0.3725\u001b[0m  0.0560\n",
            "     35        \u001b[36m0.3725\u001b[0m  0.0578\n",
            "     36        \u001b[36m0.3724\u001b[0m  0.0512\n",
            "     37        \u001b[36m0.3724\u001b[0m  0.0541\n",
            "     38        \u001b[36m0.3724\u001b[0m  0.0512\n",
            "     39        \u001b[36m0.3724\u001b[0m  0.0552\n",
            "     40        0.3725  0.0658\n",
            "     41        0.3736  0.0510\n",
            "     42        0.3734  0.0541\n",
            "     43        0.3733  0.0520\n",
            "     44        0.3731  0.0517\n",
            "     45        0.3730  0.0511\n",
            "     46        0.3729  0.0561\n",
            "     47        0.3728  0.0511\n",
            "     48        0.3727  0.0621\n",
            "     49        \u001b[36m0.3722\u001b[0m  0.0629\n",
            "     50        \u001b[36m0.3722\u001b[0m  0.0548\n",
            "     51        \u001b[36m0.3722\u001b[0m  0.0524\n",
            "     52        0.3722  0.0548\n",
            "     53        0.3722  0.0564\n",
            "     54        0.3722  0.0567\n",
            "     55        \u001b[36m0.3722\u001b[0m  0.0575\n",
            "     56        \u001b[36m0.3722\u001b[0m  0.0498\n",
            "     57        \u001b[36m0.3722\u001b[0m  0.0506\n",
            "     58        0.3722  0.0624\n",
            "     59        \u001b[36m0.3722\u001b[0m  0.0513\n",
            "     60        \u001b[36m0.3721\u001b[0m  0.0478\n",
            "     61        \u001b[36m0.3721\u001b[0m  0.0467\n",
            "     62        \u001b[36m0.3721\u001b[0m  0.0556\n",
            "     63        \u001b[36m0.3721\u001b[0m  0.0504\n",
            "     64        \u001b[36m0.3721\u001b[0m  0.0490\n",
            "     65        \u001b[36m0.3721\u001b[0m  0.0570\n",
            "     66        0.3721  0.0711\n",
            "     67        0.3721  0.0675\n",
            "     68        0.3721  0.0633\n",
            "     69        0.3721  0.0603\n",
            "     70        0.3721  0.0535\n",
            "     71        0.3721  0.0526\n",
            "     72        0.3721  0.0500\n",
            "     73        0.3721  0.0481\n",
            "     74        0.3721  0.0541\n",
            "     75        0.3721  0.0599\n",
            "     76        0.3721  0.0627\n",
            "     77        0.3721  0.0508\n",
            "     78        0.3721  0.0480\n",
            "     79        0.3721  0.0482\n",
            "     80        0.3721  0.0563\n",
            "     81        0.3721  0.0547\n",
            "     82        0.3721  0.0564\n",
            "     83        0.3721  0.0495\n",
            "     84        0.3721  0.0494\n",
            "     85        0.3721  0.0605\n",
            "     86        \u001b[36m0.3721\u001b[0m  0.0634\n",
            "     87        0.3721  0.0604\n",
            "     88        \u001b[36m0.3721\u001b[0m  0.0571\n",
            "     89        \u001b[36m0.3721\u001b[0m  0.0524\n",
            "     90        \u001b[36m0.3720\u001b[0m  0.0475\n",
            "     91        \u001b[36m0.3720\u001b[0m  0.0541\n",
            "     92        \u001b[36m0.3720\u001b[0m  0.0570\n",
            "     93        \u001b[36m0.3720\u001b[0m  0.0619\n",
            "     94        \u001b[36m0.3720\u001b[0m  0.0483\n",
            "     95        0.3720  0.0552\n",
            "     96        \u001b[36m0.3720\u001b[0m  0.0513\n",
            "     97        \u001b[36m0.3720\u001b[0m  0.0554\n",
            "     98        \u001b[36m0.3720\u001b[0m  0.0557\n",
            "     99        \u001b[36m0.3720\u001b[0m  0.0507\n",
            "    100        \u001b[36m0.3720\u001b[0m  0.0553\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9281\u001b[0m  0.0355\n",
            "      2        \u001b[36m0.9265\u001b[0m  0.0552\n",
            "      3        \u001b[36m0.9248\u001b[0m  0.0468\n",
            "      4        \u001b[36m0.9230\u001b[0m  0.0437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.9211\u001b[0m  0.0524\n",
            "      6        \u001b[36m0.9191\u001b[0m  0.0486\n",
            "      7        \u001b[36m0.9169\u001b[0m  0.0543\n",
            "      8        \u001b[36m0.9146\u001b[0m  0.0408\n",
            "      9        \u001b[36m0.9121\u001b[0m  0.0379\n",
            "     10        \u001b[36m0.9094\u001b[0m  0.0410\n",
            "     11        \u001b[36m0.9065\u001b[0m  0.0395\n",
            "     12        \u001b[36m0.9033\u001b[0m  0.0415\n",
            "     13        \u001b[36m0.8998\u001b[0m  0.0475\n",
            "     14        \u001b[36m0.8958\u001b[0m  0.0398\n",
            "     15        \u001b[36m0.8914\u001b[0m  0.0423\n",
            "     16        \u001b[36m0.8864\u001b[0m  0.0387\n",
            "     17        \u001b[36m0.8806\u001b[0m  0.0476\n",
            "     18        \u001b[36m0.8739\u001b[0m  0.0400\n",
            "     19        \u001b[36m0.8659\u001b[0m  0.0422\n",
            "     20        \u001b[36m0.8562\u001b[0m  0.0451\n",
            "     21        \u001b[36m0.8443\u001b[0m  0.0430\n",
            "     22        \u001b[36m0.8293\u001b[0m  0.0420\n",
            "     23        \u001b[36m0.8104\u001b[0m  0.0428\n",
            "     24        \u001b[36m0.7867\u001b[0m  0.0415\n",
            "     25        \u001b[36m0.7587\u001b[0m  0.0412\n",
            "     26        \u001b[36m0.7283\u001b[0m  0.0438\n",
            "     27        \u001b[36m0.6985\u001b[0m  0.0393\n",
            "     28        \u001b[36m0.6721\u001b[0m  0.0405\n",
            "     29        \u001b[36m0.6498\u001b[0m  0.0406\n",
            "     30        \u001b[36m0.6317\u001b[0m  0.0380\n",
            "     31        \u001b[36m0.6171\u001b[0m  0.0409\n",
            "     32        \u001b[36m0.6051\u001b[0m  0.0406\n",
            "     33        \u001b[36m0.5954\u001b[0m  0.0446\n",
            "     34        \u001b[36m0.5873\u001b[0m  0.0399\n",
            "     35        \u001b[36m0.5806\u001b[0m  0.0438\n",
            "     36        \u001b[36m0.5750\u001b[0m  0.0547\n",
            "     37        \u001b[36m0.5701\u001b[0m  0.0459\n",
            "     38        \u001b[36m0.5660\u001b[0m  0.0462\n",
            "     39        \u001b[36m0.5625\u001b[0m  0.0444\n",
            "     40        \u001b[36m0.5593\u001b[0m  0.0386\n",
            "     41        \u001b[36m0.5566\u001b[0m  0.0401\n",
            "     42        \u001b[36m0.5541\u001b[0m  0.0465\n",
            "     43        \u001b[36m0.5519\u001b[0m  0.0398\n",
            "     44        \u001b[36m0.5500\u001b[0m  0.0390\n",
            "     45        \u001b[36m0.5482\u001b[0m  0.0499\n",
            "     46        \u001b[36m0.5465\u001b[0m  0.0479\n",
            "     47        \u001b[36m0.5450\u001b[0m  0.0450\n",
            "     48        \u001b[36m0.5436\u001b[0m  0.0438\n",
            "     49        \u001b[36m0.5423\u001b[0m  0.0469\n",
            "     50        \u001b[36m0.5411\u001b[0m  0.0428\n",
            "     51        \u001b[36m0.5400\u001b[0m  0.0426\n",
            "     52        \u001b[36m0.5390\u001b[0m  0.0368\n",
            "     53        \u001b[36m0.5380\u001b[0m  0.0411\n",
            "     54        \u001b[36m0.5370\u001b[0m  0.0419\n",
            "     55        \u001b[36m0.5361\u001b[0m  0.0444\n",
            "     56        \u001b[36m0.5353\u001b[0m  0.0412\n",
            "     57        \u001b[36m0.5345\u001b[0m  0.0380\n",
            "     58        \u001b[36m0.5337\u001b[0m  0.0430\n",
            "     59        \u001b[36m0.5330\u001b[0m  0.0391\n",
            "     60        \u001b[36m0.5322\u001b[0m  0.0404\n",
            "     61        \u001b[36m0.5315\u001b[0m  0.0422\n",
            "     62        \u001b[36m0.5309\u001b[0m  0.0414\n",
            "     63        \u001b[36m0.5302\u001b[0m  0.0392\n",
            "     64        \u001b[36m0.5296\u001b[0m  0.0458\n",
            "     65        \u001b[36m0.5290\u001b[0m  0.0450\n",
            "     66        \u001b[36m0.5284\u001b[0m  0.0443\n",
            "     67        \u001b[36m0.5279\u001b[0m  0.0433\n",
            "     68        \u001b[36m0.5273\u001b[0m  0.0488\n",
            "     69        \u001b[36m0.5268\u001b[0m  0.0431\n",
            "     70        \u001b[36m0.5262\u001b[0m  0.0417\n",
            "     71        \u001b[36m0.5257\u001b[0m  0.0417\n",
            "     72        \u001b[36m0.5252\u001b[0m  0.0398\n",
            "     73        \u001b[36m0.5247\u001b[0m  0.0407\n",
            "     74        \u001b[36m0.5242\u001b[0m  0.0501\n",
            "     75        \u001b[36m0.5238\u001b[0m  0.0490\n",
            "     76        \u001b[36m0.5233\u001b[0m  0.0404\n",
            "     77        \u001b[36m0.5228\u001b[0m  0.0421\n",
            "     78        \u001b[36m0.5224\u001b[0m  0.0383\n",
            "     79        \u001b[36m0.5219\u001b[0m  0.0397\n",
            "     80        \u001b[36m0.5215\u001b[0m  0.0403\n",
            "     81        \u001b[36m0.5211\u001b[0m  0.0538\n",
            "     82        \u001b[36m0.5206\u001b[0m  0.0465\n",
            "     83        \u001b[36m0.5202\u001b[0m  0.0422\n",
            "     84        \u001b[36m0.5198\u001b[0m  0.0435\n",
            "     85        \u001b[36m0.5194\u001b[0m  0.0403\n",
            "     86        \u001b[36m0.5190\u001b[0m  0.0414\n",
            "     87        \u001b[36m0.5186\u001b[0m  0.0436\n",
            "     88        \u001b[36m0.5182\u001b[0m  0.0436\n",
            "     89        \u001b[36m0.5176\u001b[0m  0.0472\n",
            "     90        \u001b[36m0.5167\u001b[0m  0.0509\n",
            "     91        \u001b[36m0.5163\u001b[0m  0.0431\n",
            "     92        \u001b[36m0.5160\u001b[0m  0.0433\n",
            "     93        \u001b[36m0.5156\u001b[0m  0.0481\n",
            "     94        \u001b[36m0.5152\u001b[0m  0.0438\n",
            "     95        \u001b[36m0.5148\u001b[0m  0.0433\n",
            "     96        \u001b[36m0.5145\u001b[0m  0.0378\n",
            "     97        \u001b[36m0.5141\u001b[0m  0.0705\n",
            "     98        \u001b[36m0.5137\u001b[0m  0.0386\n",
            "     99        \u001b[36m0.5134\u001b[0m  0.0427\n",
            "    100        \u001b[36m0.5130\u001b[0m  0.0478\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8332\u001b[0m  0.0321\n",
            "      2        \u001b[36m0.8265\u001b[0m  0.0564\n",
            "      3        \u001b[36m0.8234\u001b[0m  0.0414\n",
            "      4        \u001b[36m0.8203\u001b[0m  0.0405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5        \u001b[36m0.8172\u001b[0m  0.0546\n",
            "      6        \u001b[36m0.8142\u001b[0m  0.0451\n",
            "      7        \u001b[36m0.8111\u001b[0m  0.0433\n",
            "      8        \u001b[36m0.8081\u001b[0m  0.0449\n",
            "      9        \u001b[36m0.8052\u001b[0m  0.0443\n",
            "     10        \u001b[36m0.8022\u001b[0m  0.0434\n",
            "     11        \u001b[36m0.7993\u001b[0m  0.0438\n",
            "     12        \u001b[36m0.7964\u001b[0m  0.0476\n",
            "     13        \u001b[36m0.7936\u001b[0m  0.0425\n",
            "     14        \u001b[36m0.7908\u001b[0m  0.0455\n",
            "     15        \u001b[36m0.7881\u001b[0m  0.0397\n",
            "     16        \u001b[36m0.7854\u001b[0m  0.0448\n",
            "     17        \u001b[36m0.7827\u001b[0m  0.0467\n",
            "     18        \u001b[36m0.7801\u001b[0m  0.0423\n",
            "     19        \u001b[36m0.7775\u001b[0m  0.0399\n",
            "     20        \u001b[36m0.7750\u001b[0m  0.0440\n",
            "     21        \u001b[36m0.7725\u001b[0m  0.0424\n",
            "     22        \u001b[36m0.7701\u001b[0m  0.0421\n",
            "     23        \u001b[36m0.7677\u001b[0m  0.0408\n",
            "     24        \u001b[36m0.7654\u001b[0m  0.0656\n",
            "     25        \u001b[36m0.7631\u001b[0m  0.0456\n",
            "     26        \u001b[36m0.7609\u001b[0m  0.0400\n",
            "     27        \u001b[36m0.7587\u001b[0m  0.0483\n",
            "     28        \u001b[36m0.7566\u001b[0m  0.0441\n",
            "     29        \u001b[36m0.7545\u001b[0m  0.0480\n",
            "     30        \u001b[36m0.7524\u001b[0m  0.0443\n",
            "     31        \u001b[36m0.7504\u001b[0m  0.0447\n",
            "     32        \u001b[36m0.7485\u001b[0m  0.0445\n",
            "     33        \u001b[36m0.7466\u001b[0m  0.0401\n",
            "     34        \u001b[36m0.7448\u001b[0m  0.0417\n",
            "     35        \u001b[36m0.7429\u001b[0m  0.0427\n",
            "     36        \u001b[36m0.7412\u001b[0m  0.0425\n",
            "     37        \u001b[36m0.7394\u001b[0m  0.0412\n",
            "     38        \u001b[36m0.7378\u001b[0m  0.0563\n",
            "     39        \u001b[36m0.7361\u001b[0m  0.0434\n",
            "     40        \u001b[36m0.7345\u001b[0m  0.0402\n",
            "     41        \u001b[36m0.7329\u001b[0m  0.0376\n",
            "     42        \u001b[36m0.7314\u001b[0m  0.0402\n",
            "     43        \u001b[36m0.7299\u001b[0m  0.0400\n",
            "     44        \u001b[36m0.7284\u001b[0m  0.0424\n",
            "     45        \u001b[36m0.7270\u001b[0m  0.0389\n",
            "     46        \u001b[36m0.7256\u001b[0m  0.0582\n",
            "     47        \u001b[36m0.7242\u001b[0m  0.0469\n",
            "     48        \u001b[36m0.7228\u001b[0m  0.0465\n",
            "     49        \u001b[36m0.7215\u001b[0m  0.0416\n",
            "     50        \u001b[36m0.7202\u001b[0m  0.0426\n",
            "     51        \u001b[36m0.7189\u001b[0m  0.0487\n",
            "     52        \u001b[36m0.7177\u001b[0m  0.0433\n",
            "     53        \u001b[36m0.7165\u001b[0m  0.0425\n",
            "     54        \u001b[36m0.7153\u001b[0m  0.0420\n",
            "     55        \u001b[36m0.7141\u001b[0m  0.0442\n",
            "     56        \u001b[36m0.7129\u001b[0m  0.0500\n",
            "     57        \u001b[36m0.7118\u001b[0m  0.0440\n",
            "     58        \u001b[36m0.7107\u001b[0m  0.0415\n",
            "     59        \u001b[36m0.7096\u001b[0m  0.0405\n",
            "     60        \u001b[36m0.7085\u001b[0m  0.0392\n",
            "     61        \u001b[36m0.7074\u001b[0m  0.0385\n",
            "     62        \u001b[36m0.7063\u001b[0m  0.0422\n",
            "     63        \u001b[36m0.7053\u001b[0m  0.0417\n",
            "     64        \u001b[36m0.7043\u001b[0m  0.0393\n",
            "     65        \u001b[36m0.7033\u001b[0m  0.0416\n",
            "     66        \u001b[36m0.7023\u001b[0m  0.0407\n",
            "     67        \u001b[36m0.7013\u001b[0m  0.0440\n",
            "     68        \u001b[36m0.7003\u001b[0m  0.0536\n",
            "     69        \u001b[36m0.6993\u001b[0m  0.0432\n",
            "     70        \u001b[36m0.6984\u001b[0m  0.0478\n",
            "     71        \u001b[36m0.6974\u001b[0m  0.0429\n",
            "     72        \u001b[36m0.6965\u001b[0m  0.0427\n",
            "     73        \u001b[36m0.6955\u001b[0m  0.0493\n",
            "     74        \u001b[36m0.6946\u001b[0m  0.0398\n",
            "     75        \u001b[36m0.6937\u001b[0m  0.0476\n",
            "     76        \u001b[36m0.6927\u001b[0m  0.0515\n",
            "     77        \u001b[36m0.6918\u001b[0m  0.0385\n",
            "     78        \u001b[36m0.6909\u001b[0m  0.0428\n",
            "     79        \u001b[36m0.6900\u001b[0m  0.0393\n",
            "     80        \u001b[36m0.6891\u001b[0m  0.0411\n",
            "     81        \u001b[36m0.6881\u001b[0m  0.0379\n",
            "     82        \u001b[36m0.6872\u001b[0m  0.0423\n",
            "     83        \u001b[36m0.6863\u001b[0m  0.0487\n",
            "     84        \u001b[36m0.6854\u001b[0m  0.0373\n",
            "     85        \u001b[36m0.6845\u001b[0m  0.0424\n",
            "     86        \u001b[36m0.6835\u001b[0m  0.0398\n",
            "     87        \u001b[36m0.6826\u001b[0m  0.0392\n",
            "     88        \u001b[36m0.6817\u001b[0m  0.0394\n",
            "     89        \u001b[36m0.6807\u001b[0m  0.0435\n",
            "     90        \u001b[36m0.6798\u001b[0m  0.0486\n",
            "     91        \u001b[36m0.6788\u001b[0m  0.0411\n",
            "     92        \u001b[36m0.6779\u001b[0m  0.0502\n",
            "     93        \u001b[36m0.6769\u001b[0m  0.0435\n",
            "     94        \u001b[36m0.6759\u001b[0m  0.0429\n",
            "     95        \u001b[36m0.6749\u001b[0m  0.0486\n",
            "     96        \u001b[36m0.6739\u001b[0m  0.0420\n",
            "     97        \u001b[36m0.6729\u001b[0m  0.0445\n",
            "     98        \u001b[36m0.6719\u001b[0m  0.0406\n",
            "     99        \u001b[36m0.6708\u001b[0m  0.0361\n",
            "    100        \u001b[36m0.6698\u001b[0m  0.0419\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0196\n",
            "      2       37.3239  0.0215\n",
            "      3       37.3239  0.0145\n",
            "      4       37.3239  0.0228\n",
            "      5       37.3239  0.0181\n",
            "      6       37.3239  0.0153\n",
            "      7       37.3239  0.0182\n",
            "      8       37.3239  0.0192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9       37.3239  0.0243\n",
            "     10       37.3239  0.0230\n",
            "     11       37.3239  0.0253\n",
            "     12       37.3239  0.0199\n",
            "     13       37.3239  0.0141\n",
            "     14       37.3239  0.0185\n",
            "     15       37.3239  0.0193\n",
            "     16       37.3239  0.0223\n",
            "     17       37.3239  0.0208\n",
            "     18       37.3239  0.0236\n",
            "     19       37.3239  0.0245\n",
            "     20       37.3239  0.0190\n",
            "     21       37.3239  0.0233\n",
            "     22       37.3239  0.0226\n",
            "     23       37.3239  0.0269\n",
            "     24       37.3239  0.0252\n",
            "     25       37.3239  0.0244\n",
            "     26       37.3239  0.0176\n",
            "     27       37.3239  0.0277\n",
            "     28       37.3239  0.0189\n",
            "     29       37.3239  0.0228\n",
            "     30       37.3239  0.0182\n",
            "     31       37.3239  0.0219\n",
            "     32       37.3239  0.0244\n",
            "     33       37.3239  0.0267\n",
            "     34       37.3239  0.0244\n",
            "     35       37.3239  0.0245\n",
            "     36       37.3239  0.0198\n",
            "     37       37.3239  0.0256\n",
            "     38       37.3239  0.0188\n",
            "     39       37.3239  0.0239\n",
            "     40       37.3239  0.0199\n",
            "     41       37.3239  0.0151\n",
            "     42       37.3239  0.0206\n",
            "     43       37.3239  0.0178\n",
            "     44       37.3239  0.0250\n",
            "     45       37.3239  0.0219\n",
            "     46       37.3239  0.0198\n",
            "     47       37.3239  0.0202\n",
            "     48       37.3239  0.0194\n",
            "     49       37.3239  0.0223\n",
            "     50       37.3239  0.0339\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0173\n",
            "      2       37.1930  0.0209\n",
            "      3       37.1930  0.0191\n",
            "      4       37.1930  0.0205\n",
            "      5       37.1930  0.0207\n",
            "      6       37.1930  0.0232\n",
            "      7       37.1930  0.0197\n",
            "      8       37.1930  0.0227\n",
            "      9       37.1930  0.0222\n",
            "     10       37.1930  0.0175\n",
            "     11       37.1930  0.0220\n",
            "     12       37.1930  0.0148\n",
            "     13       37.1930  0.0254\n",
            "     14       37.1930  0.0231\n",
            "     15       37.1930  0.0219\n",
            "     16       37.1930  0.0241\n",
            "     17       37.1930  0.0216\n",
            "     18       37.1930  0.0288\n",
            "     19       37.1930  0.0186\n",
            "     20       37.1930  0.0185\n",
            "     21       37.1930  0.0225\n",
            "     22       37.1930  0.0243\n",
            "     23       37.1930  0.0254\n",
            "     24       37.1930  0.0205\n",
            "     25       37.1930  0.0208\n",
            "     26       37.1930  0.0213\n",
            "     27       37.1930  0.0189\n",
            "     28       37.1930  0.0231\n",
            "     29       37.1930  0.0277\n",
            "     30       37.1930  0.0199\n",
            "     31       37.1930  0.0178\n",
            "     32       37.1930  0.0229\n",
            "     33       37.1930  0.0175\n",
            "     34       37.1930  0.0286\n",
            "     35       37.1930  0.0194\n",
            "     36       37.1930  0.0212\n",
            "     37       37.1930  0.0142\n",
            "     38       37.1930  0.0180\n",
            "     39       37.1930  0.0211\n",
            "     40       37.1930  0.0212\n",
            "     41       37.1930  0.0229\n",
            "     42       37.1930  0.0179\n",
            "     43       37.1930  0.0219\n",
            "     44       37.1930  0.0152\n",
            "     45       37.1930  0.0215\n",
            "     46       37.1930  0.0161\n",
            "     47       37.1930  0.0210\n",
            "     48       37.1930  0.0194\n",
            "     49       37.1930  0.0223\n",
            "     50       37.1930  0.0261\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0205\n",
            "      2       37.3239  0.0203\n",
            "      3       37.3239  0.0173\n",
            "      4       37.3239  0.0182\n",
            "      5       37.3239  0.0129\n",
            "      6       37.3239  0.0193\n",
            "      7       37.3239  0.0232\n",
            "      8       37.3239  0.0177\n",
            "      9       37.3239  0.0156\n",
            "     10       37.3239  0.0264\n",
            "     11       37.3239  0.0223\n",
            "     12       37.3239  0.0133\n",
            "     13       37.3239  0.0189\n",
            "     14       37.3239  0.0150\n",
            "     15       37.3239  0.0187\n",
            "     16       37.3239  0.0174\n",
            "     17       37.3239  0.0222\n",
            "     18       37.3239  0.0169\n",
            "     19       37.3239  0.0168\n",
            "     20       37.3239  0.0165\n",
            "     21       37.3239  0.0172\n",
            "     22       37.3239  0.0133\n",
            "     23       37.3239  0.0175\n",
            "     24       37.3239  0.0150\n",
            "     25       37.3239  0.0165\n",
            "     26       37.3239  0.0173\n",
            "     27       37.3239  0.0171\n",
            "     28       37.3239  0.0133\n",
            "     29       37.3239  0.0158\n",
            "     30       37.3239  0.0136\n",
            "     31       37.3239  0.0164\n",
            "     32       37.3239  0.0147\n",
            "     33       37.3239  0.0160\n",
            "     34       37.3239  0.0163\n",
            "     35       37.3239  0.0134\n",
            "     36       37.3239  0.0157\n",
            "     37       37.3239  0.0126\n",
            "     38       37.3239  0.0182\n",
            "     39       37.3239  0.0167\n",
            "     40       37.3239  0.0171\n",
            "     41       37.3239  0.0224\n",
            "     42       37.3239  0.0135\n",
            "     43       37.3239  0.0191\n",
            "     44       37.3239  0.0154\n",
            "     45       37.3239  0.0251\n",
            "     46       37.3239  0.0139\n",
            "     47       37.3239  0.0246\n",
            "     48       37.3239  0.0205\n",
            "     49       37.3239  0.0198\n",
            "     50       37.3239  0.0267\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0137\n",
            "      2       37.1930  0.0161\n",
            "      3       37.1930  0.0250\n",
            "      4       37.1930  0.0217\n",
            "      5       37.1930  0.0245\n",
            "      6       37.1930  0.0166\n",
            "      7       37.1930  0.0183\n",
            "      8       37.1930  0.0187\n",
            "      9       37.1930  0.0168\n",
            "     10       37.1930  0.0166\n",
            "     11       37.1930  0.0206\n",
            "     12       37.1930  0.0185\n",
            "     13       37.1930  0.0154\n",
            "     14       37.1930  0.0218\n",
            "     15       37.1930  0.0146\n",
            "     16       37.1930  0.0194\n",
            "     17       37.1930  0.0279\n",
            "     18       37.1930  0.0205\n",
            "     19       37.1930  0.0143\n",
            "     20       37.1930  0.0270\n",
            "     21       37.1930  0.0146\n",
            "     22       37.1930  0.0144\n",
            "     23       37.1930  0.0159\n",
            "     24       37.1930  0.0152\n",
            "     25       37.1930  0.0147\n",
            "     26       37.1930  0.0181\n",
            "     27       37.1930  0.0128\n",
            "     28       37.1930  0.0191\n",
            "     29       37.1930  0.0143\n",
            "     30       37.1930  0.0174\n",
            "     31       37.1930  0.0118\n",
            "     32       37.1930  0.0156\n",
            "     33       37.1930  0.0149\n",
            "     34       37.1930  0.0149\n",
            "     35       37.1930  0.0138\n",
            "     36       37.1930  0.0182\n",
            "     37       37.1930  0.0198\n",
            "     38       37.1930  0.0182\n",
            "     39       37.1930  0.0166\n",
            "     40       37.1930  0.0186\n",
            "     41       37.1930  0.0108\n",
            "     42       37.1930  0.0235\n",
            "     43       37.1930  0.0203\n",
            "     44       37.1930  0.0172\n",
            "     45       37.1930  0.0164\n",
            "     46       37.1930  0.0169\n",
            "     47       37.1930  0.0126\n",
            "     48       37.1930  0.0185\n",
            "     49       37.1930  0.0165\n",
            "     50       37.1930  0.0168\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0245\n",
            "      2       37.3239  0.0301\n",
            "      3       37.3239  0.0203\n",
            "      4       37.3239  0.0223\n",
            "      5       37.3239  0.0179\n",
            "      6       37.3239  0.0295\n",
            "      7       37.3239  0.0258\n",
            "      8       37.3239  0.0248\n",
            "      9       37.3239  0.0202\n",
            "     10       37.3239  0.0167\n",
            "     11       37.3239  0.0219\n",
            "     12       37.3239  0.0181\n",
            "     13       37.3239  0.0204\n",
            "     14       37.3239  0.0191\n",
            "     15       37.3239  0.0200\n",
            "     16       37.3239  0.0202\n",
            "     17       37.3239  0.0191\n",
            "     18       37.3239  0.0176\n",
            "     19       37.3239  0.0209\n",
            "     20       37.3239  0.0205\n",
            "     21       37.3239  0.0264\n",
            "     22       37.3239  0.0209\n",
            "     23       37.3239  0.0195\n",
            "     24       37.3239  0.0164\n",
            "     25       37.3239  0.0244\n",
            "     26       37.3239  0.0158\n",
            "     27       37.3239  0.0185\n",
            "     28       37.3239  0.0188\n",
            "     29       37.3239  0.0160\n",
            "     30       37.3239  0.0167\n",
            "     31       37.3239  0.0211\n",
            "     32       37.3239  0.0265\n",
            "     33       37.3239  0.0210\n",
            "     34       37.3239  0.0438\n",
            "     35       37.3239  0.0318\n",
            "     36       37.3239  0.0208\n",
            "     37       37.3239  0.0228\n",
            "     38       37.3239  0.0217\n",
            "     39       37.3239  0.0314\n",
            "     40       37.3239  0.0274\n",
            "     41       37.3239  0.0199\n",
            "     42       37.3239  0.0229\n",
            "     43       37.3239  0.0263\n",
            "     44       37.3239  0.0177\n",
            "     45       37.3239  0.0274\n",
            "     46       37.3239  0.0188\n",
            "     47       37.3239  0.0186\n",
            "     48       37.3239  0.0200\n",
            "     49       37.3239  0.0219\n",
            "     50       37.3239  0.0209\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0193\n",
            "      2       37.1930  0.0186\n",
            "      3       37.1930  0.0297\n",
            "      4       37.1930  0.0230\n",
            "      5       37.1930  0.0186\n",
            "      6       37.1930  0.0245\n",
            "      7       37.1930  0.0153\n",
            "      8       37.1930  0.0210\n",
            "      9       37.1930  0.0164\n",
            "     10       37.1930  0.0212\n",
            "     11       37.1930  0.0167\n",
            "     12       37.1930  0.0231\n",
            "     13       37.1930  0.0178\n",
            "     14       37.1930  0.0199\n",
            "     15       37.1930  0.0211\n",
            "     16       37.1930  0.0285\n",
            "     17       37.1930  0.0230\n",
            "     18       37.1930  0.0175\n",
            "     19       37.1930  0.0187\n",
            "     20       37.1930  0.0196\n",
            "     21       37.1930  0.0257\n",
            "     22       37.1930  0.0223\n",
            "     23       37.1930  0.0212\n",
            "     24       37.1930  0.0239\n",
            "     25       37.1930  0.0160\n",
            "     26       37.1930  0.0201\n",
            "     27       37.1930  0.0220\n",
            "     28       37.1930  0.0178\n",
            "     29       37.1930  0.0252\n",
            "     30       37.1930  0.0167\n",
            "     31       37.1930  0.0311\n",
            "     32       37.1930  0.0218\n",
            "     33       37.1930  0.0191\n",
            "     34       37.1930  0.0173\n",
            "     35       37.1930  0.0192\n",
            "     36       37.1930  0.0240\n",
            "     37       37.1930  0.0211\n",
            "     38       37.1930  0.0235\n",
            "     39       37.1930  0.0158\n",
            "     40       37.1930  0.0283\n",
            "     41       37.1930  0.0220\n",
            "     42       37.1930  0.0217\n",
            "     43       37.1930  0.0219\n",
            "     44       37.1930  0.0210\n",
            "     45       37.1930  0.0229\n",
            "     46       37.1930  0.0150\n",
            "     47       37.1930  0.0210\n",
            "     48       37.1930  0.0204\n",
            "     49       37.1930  0.0210\n",
            "     50       37.1930  0.0202\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0157\n",
            "      2       37.3239  0.0131\n",
            "      3       37.3239  0.0152\n",
            "      4       37.3239  0.0177\n",
            "      5       37.3239  0.0193\n",
            "      6       37.3239  0.0197\n",
            "      7       37.3239  0.0191\n",
            "      8       37.3239  0.0114\n",
            "      9       37.3239  0.0190\n",
            "     10       37.3239  0.0108\n",
            "     11       37.3239  0.0170\n",
            "     12       37.3239  0.0169\n",
            "     13       37.3239  0.0218\n",
            "     14       37.3239  0.0238\n",
            "     15       37.3239  0.0213\n",
            "     16       37.3239  0.0243\n",
            "     17       37.3239  0.0181\n",
            "     18       37.3239  0.0189\n",
            "     19       37.3239  0.0173\n",
            "     20       37.3239  0.0204\n",
            "     21       37.3239  0.0171\n",
            "     22       37.3239  0.0193\n",
            "     23       37.3239  0.0225\n",
            "     24       37.3239  0.0250\n",
            "     25       37.3239  0.0169\n",
            "     26       37.3239  0.0138\n",
            "     27       37.3239  0.0240\n",
            "     28       37.3239  0.0189\n",
            "     29       37.3239  0.0147\n",
            "     30       37.3239  0.0129\n",
            "     31       37.3239  0.0156\n",
            "     32       37.3239  0.0204\n",
            "     33       37.3239  0.0147\n",
            "     34       37.3239  0.0174\n",
            "     35       37.3239  0.0183\n",
            "     36       37.3239  0.0134\n",
            "     37       37.3239  0.0156\n",
            "     38       37.3239  0.0184\n",
            "     39       37.3239  0.0201\n",
            "     40       37.3239  0.0188\n",
            "     41       37.3239  0.0155\n",
            "     42       37.3239  0.0144\n",
            "     43       37.3239  0.0166\n",
            "     44       37.3239  0.0148\n",
            "     45       37.3239  0.0165\n",
            "     46       37.3239  0.0165\n",
            "     47       37.3239  0.0137\n",
            "     48       37.3239  0.0165\n",
            "     49       37.3239  0.0176\n",
            "     50       37.3239  0.0180\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0148\n",
            "      2       37.1930  0.0167\n",
            "      3       37.1930  0.0157\n",
            "      4       37.1930  0.0154\n",
            "      5       37.1930  0.0162\n",
            "      6       37.1930  0.0194\n",
            "      7       37.1930  0.0222\n",
            "      8       37.1930  0.0190\n",
            "      9       37.1930  0.0198\n",
            "     10       37.1930  0.0166\n",
            "     11       37.1930  0.0131\n",
            "     12       37.1930  0.0210\n",
            "     13       37.1930  0.0153\n",
            "     14       37.1930  0.0260\n",
            "     15       37.1930  0.0220\n",
            "     16       37.1930  0.0143\n",
            "     17       37.1930  0.0201\n",
            "     18       37.1930  0.0139\n",
            "     19       37.1930  0.0202\n",
            "     20       37.1930  0.0193\n",
            "     21       37.1930  0.0238\n",
            "     22       37.1930  0.0177\n",
            "     23       37.1930  0.0184\n",
            "     24       37.1930  0.0175\n",
            "     25       37.1930  0.0144\n",
            "     26       37.1930  0.0174\n",
            "     27       37.1930  0.0189\n",
            "     28       37.1930  0.0214\n",
            "     29       37.1930  0.0169\n",
            "     30       37.1930  0.0164\n",
            "     31       37.1930  0.0178\n",
            "     32       37.1930  0.0211\n",
            "     33       37.1930  0.0189\n",
            "     34       37.1930  0.0206\n",
            "     35       37.1930  0.0235\n",
            "     36       37.1930  0.0198\n",
            "     37       37.1930  0.0203\n",
            "     38       37.1930  0.0232\n",
            "     39       37.1930  0.0185\n",
            "     40       37.1930  0.0189\n",
            "     41       37.1930  0.0179\n",
            "     42       37.1930  0.0137\n",
            "     43       37.1930  0.0218\n",
            "     44       37.1930  0.0134\n",
            "     45       37.1930  0.0232\n",
            "     46       37.1930  0.0208\n",
            "     47       37.1930  0.0153\n",
            "     48       37.1930  0.0139\n",
            "     49       37.1930  0.0191\n",
            "     50       37.1930  0.0179\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m62.6761\u001b[0m  0.0211\n",
            "      2       62.6761  0.0198\n",
            "      3       62.6761  0.0203\n",
            "      4       62.6761  0.0186\n",
            "      5       62.6761  0.0162\n",
            "      6       62.6761  0.0310\n",
            "      7       62.6761  0.0213\n",
            "      8       62.6761  0.0224\n",
            "      9       62.6761  0.0187\n",
            "     10       62.6761  0.0188\n",
            "     11       62.6761  0.0274\n",
            "     12       62.6761  0.0199\n",
            "     13       62.6761  0.0294\n",
            "     14       62.6761  0.0285\n",
            "     15       62.6761  0.0187\n",
            "     16       62.6761  0.0176\n",
            "     17       62.6761  0.0170\n",
            "     18       62.6761  0.0216\n",
            "     19       62.6761  0.0218\n",
            "     20       62.6761  0.0165\n",
            "     21       62.6761  0.0237\n",
            "     22       62.6761  0.0252\n",
            "     23       62.6761  0.0211\n",
            "     24       62.6761  0.0209\n",
            "     25       62.6761  0.0174\n",
            "     26       62.6761  0.0243\n",
            "     27       62.6761  0.0175\n",
            "     28       62.6761  0.0249\n",
            "     29       62.6761  0.0255\n",
            "     30       62.6761  0.0156\n",
            "     31       62.6761  0.0223\n",
            "     32       62.6761  0.0221\n",
            "     33       62.6761  0.0231\n",
            "     34       62.6761  0.0194\n",
            "     35       62.6761  0.0222\n",
            "     36       62.6761  0.0206\n",
            "     37       62.6761  0.0302\n",
            "     38       62.6761  0.0268\n",
            "     39       62.6761  0.0214\n",
            "     40       62.6761  0.0222\n",
            "     41       62.6761  0.0339\n",
            "     42       62.6761  0.0223\n",
            "     43       62.6761  0.0220\n",
            "     44       62.6761  0.0232\n",
            "     45       62.6761  0.0224\n",
            "     46       62.6761  0.0253\n",
            "     47       62.6761  0.0255\n",
            "     48       62.6761  0.0259\n",
            "     49       62.6761  0.0173\n",
            "     50       62.6761  0.0217\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m39.0951\u001b[0m  0.0320\n",
            "      2       \u001b[36m39.0692\u001b[0m  0.0315\n",
            "      3       \u001b[36m38.4452\u001b[0m  0.0247\n",
            "      4       \u001b[36m38.3826\u001b[0m  0.0319\n",
            "      5       \u001b[36m38.0618\u001b[0m  0.0278\n",
            "      6       \u001b[36m38.0457\u001b[0m  0.0279\n",
            "      7       \u001b[36m37.6475\u001b[0m  0.0268\n",
            "      8       \u001b[36m37.5439\u001b[0m  0.0222\n",
            "      9       \u001b[36m37.0836\u001b[0m  0.0239\n",
            "     10       \u001b[36m36.8421\u001b[0m  0.0265\n",
            "     11       \u001b[36m36.5298\u001b[0m  0.0318\n",
            "     12       36.5444  0.0302\n",
            "     13       \u001b[36m36.4912\u001b[0m  0.0259\n",
            "     14       36.4912  0.0221\n",
            "     15       36.4912  0.0323\n",
            "     16       36.4912  0.0298\n",
            "     17       36.4912  0.0318\n",
            "     18       36.4912  0.0265\n",
            "     19       36.4912  0.0315\n",
            "     20       36.4912  0.0367\n",
            "     21       36.4912  0.0283\n",
            "     22       36.4912  0.0389\n",
            "     23       36.4912  0.0290\n",
            "     24       36.4912  0.0289\n",
            "     25       36.4912  0.0250\n",
            "     26       36.4912  0.0290\n",
            "     27       36.4912  0.0287\n",
            "     28       36.4912  0.0263\n",
            "     29       36.4912  0.0278\n",
            "     30       36.4912  0.0228\n",
            "     31       36.4912  0.0315\n",
            "     32       36.6544  0.0233\n",
            "     33       36.8421  0.0290\n",
            "     34       36.8421  0.0325\n",
            "     35       36.8421  0.0279\n",
            "     36       36.8421  0.0302\n",
            "     37       36.8421  0.0267\n",
            "     38       36.8421  0.0297\n",
            "     39       36.8421  0.0302\n",
            "     40       36.8421  0.0293\n",
            "     41       36.8421  0.0270\n",
            "     42       36.8421  0.0376\n",
            "     43       36.8421  0.0345\n",
            "     44       36.8421  0.0312\n",
            "     45       36.8421  0.0300\n",
            "     46       36.8421  0.0248\n",
            "     47       36.8421  0.0268\n",
            "     48       36.8421  0.0239\n",
            "     49       36.8421  0.0262\n",
            "     50       36.8421  0.0284\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m42.2535\u001b[0m  0.0202\n",
            "      2       42.2535  0.0232\n",
            "      3       42.2535  0.0268\n",
            "      4       42.2535  0.0258\n",
            "      5       42.2535  0.0223\n",
            "      6       42.2535  0.0241\n",
            "      7       42.2535  0.0199\n",
            "      8       42.2535  0.0191\n",
            "      9       42.2535  0.0155\n",
            "     10       42.2535  0.0153\n",
            "     11       42.2535  0.0236\n",
            "     12       42.2535  0.0205\n",
            "     13       42.2535  0.0193\n",
            "     14       42.2535  0.0223\n",
            "     15       42.2535  0.0192\n",
            "     16       42.2535  0.0210\n",
            "     17       42.2535  0.0214\n",
            "     18       42.2535  0.0186\n",
            "     19       42.2535  0.0180\n",
            "     20       42.2535  0.0195\n",
            "     21       42.2535  0.0206\n",
            "     22       42.2535  0.0184\n",
            "     23       42.2535  0.0166\n",
            "     24       42.2535  0.0156\n",
            "     25       42.2535  0.0149\n",
            "     26       42.2535  0.0198\n",
            "     27       42.2535  0.0220\n",
            "     28       42.2535  0.0169\n",
            "     29       42.2535  0.0164\n",
            "     30       42.2535  0.0177\n",
            "     31       42.2535  0.0259\n",
            "     32       42.2535  0.0262\n",
            "     33       42.2535  0.0164\n",
            "     34       42.2535  0.0175\n",
            "     35       42.2535  0.0286\n",
            "     36       42.2535  0.0250\n",
            "     37       42.2535  0.0202\n",
            "     38       42.2535  0.0187\n",
            "     39       42.2535  0.0207\n",
            "     40       42.2535  0.0227\n",
            "     41       42.2535  0.0187\n",
            "     42       42.2535  0.0197\n",
            "     43       42.2535  0.0253\n",
            "     44       42.2535  0.0203\n",
            "     45       42.2535  0.0172\n",
            "     46       42.2535  0.0290\n",
            "     47       42.2535  0.0204\n",
            "     48       42.2535  0.0157\n",
            "     49       42.2535  0.0154\n",
            "     50       42.2535  0.0231\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0242\n",
            "      2       37.1930  0.0157\n",
            "      3       37.1930  0.0151\n",
            "      4       37.1930  0.0152\n",
            "      5       37.1930  0.0155\n",
            "      6       37.1930  0.0242\n",
            "      7       37.1930  0.0177\n",
            "      8       37.1930  0.0152\n",
            "      9       37.1930  0.0153\n",
            "     10       37.1930  0.0169\n",
            "     11       37.1930  0.0197\n",
            "     12       37.1930  0.0207\n",
            "     13       37.1930  0.0206\n",
            "     14       37.1930  0.0176\n",
            "     15       37.1930  0.0180\n",
            "     16       37.1930  0.0154\n",
            "     17       37.1930  0.0150\n",
            "     18       37.1930  0.0149\n",
            "     19       37.1930  0.0149\n",
            "     20       37.1930  0.0222\n",
            "     21       37.1930  0.0210\n",
            "     22       37.1930  0.0229\n",
            "     23       37.1930  0.0198\n",
            "     24       37.1930  0.0216\n",
            "     25       37.1930  0.0192\n",
            "     26       37.1930  0.0158\n",
            "     27       37.1930  0.0156\n",
            "     28       37.1930  0.0202\n",
            "     29       37.1930  0.0216\n",
            "     30       37.1930  0.0227\n",
            "     31       37.1930  0.0195\n",
            "     32       37.1930  0.0168\n",
            "     33       37.1930  0.0200\n",
            "     34       37.1930  0.0296\n",
            "     35       37.1930  0.0246\n",
            "     36       37.1930  0.0257\n",
            "     37       37.1930  0.0218\n",
            "     38       37.1930  0.0200\n",
            "     39       37.1930  0.0206\n",
            "     40       37.1930  0.0213\n",
            "     41       37.1930  0.0216\n",
            "     42       37.1930  0.0176\n",
            "     43       37.1930  0.0197\n",
            "     44       37.1930  0.0193\n",
            "     45       37.1930  0.0222\n",
            "     46       37.1930  0.0211\n",
            "     47       37.1930  0.0244\n",
            "     48       37.1930  0.0191\n",
            "     49       37.1930  0.0191\n",
            "     50       37.1930  0.0234\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m47.1831\u001b[0m  0.0257\n",
            "      2       47.1831  0.0278\n",
            "      3       47.1831  0.0245\n",
            "      4       \u001b[36m46.8310\u001b[0m  0.0331\n",
            "      5       \u001b[36m46.4982\u001b[0m  0.0292\n",
            "      6       47.1831  0.0243\n",
            "      7       47.1831  0.0203\n",
            "      8       47.1831  0.0334\n",
            "      9       47.1831  0.0326\n",
            "     10       47.1970  0.0385\n",
            "     11       47.1831  0.0338\n",
            "     12       47.1831  0.0298\n",
            "     13       \u001b[36m46.4907\u001b[0m  0.0278\n",
            "     14       47.1831  0.0293\n",
            "     15       47.1831  0.0303\n",
            "     16       47.1831  0.0306\n",
            "     17       47.1831  0.0251\n",
            "     18       47.1831  0.0315\n",
            "     19       47.1831  0.0291\n",
            "     20       47.1831  0.0264\n",
            "     21       47.1831  0.0259\n",
            "     22       47.1831  0.0262\n",
            "     23       47.1831  0.0265\n",
            "     24       47.1831  0.0244\n",
            "     25       46.8310  0.0248\n",
            "     26       46.8310  0.0261\n",
            "     27       46.8310  0.0243\n",
            "     28       46.8310  0.0353\n",
            "     29       46.8310  0.0292\n",
            "     30       46.8310  0.0243\n",
            "     31       46.8310  0.0221\n",
            "     32       46.8310  0.0198\n",
            "     33       46.8310  0.0199\n",
            "     34       46.8310  0.0200\n",
            "     35       46.8310  0.0228\n",
            "     36       46.8310  0.0248\n",
            "     37       46.8310  0.0198\n",
            "     38       46.8310  0.0251\n",
            "     39       46.8310  0.0267\n",
            "     40       46.8310  0.0256\n",
            "     41       46.8310  0.0217\n",
            "     42       46.8310  0.0195\n",
            "     43       46.8310  0.0278\n",
            "     44       46.8310  0.0292\n",
            "     45       46.8310  0.0214\n",
            "     46       46.8310  0.0304\n",
            "     47       46.8310  0.0320\n",
            "     48       46.8310  0.0434\n",
            "     49       46.8310  0.0369\n",
            "     50       46.8310  0.0420\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m59.7545\u001b[0m  0.0196\n",
            "      2       59.8240  0.0243\n",
            "      3       59.8525  0.0207\n",
            "      4       59.9566  0.0371\n",
            "      5       60.0000  0.0213\n",
            "      6       60.0000  0.0270\n",
            "      7       60.0000  0.0234\n",
            "      8       60.0000  0.0236\n",
            "      9       60.0000  0.0227\n",
            "     10       60.0000  0.0229\n",
            "     11       60.0000  0.0268\n",
            "     12       60.0000  0.0202\n",
            "     13       60.0000  0.0199\n",
            "     14       60.0000  0.0210\n",
            "     15       60.0000  0.0218\n",
            "     16       60.0000  0.0200\n",
            "     17       \u001b[36m59.5425\u001b[0m  0.0209\n",
            "     18       \u001b[36m59.3990\u001b[0m  0.0214\n",
            "     19       \u001b[36m58.4969\u001b[0m  0.0200\n",
            "     20       \u001b[36m57.5439\u001b[0m  0.0216\n",
            "     21       \u001b[36m56.4912\u001b[0m  0.0224\n",
            "     22       \u001b[36m56.1404\u001b[0m  0.0249\n",
            "     23       56.1404  0.0209\n",
            "     24       \u001b[36m55.9883\u001b[0m  0.0254\n",
            "     25       \u001b[36m55.9412\u001b[0m  0.0254\n",
            "     26       \u001b[36m55.9400\u001b[0m  0.0276\n",
            "     27       55.9536  0.0226\n",
            "     28       55.9907  0.0234\n",
            "     29       56.0354  0.0331\n",
            "     30       56.0838  0.0242\n",
            "     31       56.1404  0.0260\n",
            "     32       56.1404  0.0337\n",
            "     33       56.1404  0.0327\n",
            "     34       \u001b[36m55.8101\u001b[0m  0.0295\n",
            "     35       57.5213  0.0441\n",
            "     36       57.8947  0.0318\n",
            "     37       57.8947  0.0310\n",
            "     38       57.8947  0.0281\n",
            "     39       57.8947  0.0296\n",
            "     40       57.8947  0.0335\n",
            "     41       57.5439  0.0347\n",
            "     42       57.5439  0.0357\n",
            "     43       57.5439  0.0223\n",
            "     44       57.1930  0.0229\n",
            "     45       56.8421  0.0360\n",
            "     46       56.8421  0.0308\n",
            "     47       56.8421  0.0321\n",
            "     48       56.8421  0.0347\n",
            "     49       56.8421  0.0280\n",
            "     50       56.8421  0.0297\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m57.7465\u001b[0m  0.0207\n",
            "      2       57.7465  0.0244\n",
            "      3       57.7465  0.0218\n",
            "      4       57.7465  0.0200\n",
            "      5       57.7465  0.0223\n",
            "      6       57.7465  0.0262\n",
            "      7       57.7465  0.0283\n",
            "      8       57.7465  0.0226\n",
            "      9       57.7465  0.0222\n",
            "     10       57.7465  0.0203\n",
            "     11       57.7465  0.0216\n",
            "     12       57.7465  0.0221\n",
            "     13       57.7465  0.0198\n",
            "     14       57.7465  0.0192\n",
            "     15       57.7465  0.0255\n",
            "     16       57.7465  0.0230\n",
            "     17       57.7465  0.0228\n",
            "     18       57.7465  0.0193\n",
            "     19       57.7465  0.0260\n",
            "     20       57.7465  0.0219\n",
            "     21       57.7465  0.0159\n",
            "     22       57.7465  0.0211\n",
            "     23       57.7465  0.0247\n",
            "     24       57.7465  0.0242\n",
            "     25       57.7465  0.0214\n",
            "     26       57.7465  0.0160\n",
            "     27       57.7465  0.0161\n",
            "     28       57.7465  0.0162\n",
            "     29       57.7465  0.0158\n",
            "     30       57.7465  0.0185\n",
            "     31       57.7465  0.0196\n",
            "     32       57.7465  0.0244\n",
            "     33       57.7465  0.0199\n",
            "     34       57.7465  0.0191\n",
            "     35       57.7465  0.0174\n",
            "     36       57.7465  0.0178\n",
            "     37       57.7465  0.0199\n",
            "     38       57.7465  0.0206\n",
            "     39       57.7465  0.0178\n",
            "     40       57.7465  0.0237\n",
            "     41       57.7465  0.0183\n",
            "     42       57.7465  0.0192\n",
            "     43       57.7465  0.0164\n",
            "     44       57.7465  0.0194\n",
            "     45       57.7465  0.0200\n",
            "     46       57.7465  0.0219\n",
            "     47       57.7465  0.0169\n",
            "     48       57.7465  0.0238\n",
            "     49       57.7465  0.0236\n",
            "     50       57.7465  0.0254\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.5439\u001b[0m  0.0200\n",
            "      2       37.5439  0.0190\n",
            "      3       37.5439  0.0233\n",
            "      4       37.5439  0.0200\n",
            "      5       37.5439  0.0170\n",
            "      6       37.5439  0.0152\n",
            "      7       37.5439  0.0212\n",
            "      8       37.5439  0.0207\n",
            "      9       37.5439  0.0348\n",
            "     10       37.5439  0.0303\n",
            "     11       37.5439  0.0251\n",
            "     12       37.5439  0.0260\n",
            "     13       37.5439  0.0241\n",
            "     14       37.5439  0.0246\n",
            "     15       37.5439  0.0244\n",
            "     16       37.5439  0.0170\n",
            "     17       37.5439  0.0181\n",
            "     18       37.5439  0.0193\n",
            "     19       37.5439  0.0293\n",
            "     20       37.5439  0.0218\n",
            "     21       37.5439  0.0149\n",
            "     22       37.5439  0.0180\n",
            "     23       37.5439  0.0153\n",
            "     24       37.5439  0.0199\n",
            "     25       37.5439  0.0220\n",
            "     26       37.5439  0.0164\n",
            "     27       37.5439  0.0226\n",
            "     28       37.5439  0.0170\n",
            "     29       37.5439  0.0149\n",
            "     30       37.5439  0.0160\n",
            "     31       37.5439  0.0179\n",
            "     32       37.5439  0.0166\n",
            "     33       37.5439  0.0166\n",
            "     34       37.5439  0.0223\n",
            "     35       37.5439  0.0181\n",
            "     36       37.5439  0.0176\n",
            "     37       37.5439  0.0192\n",
            "     38       37.5439  0.0191\n",
            "     39       37.5439  0.0173\n",
            "     40       37.5439  0.0223\n",
            "     41       37.5439  0.0223\n",
            "     42       37.5439  0.0196\n",
            "     43       37.5439  0.0160\n",
            "     44       37.5439  0.0150\n",
            "     45       37.5439  0.0148\n",
            "     46       37.5439  0.0154\n",
            "     47       37.5439  0.0148\n",
            "     48       37.5439  0.0214\n",
            "     49       37.5439  0.0152\n",
            "     50       37.5439  0.0150\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.6297\u001b[0m  0.0214\n",
            "      2        \u001b[36m1.6066\u001b[0m  0.0210\n",
            "      3        \u001b[36m1.5839\u001b[0m  0.0202\n",
            "      4        \u001b[36m1.5611\u001b[0m  0.0193\n",
            "      5        \u001b[36m1.5383\u001b[0m  0.0200\n",
            "      6        \u001b[36m1.5153\u001b[0m  0.0228\n",
            "      7        \u001b[36m1.4923\u001b[0m  0.0219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m1.4692\u001b[0m  0.0214\n",
            "      9        \u001b[36m1.4461\u001b[0m  0.0211\n",
            "     10        \u001b[36m1.4229\u001b[0m  0.0219\n",
            "     11        \u001b[36m1.3997\u001b[0m  0.0201\n",
            "     12        \u001b[36m1.3763\u001b[0m  0.0207\n",
            "     13        \u001b[36m1.3528\u001b[0m  0.0197\n",
            "     14        \u001b[36m1.3290\u001b[0m  0.0247\n",
            "     15        \u001b[36m1.3048\u001b[0m  0.0205\n",
            "     16        \u001b[36m1.2800\u001b[0m  0.0195\n",
            "     17        \u001b[36m1.2543\u001b[0m  0.0269\n",
            "     18        \u001b[36m1.2276\u001b[0m  0.0216\n",
            "     19        \u001b[36m1.1992\u001b[0m  0.0220\n",
            "     20        \u001b[36m1.1687\u001b[0m  0.0237\n",
            "     21        \u001b[36m1.1355\u001b[0m  0.0255\n",
            "     22        \u001b[36m1.0988\u001b[0m  0.0245\n",
            "     23        \u001b[36m1.0581\u001b[0m  0.0220\n",
            "     24        \u001b[36m1.0128\u001b[0m  0.0233\n",
            "     25        \u001b[36m0.9633\u001b[0m  0.0233\n",
            "     26        \u001b[36m0.9107\u001b[0m  0.0217\n",
            "     27        \u001b[36m0.8574\u001b[0m  0.0235\n",
            "     28        \u001b[36m0.8069\u001b[0m  0.0265\n",
            "     29        \u001b[36m0.7628\u001b[0m  0.0247\n",
            "     30        \u001b[36m0.7277\u001b[0m  0.0217\n",
            "     31        \u001b[36m0.7023\u001b[0m  0.0223\n",
            "     32        \u001b[36m0.6856\u001b[0m  0.0267\n",
            "     33        \u001b[36m0.6754\u001b[0m  0.0293\n",
            "     34        \u001b[36m0.6696\u001b[0m  0.0199\n",
            "     35        \u001b[36m0.6662\u001b[0m  0.0205\n",
            "     36        \u001b[36m0.6644\u001b[0m  0.0227\n",
            "     37        \u001b[36m0.6633\u001b[0m  0.0201\n",
            "     38        \u001b[36m0.6627\u001b[0m  0.0274\n",
            "     39        \u001b[36m0.6623\u001b[0m  0.0279\n",
            "     40        \u001b[36m0.6621\u001b[0m  0.0253\n",
            "     41        \u001b[36m0.6619\u001b[0m  0.0238\n",
            "     42        \u001b[36m0.6618\u001b[0m  0.0254\n",
            "     43        \u001b[36m0.6618\u001b[0m  0.0251\n",
            "     44        \u001b[36m0.6617\u001b[0m  0.0198\n",
            "     45        \u001b[36m0.6617\u001b[0m  0.0239\n",
            "     46        \u001b[36m0.6617\u001b[0m  0.0198\n",
            "     47        \u001b[36m0.6617\u001b[0m  0.0193\n",
            "     48        \u001b[36m0.6617\u001b[0m  0.0220\n",
            "     49        \u001b[36m0.6616\u001b[0m  0.0202\n",
            "     50        \u001b[36m0.6616\u001b[0m  0.0242\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.6809\u001b[0m  0.0223\n",
            "      2        \u001b[36m1.6561\u001b[0m  0.0226\n",
            "      3        \u001b[36m1.6329\u001b[0m  0.0252\n",
            "      4        \u001b[36m1.6097\u001b[0m  0.0257\n",
            "      5        \u001b[36m1.5863\u001b[0m  0.0212\n",
            "      6        \u001b[36m1.5628\u001b[0m  0.0247\n",
            "      7        \u001b[36m1.5391\u001b[0m  0.0254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m1.5150\u001b[0m  0.0334\n",
            "      9        \u001b[36m1.4907\u001b[0m  0.0265\n",
            "     10        \u001b[36m1.4659\u001b[0m  0.0233\n",
            "     11        \u001b[36m1.4405\u001b[0m  0.0263\n",
            "     12        \u001b[36m1.4145\u001b[0m  0.0230\n",
            "     13        \u001b[36m1.3875\u001b[0m  0.0246\n",
            "     14        \u001b[36m1.3592\u001b[0m  0.0303\n",
            "     15        \u001b[36m1.3293\u001b[0m  0.0235\n",
            "     16        \u001b[36m1.2972\u001b[0m  0.0165\n",
            "     17        \u001b[36m1.2625\u001b[0m  0.0289\n",
            "     18        \u001b[36m1.2243\u001b[0m  0.0272\n",
            "     19        \u001b[36m1.1822\u001b[0m  0.0217\n",
            "     20        \u001b[36m1.1354\u001b[0m  0.0248\n",
            "     21        \u001b[36m1.0839\u001b[0m  0.0227\n",
            "     22        \u001b[36m1.0279\u001b[0m  0.0295\n",
            "     23        \u001b[36m0.9690\u001b[0m  0.0191\n",
            "     24        \u001b[36m0.9095\u001b[0m  0.0264\n",
            "     25        \u001b[36m0.8525\u001b[0m  0.0150\n",
            "     26        \u001b[36m0.8015\u001b[0m  0.0281\n",
            "     27        \u001b[36m0.7591\u001b[0m  0.0243\n",
            "     28        \u001b[36m0.7263\u001b[0m  0.0171\n",
            "     29        \u001b[36m0.7028\u001b[0m  0.0206\n",
            "     30        \u001b[36m0.6870\u001b[0m  0.0171\n",
            "     31        \u001b[36m0.6769\u001b[0m  0.0233\n",
            "     32        \u001b[36m0.6707\u001b[0m  0.0194\n",
            "     33        \u001b[36m0.6669\u001b[0m  0.0196\n",
            "     34        \u001b[36m0.6646\u001b[0m  0.0210\n",
            "     35        \u001b[36m0.6633\u001b[0m  0.0189\n",
            "     36        \u001b[36m0.6624\u001b[0m  0.0233\n",
            "     37        \u001b[36m0.6619\u001b[0m  0.0213\n",
            "     38        \u001b[36m0.6615\u001b[0m  0.0188\n",
            "     39        \u001b[36m0.6613\u001b[0m  0.0170\n",
            "     40        \u001b[36m0.6612\u001b[0m  0.0209\n",
            "     41        \u001b[36m0.6611\u001b[0m  0.0254\n",
            "     42        \u001b[36m0.6610\u001b[0m  0.0172\n",
            "     43        \u001b[36m0.6610\u001b[0m  0.0240\n",
            "     44        \u001b[36m0.6609\u001b[0m  0.0208\n",
            "     45        \u001b[36m0.6609\u001b[0m  0.0182\n",
            "     46        \u001b[36m0.6609\u001b[0m  0.0267\n",
            "     47        \u001b[36m0.6609\u001b[0m  0.0189\n",
            "     48        \u001b[36m0.6609\u001b[0m  0.0229\n",
            "     49        \u001b[36m0.6609\u001b[0m  0.0168\n",
            "     50        \u001b[36m0.6608\u001b[0m  0.0214\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.5247\u001b[0m  0.0196\n",
            "      2        \u001b[36m1.5146\u001b[0m  0.0198\n",
            "      3        \u001b[36m1.5045\u001b[0m  0.0200\n",
            "      4        \u001b[36m1.4944\u001b[0m  0.0203\n",
            "      5        \u001b[36m1.4844\u001b[0m  0.0136\n",
            "      6        \u001b[36m1.4744\u001b[0m  0.0185\n",
            "      7        \u001b[36m1.4644\u001b[0m  0.0187\n",
            "      8        \u001b[36m1.4545\u001b[0m  0.0307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m1.4445\u001b[0m  0.0182\n",
            "     10        \u001b[36m1.4347\u001b[0m  0.0167\n",
            "     11        \u001b[36m1.4248\u001b[0m  0.0152\n",
            "     12        \u001b[36m1.4150\u001b[0m  0.0173\n",
            "     13        \u001b[36m1.4053\u001b[0m  0.0203\n",
            "     14        \u001b[36m1.3955\u001b[0m  0.0143\n",
            "     15        \u001b[36m1.3858\u001b[0m  0.0120\n",
            "     16        \u001b[36m1.3762\u001b[0m  0.0183\n",
            "     17        \u001b[36m1.3666\u001b[0m  0.0132\n",
            "     18        \u001b[36m1.3570\u001b[0m  0.0177\n",
            "     19        \u001b[36m1.3475\u001b[0m  0.0138\n",
            "     20        \u001b[36m1.3380\u001b[0m  0.0193\n",
            "     21        \u001b[36m1.3285\u001b[0m  0.0206\n",
            "     22        \u001b[36m1.3191\u001b[0m  0.0154\n",
            "     23        \u001b[36m1.3097\u001b[0m  0.0139\n",
            "     24        \u001b[36m1.3004\u001b[0m  0.0158\n",
            "     25        \u001b[36m1.2912\u001b[0m  0.0169\n",
            "     26        \u001b[36m1.2819\u001b[0m  0.0190\n",
            "     27        \u001b[36m1.2728\u001b[0m  0.0186\n",
            "     28        \u001b[36m1.2637\u001b[0m  0.0138\n",
            "     29        \u001b[36m1.2546\u001b[0m  0.0175\n",
            "     30        \u001b[36m1.2456\u001b[0m  0.0168\n",
            "     31        \u001b[36m1.2366\u001b[0m  0.0169\n",
            "     32        \u001b[36m1.2277\u001b[0m  0.0179\n",
            "     33        \u001b[36m1.2188\u001b[0m  0.0187\n",
            "     34        \u001b[36m1.2100\u001b[0m  0.0179\n",
            "     35        \u001b[36m1.2013\u001b[0m  0.0191\n",
            "     36        \u001b[36m1.1926\u001b[0m  0.0125\n",
            "     37        \u001b[36m1.1840\u001b[0m  0.0219\n",
            "     38        \u001b[36m1.1754\u001b[0m  0.0191\n",
            "     39        \u001b[36m1.1669\u001b[0m  0.0198\n",
            "     40        \u001b[36m1.1585\u001b[0m  0.0128\n",
            "     41        \u001b[36m1.1501\u001b[0m  0.0204\n",
            "     42        \u001b[36m1.1418\u001b[0m  0.0148\n",
            "     43        \u001b[36m1.1335\u001b[0m  0.0182\n",
            "     44        \u001b[36m1.1253\u001b[0m  0.0180\n",
            "     45        \u001b[36m1.1172\u001b[0m  0.0235\n",
            "     46        \u001b[36m1.1092\u001b[0m  0.0200\n",
            "     47        \u001b[36m1.1012\u001b[0m  0.0185\n",
            "     48        \u001b[36m1.0933\u001b[0m  0.0136\n",
            "     49        \u001b[36m1.0854\u001b[0m  0.0141\n",
            "     50        \u001b[36m1.0777\u001b[0m  0.0164\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.6753\u001b[0m  0.0165\n",
            "      2        \u001b[36m1.6649\u001b[0m  0.0135\n",
            "      3        \u001b[36m1.6546\u001b[0m  0.0193\n",
            "      4        \u001b[36m1.6442\u001b[0m  0.0244\n",
            "      5        \u001b[36m1.6339\u001b[0m  0.0129\n",
            "      6        \u001b[36m1.6236\u001b[0m  0.0133\n",
            "      7        \u001b[36m1.6133\u001b[0m  0.0128\n",
            "      8        \u001b[36m1.6030\u001b[0m  0.0189\n",
            "      9        \u001b[36m1.5928\u001b[0m  0.0131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.5826\u001b[0m  0.0253\n",
            "     11        \u001b[36m1.5724\u001b[0m  0.0169\n",
            "     12        \u001b[36m1.5622\u001b[0m  0.0240\n",
            "     13        \u001b[36m1.5521\u001b[0m  0.0160\n",
            "     14        \u001b[36m1.5419\u001b[0m  0.0142\n",
            "     15        \u001b[36m1.5318\u001b[0m  0.0161\n",
            "     16        \u001b[36m1.5218\u001b[0m  0.0154\n",
            "     17        \u001b[36m1.5117\u001b[0m  0.0171\n",
            "     18        \u001b[36m1.5017\u001b[0m  0.0159\n",
            "     19        \u001b[36m1.4917\u001b[0m  0.0184\n",
            "     20        \u001b[36m1.4818\u001b[0m  0.0151\n",
            "     21        \u001b[36m1.4719\u001b[0m  0.0204\n",
            "     22        \u001b[36m1.4620\u001b[0m  0.0180\n",
            "     23        \u001b[36m1.4521\u001b[0m  0.0185\n",
            "     24        \u001b[36m1.4423\u001b[0m  0.0192\n",
            "     25        \u001b[36m1.4325\u001b[0m  0.0184\n",
            "     26        \u001b[36m1.4227\u001b[0m  0.0162\n",
            "     27        \u001b[36m1.4130\u001b[0m  0.0201\n",
            "     28        \u001b[36m1.4033\u001b[0m  0.0159\n",
            "     29        \u001b[36m1.3937\u001b[0m  0.0248\n",
            "     30        \u001b[36m1.3841\u001b[0m  0.0252\n",
            "     31        \u001b[36m1.3745\u001b[0m  0.0185\n",
            "     32        \u001b[36m1.3649\u001b[0m  0.0193\n",
            "     33        \u001b[36m1.3554\u001b[0m  0.0145\n",
            "     34        \u001b[36m1.3460\u001b[0m  0.0174\n",
            "     35        \u001b[36m1.3366\u001b[0m  0.0189\n",
            "     36        \u001b[36m1.3272\u001b[0m  0.0185\n",
            "     37        \u001b[36m1.3179\u001b[0m  0.0209\n",
            "     38        \u001b[36m1.3086\u001b[0m  0.0177\n",
            "     39        \u001b[36m1.2993\u001b[0m  0.0160\n",
            "     40        \u001b[36m1.2901\u001b[0m  0.0222\n",
            "     41        \u001b[36m1.2810\u001b[0m  0.0188\n",
            "     42        \u001b[36m1.2719\u001b[0m  0.0183\n",
            "     43        \u001b[36m1.2628\u001b[0m  0.0205\n",
            "     44        \u001b[36m1.2538\u001b[0m  0.0185\n",
            "     45        \u001b[36m1.2449\u001b[0m  0.0192\n",
            "     46        \u001b[36m1.2360\u001b[0m  0.0139\n",
            "     47        \u001b[36m1.2271\u001b[0m  0.0192\n",
            "     48        \u001b[36m1.2183\u001b[0m  0.0115\n",
            "     49        \u001b[36m1.2096\u001b[0m  0.0223\n",
            "     50        \u001b[36m1.2009\u001b[0m  0.0249\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.3862\u001b[0m  0.0240\n",
            "      2        \u001b[36m2.3410\u001b[0m  0.0219\n",
            "      3        \u001b[36m2.2966\u001b[0m  0.0140\n",
            "      4        \u001b[36m2.2519\u001b[0m  0.0231\n",
            "      5        \u001b[36m2.2070\u001b[0m  0.0201\n",
            "      6        \u001b[36m2.1618\u001b[0m  0.0172\n",
            "      7        \u001b[36m2.1165\u001b[0m  0.0202\n",
            "      8        \u001b[36m2.0712\u001b[0m  0.0230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m2.0258\u001b[0m  0.0254\n",
            "     10        \u001b[36m1.9804\u001b[0m  0.0243\n",
            "     11        \u001b[36m1.9351\u001b[0m  0.0233\n",
            "     12        \u001b[36m1.8898\u001b[0m  0.0208\n",
            "     13        \u001b[36m1.8446\u001b[0m  0.0172\n",
            "     14        \u001b[36m1.7994\u001b[0m  0.0215\n",
            "     15        \u001b[36m1.7543\u001b[0m  0.0208\n",
            "     16        \u001b[36m1.7092\u001b[0m  0.0197\n",
            "     17        \u001b[36m1.6637\u001b[0m  0.0281\n",
            "     18        \u001b[36m1.6175\u001b[0m  0.0228\n",
            "     19        \u001b[36m1.5697\u001b[0m  0.0206\n",
            "     20        \u001b[36m1.5185\u001b[0m  0.0161\n",
            "     21        \u001b[36m1.4612\u001b[0m  0.0216\n",
            "     22        \u001b[36m1.3941\u001b[0m  0.0153\n",
            "     23        \u001b[36m1.3162\u001b[0m  0.0215\n",
            "     24        \u001b[36m1.2331\u001b[0m  0.0250\n",
            "     25        \u001b[36m1.1558\u001b[0m  0.0210\n",
            "     26        \u001b[36m1.0909\u001b[0m  0.0284\n",
            "     27        \u001b[36m1.0364\u001b[0m  0.0247\n",
            "     28        \u001b[36m0.9863\u001b[0m  0.0226\n",
            "     29        \u001b[36m0.9340\u001b[0m  0.0189\n",
            "     30        \u001b[36m0.8742\u001b[0m  0.0237\n",
            "     31        \u001b[36m0.8075\u001b[0m  0.0191\n",
            "     32        \u001b[36m0.7455\u001b[0m  0.0246\n",
            "     33        \u001b[36m0.7021\u001b[0m  0.0191\n",
            "     34        \u001b[36m0.6794\u001b[0m  0.0200\n",
            "     35        \u001b[36m0.6695\u001b[0m  0.0283\n",
            "     36        \u001b[36m0.6653\u001b[0m  0.0187\n",
            "     37        \u001b[36m0.6633\u001b[0m  0.0261\n",
            "     38        \u001b[36m0.6624\u001b[0m  0.0239\n",
            "     39        \u001b[36m0.6620\u001b[0m  0.0250\n",
            "     40        \u001b[36m0.6619\u001b[0m  0.0244\n",
            "     41        0.6620  0.0184\n",
            "     42        0.6620  0.0187\n",
            "     43        0.6620  0.0151\n",
            "     44        0.6621  0.0204\n",
            "     45        0.6621  0.0197\n",
            "     46        0.6621  0.0199\n",
            "     47        0.6620  0.0200\n",
            "     48        0.6620  0.0202\n",
            "     49        0.6620  0.0202\n",
            "     50        0.6620  0.0209\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m3.5117\u001b[0m  0.0213\n",
            "      2        \u001b[36m3.4646\u001b[0m  0.0192\n",
            "      3        \u001b[36m3.4201\u001b[0m  0.0198\n",
            "      4        \u001b[36m3.3757\u001b[0m  0.0269\n",
            "      5        \u001b[36m3.3312\u001b[0m  0.0228\n",
            "      6        \u001b[36m3.2866\u001b[0m  0.0173\n",
            "      7        \u001b[36m3.2417\u001b[0m  0.0253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m3.1968\u001b[0m  0.0158\n",
            "      9        \u001b[36m3.1518\u001b[0m  0.0296\n",
            "     10        \u001b[36m3.1066\u001b[0m  0.0225\n",
            "     11        \u001b[36m3.0613\u001b[0m  0.0219\n",
            "     12        \u001b[36m3.0161\u001b[0m  0.0240\n",
            "     13        \u001b[36m2.9707\u001b[0m  0.0236\n",
            "     14        \u001b[36m2.9253\u001b[0m  0.0190\n",
            "     15        \u001b[36m2.8799\u001b[0m  0.0229\n",
            "     16        \u001b[36m2.8343\u001b[0m  0.0188\n",
            "     17        \u001b[36m2.7887\u001b[0m  0.0213\n",
            "     18        \u001b[36m2.7427\u001b[0m  0.0238\n",
            "     19        \u001b[36m2.6963\u001b[0m  0.0189\n",
            "     20        \u001b[36m2.6488\u001b[0m  0.0266\n",
            "     21        \u001b[36m2.5992\u001b[0m  0.0236\n",
            "     22        \u001b[36m2.5450\u001b[0m  0.0161\n",
            "     23        \u001b[36m2.4812\u001b[0m  0.0227\n",
            "     24        \u001b[36m2.3977\u001b[0m  0.0175\n",
            "     25        \u001b[36m2.2755\u001b[0m  0.0258\n",
            "     26        \u001b[36m2.0867\u001b[0m  0.0192\n",
            "     27        \u001b[36m1.8111\u001b[0m  0.0201\n",
            "     28        \u001b[36m1.4729\u001b[0m  0.0199\n",
            "     29        \u001b[36m1.1452\u001b[0m  0.0230\n",
            "     30        \u001b[36m0.8947\u001b[0m  0.0260\n",
            "     31        \u001b[36m0.7502\u001b[0m  0.0219\n",
            "     32        \u001b[36m0.6912\u001b[0m  0.0150\n",
            "     33        \u001b[36m0.6717\u001b[0m  0.0238\n",
            "     34        \u001b[36m0.6640\u001b[0m  0.0180\n",
            "     35        \u001b[36m0.6606\u001b[0m  0.0196\n",
            "     36        \u001b[36m0.6598\u001b[0m  0.0242\n",
            "     37        0.6602  0.0143\n",
            "     38        0.6608  0.0217\n",
            "     39        0.6614  0.0187\n",
            "     40        0.6617  0.0179\n",
            "     41        0.6618  0.0188\n",
            "     42        0.6618  0.0152\n",
            "     43        0.6618  0.0198\n",
            "     44        0.6617  0.0201\n",
            "     45        0.6617  0.0218\n",
            "     46        0.6617  0.0206\n",
            "     47        0.6616  0.0178\n",
            "     48        0.6616  0.0241\n",
            "     49        0.6616  0.0195\n",
            "     50        0.6616  0.0195\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.2044\u001b[0m  0.0185\n",
            "      2        \u001b[36m2.1835\u001b[0m  0.0138\n",
            "      3        \u001b[36m2.1627\u001b[0m  0.0215\n",
            "      4        \u001b[36m2.1418\u001b[0m  0.0166\n",
            "      5        \u001b[36m2.1210\u001b[0m  0.0143\n",
            "      6        \u001b[36m2.1002\u001b[0m  0.0155\n",
            "      7        \u001b[36m2.0795\u001b[0m  0.0172\n",
            "      8        \u001b[36m2.0587\u001b[0m  0.0231\n",
            "      9        \u001b[36m2.0380\u001b[0m  0.0164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m2.0173\u001b[0m  0.0374\n",
            "     11        \u001b[36m1.9967\u001b[0m  0.0220\n",
            "     12        \u001b[36m1.9760\u001b[0m  0.0188\n",
            "     13        \u001b[36m1.9554\u001b[0m  0.0178\n",
            "     14        \u001b[36m1.9349\u001b[0m  0.0158\n",
            "     15        \u001b[36m1.9143\u001b[0m  0.0203\n",
            "     16        \u001b[36m1.8938\u001b[0m  0.0143\n",
            "     17        \u001b[36m1.8734\u001b[0m  0.0157\n",
            "     18        \u001b[36m1.8530\u001b[0m  0.0118\n",
            "     19        \u001b[36m1.8326\u001b[0m  0.0231\n",
            "     20        \u001b[36m1.8123\u001b[0m  0.0179\n",
            "     21        \u001b[36m1.7920\u001b[0m  0.0272\n",
            "     22        \u001b[36m1.7718\u001b[0m  0.0250\n",
            "     23        \u001b[36m1.7516\u001b[0m  0.0168\n",
            "     24        \u001b[36m1.7315\u001b[0m  0.0146\n",
            "     25        \u001b[36m1.7115\u001b[0m  0.0127\n",
            "     26        \u001b[36m1.6915\u001b[0m  0.0209\n",
            "     27        \u001b[36m1.6716\u001b[0m  0.0109\n",
            "     28        \u001b[36m1.6517\u001b[0m  0.0185\n",
            "     29        \u001b[36m1.6319\u001b[0m  0.0133\n",
            "     30        \u001b[36m1.6122\u001b[0m  0.0184\n",
            "     31        \u001b[36m1.5926\u001b[0m  0.0167\n",
            "     32        \u001b[36m1.5731\u001b[0m  0.0171\n",
            "     33        \u001b[36m1.5536\u001b[0m  0.0156\n",
            "     34        \u001b[36m1.5343\u001b[0m  0.0178\n",
            "     35        \u001b[36m1.5150\u001b[0m  0.0195\n",
            "     36        \u001b[36m1.4959\u001b[0m  0.0189\n",
            "     37        \u001b[36m1.4768\u001b[0m  0.0129\n",
            "     38        \u001b[36m1.4579\u001b[0m  0.0215\n",
            "     39        \u001b[36m1.4391\u001b[0m  0.0131\n",
            "     40        \u001b[36m1.4204\u001b[0m  0.0198\n",
            "     41        \u001b[36m1.4018\u001b[0m  0.0159\n",
            "     42        \u001b[36m1.3834\u001b[0m  0.0176\n",
            "     43        \u001b[36m1.3651\u001b[0m  0.0159\n",
            "     44        \u001b[36m1.3470\u001b[0m  0.0183\n",
            "     45        \u001b[36m1.3290\u001b[0m  0.0137\n",
            "     46        \u001b[36m1.3112\u001b[0m  0.0232\n",
            "     47        \u001b[36m1.2935\u001b[0m  0.0199\n",
            "     48        \u001b[36m1.2760\u001b[0m  0.0172\n",
            "     49        \u001b[36m1.2587\u001b[0m  0.0273\n",
            "     50        \u001b[36m1.2416\u001b[0m  0.0132\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m3.2198\u001b[0m  0.0201\n",
            "      2        \u001b[36m3.1988\u001b[0m  0.0163\n",
            "      3        \u001b[36m3.1779\u001b[0m  0.0137\n",
            "      4        \u001b[36m3.1569\u001b[0m  0.0178\n",
            "      5        \u001b[36m3.1361\u001b[0m  0.0147\n",
            "      6        \u001b[36m3.1151\u001b[0m  0.0121\n",
            "      7        \u001b[36m3.0942\u001b[0m  0.0165\n",
            "      8        \u001b[36m3.0732\u001b[0m  0.0135\n",
            "      9        \u001b[36m3.0523\u001b[0m  0.0166\n",
            "     10        \u001b[36m3.0314\u001b[0m  0.0158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     11        \u001b[36m3.0105\u001b[0m  0.0185\n",
            "     12        \u001b[36m2.9896\u001b[0m  0.0207\n",
            "     13        \u001b[36m2.9687\u001b[0m  0.0158\n",
            "     14        \u001b[36m2.9478\u001b[0m  0.0128\n",
            "     15        \u001b[36m2.9269\u001b[0m  0.0175\n",
            "     16        \u001b[36m2.9060\u001b[0m  0.0169\n",
            "     17        \u001b[36m2.8851\u001b[0m  0.0199\n",
            "     18        \u001b[36m2.8642\u001b[0m  0.0221\n",
            "     19        \u001b[36m2.8433\u001b[0m  0.0209\n",
            "     20        \u001b[36m2.8224\u001b[0m  0.0128\n",
            "     21        \u001b[36m2.8015\u001b[0m  0.0149\n",
            "     22        \u001b[36m2.7806\u001b[0m  0.0201\n",
            "     23        \u001b[36m2.7597\u001b[0m  0.0159\n",
            "     24        \u001b[36m2.7389\u001b[0m  0.0179\n",
            "     25        \u001b[36m2.7180\u001b[0m  0.0148\n",
            "     26        \u001b[36m2.6971\u001b[0m  0.0140\n",
            "     27        \u001b[36m2.6763\u001b[0m  0.0130\n",
            "     28        \u001b[36m2.6554\u001b[0m  0.0162\n",
            "     29        \u001b[36m2.6346\u001b[0m  0.0152\n",
            "     30        \u001b[36m2.6137\u001b[0m  0.0161\n",
            "     31        \u001b[36m2.5929\u001b[0m  0.0129\n",
            "     32        \u001b[36m2.5720\u001b[0m  0.0185\n",
            "     33        \u001b[36m2.5512\u001b[0m  0.0107\n",
            "     34        \u001b[36m2.5304\u001b[0m  0.0177\n",
            "     35        \u001b[36m2.5096\u001b[0m  0.0179\n",
            "     36        \u001b[36m2.4888\u001b[0m  0.0168\n",
            "     37        \u001b[36m2.4679\u001b[0m  0.0180\n",
            "     38        \u001b[36m2.4471\u001b[0m  0.0136\n",
            "     39        \u001b[36m2.4264\u001b[0m  0.0171\n",
            "     40        \u001b[36m2.4056\u001b[0m  0.0181\n",
            "     41        \u001b[36m2.3848\u001b[0m  0.0178\n",
            "     42        \u001b[36m2.3641\u001b[0m  0.0179\n",
            "     43        \u001b[36m2.3433\u001b[0m  0.0203\n",
            "     44        \u001b[36m2.3226\u001b[0m  0.0136\n",
            "     45        \u001b[36m2.3019\u001b[0m  0.0216\n",
            "     46        \u001b[36m2.2812\u001b[0m  0.0180\n",
            "     47        \u001b[36m2.2605\u001b[0m  0.0203\n",
            "     48        \u001b[36m2.2398\u001b[0m  0.0170\n",
            "     49        \u001b[36m2.2191\u001b[0m  0.0156\n",
            "     50        \u001b[36m2.1985\u001b[0m  0.0147\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8150\u001b[0m  0.0187\n",
            "      2        \u001b[36m0.7853\u001b[0m  0.0265\n",
            "      3        \u001b[36m0.7725\u001b[0m  0.0225\n",
            "      4        \u001b[36m0.7549\u001b[0m  0.0202\n",
            "      5        \u001b[36m0.7394\u001b[0m  0.0150\n",
            "      6        \u001b[36m0.7292\u001b[0m  0.0155\n",
            "      7        \u001b[36m0.7201\u001b[0m  0.0170\n",
            "      8        \u001b[36m0.7132\u001b[0m  0.0227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.7050\u001b[0m  0.0252\n",
            "     10        \u001b[36m0.6990\u001b[0m  0.0228\n",
            "     11        \u001b[36m0.6937\u001b[0m  0.0208\n",
            "     12        \u001b[36m0.6889\u001b[0m  0.0200\n",
            "     13        \u001b[36m0.6862\u001b[0m  0.0243\n",
            "     14        \u001b[36m0.6823\u001b[0m  0.0231\n",
            "     15        \u001b[36m0.6788\u001b[0m  0.0174\n",
            "     16        \u001b[36m0.6756\u001b[0m  0.0283\n",
            "     17        \u001b[36m0.6727\u001b[0m  0.0223\n",
            "     18        \u001b[36m0.6701\u001b[0m  0.0228\n",
            "     19        0.6736  0.0208\n",
            "     20        0.6712  0.0208\n",
            "     21        \u001b[36m0.6691\u001b[0m  0.0256\n",
            "     22        \u001b[36m0.6671\u001b[0m  0.0237\n",
            "     23        \u001b[36m0.6642\u001b[0m  0.0229\n",
            "     24        0.6656  0.0215\n",
            "     25        0.6651  0.0271\n",
            "     26        \u001b[36m0.6634\u001b[0m  0.0243\n",
            "     27        \u001b[36m0.6619\u001b[0m  0.0270\n",
            "     28        \u001b[36m0.6604\u001b[0m  0.0203\n",
            "     29        \u001b[36m0.6590\u001b[0m  0.0175\n",
            "     30        \u001b[36m0.6576\u001b[0m  0.0234\n",
            "     31        \u001b[36m0.6563\u001b[0m  0.0301\n",
            "     32        \u001b[36m0.6550\u001b[0m  0.0240\n",
            "     33        \u001b[36m0.6537\u001b[0m  0.0253\n",
            "     34        \u001b[36m0.6507\u001b[0m  0.0261\n",
            "     35        \u001b[36m0.6496\u001b[0m  0.0202\n",
            "     36        \u001b[36m0.6484\u001b[0m  0.0240\n",
            "     37        \u001b[36m0.6473\u001b[0m  0.0277\n",
            "     38        \u001b[36m0.6462\u001b[0m  0.0240\n",
            "     39        \u001b[36m0.6451\u001b[0m  0.0248\n",
            "     40        \u001b[36m0.6441\u001b[0m  0.0198\n",
            "     41        \u001b[36m0.6431\u001b[0m  0.0262\n",
            "     42        \u001b[36m0.6421\u001b[0m  0.0181\n",
            "     43        \u001b[36m0.6411\u001b[0m  0.0148\n",
            "     44        \u001b[36m0.6402\u001b[0m  0.0167\n",
            "     45        \u001b[36m0.6393\u001b[0m  0.0232\n",
            "     46        \u001b[36m0.6382\u001b[0m  0.0229\n",
            "     47        \u001b[36m0.6340\u001b[0m  0.0159\n",
            "     48        \u001b[36m0.6293\u001b[0m  0.0237\n",
            "     49        \u001b[36m0.6267\u001b[0m  0.0203\n",
            "     50        \u001b[36m0.6259\u001b[0m  0.0173\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9443\u001b[0m  0.0263\n",
            "      2        \u001b[36m0.9000\u001b[0m  0.0229\n",
            "      3        \u001b[36m0.8701\u001b[0m  0.0227\n",
            "      4        \u001b[36m0.8424\u001b[0m  0.0204\n",
            "      5        \u001b[36m0.8153\u001b[0m  0.0155\n",
            "      6        \u001b[36m0.7922\u001b[0m  0.0216\n",
            "      7        \u001b[36m0.7745\u001b[0m  0.0168\n",
            "      8        \u001b[36m0.7606\u001b[0m  0.0236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.7488\u001b[0m  0.0272\n",
            "     10        \u001b[36m0.7388\u001b[0m  0.0246\n",
            "     11        \u001b[36m0.7332\u001b[0m  0.0217\n",
            "     12        \u001b[36m0.7256\u001b[0m  0.0169\n",
            "     13        \u001b[36m0.7189\u001b[0m  0.0208\n",
            "     14        \u001b[36m0.7078\u001b[0m  0.0219\n",
            "     15        \u001b[36m0.7025\u001b[0m  0.0232\n",
            "     16        \u001b[36m0.6978\u001b[0m  0.0283\n",
            "     17        \u001b[36m0.6936\u001b[0m  0.0192\n",
            "     18        \u001b[36m0.6898\u001b[0m  0.0179\n",
            "     19        \u001b[36m0.6863\u001b[0m  0.0271\n",
            "     20        \u001b[36m0.6830\u001b[0m  0.0278\n",
            "     21        \u001b[36m0.6800\u001b[0m  0.0222\n",
            "     22        \u001b[36m0.6772\u001b[0m  0.0261\n",
            "     23        \u001b[36m0.6745\u001b[0m  0.0162\n",
            "     24        \u001b[36m0.6720\u001b[0m  0.0319\n",
            "     25        \u001b[36m0.6696\u001b[0m  0.0231\n",
            "     26        \u001b[36m0.6674\u001b[0m  0.0208\n",
            "     27        \u001b[36m0.6653\u001b[0m  0.0180\n",
            "     28        \u001b[36m0.6633\u001b[0m  0.0208\n",
            "     29        \u001b[36m0.6613\u001b[0m  0.0228\n",
            "     30        \u001b[36m0.6595\u001b[0m  0.0166\n",
            "     31        \u001b[36m0.6577\u001b[0m  0.0207\n",
            "     32        \u001b[36m0.6561\u001b[0m  0.0242\n",
            "     33        \u001b[36m0.6526\u001b[0m  0.0188\n",
            "     34        \u001b[36m0.6511\u001b[0m  0.0188\n",
            "     35        0.6519  0.0201\n",
            "     36        \u001b[36m0.6483\u001b[0m  0.0259\n",
            "     37        \u001b[36m0.6470\u001b[0m  0.0234\n",
            "     38        \u001b[36m0.6457\u001b[0m  0.0206\n",
            "     39        \u001b[36m0.6445\u001b[0m  0.0186\n",
            "     40        \u001b[36m0.6434\u001b[0m  0.0228\n",
            "     41        \u001b[36m0.6431\u001b[0m  0.0224\n",
            "     42        \u001b[36m0.6412\u001b[0m  0.0167\n",
            "     43        \u001b[36m0.6412\u001b[0m  0.0276\n",
            "     44        \u001b[36m0.6402\u001b[0m  0.0161\n",
            "     45        \u001b[36m0.6392\u001b[0m  0.0198\n",
            "     46        \u001b[36m0.6383\u001b[0m  0.0236\n",
            "     47        \u001b[36m0.6373\u001b[0m  0.0214\n",
            "     48        \u001b[36m0.6364\u001b[0m  0.0216\n",
            "     49        \u001b[36m0.6358\u001b[0m  0.0167\n",
            "     50        \u001b[36m0.6349\u001b[0m  0.0196\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.3977\u001b[0m  0.0186\n",
            "      2        \u001b[36m1.3861\u001b[0m  0.0160\n",
            "      3        \u001b[36m1.3578\u001b[0m  0.0168\n",
            "      4        1.3710  0.0235\n",
            "      5        \u001b[36m1.3545\u001b[0m  0.0143\n",
            "      6        \u001b[36m1.3447\u001b[0m  0.0205\n",
            "      7        \u001b[36m1.3356\u001b[0m  0.0169\n",
            "      8        \u001b[36m1.3115\u001b[0m  0.0177\n",
            "      9        \u001b[36m1.2989\u001b[0m  0.0149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.2847\u001b[0m  0.0202\n",
            "     11        \u001b[36m1.2742\u001b[0m  0.0188\n",
            "     12        \u001b[36m1.2653\u001b[0m  0.0222\n",
            "     13        \u001b[36m1.2566\u001b[0m  0.0191\n",
            "     14        \u001b[36m1.2480\u001b[0m  0.0170\n",
            "     15        \u001b[36m1.2396\u001b[0m  0.0165\n",
            "     16        \u001b[36m1.2307\u001b[0m  0.0185\n",
            "     17        \u001b[36m1.2227\u001b[0m  0.0185\n",
            "     18        \u001b[36m1.2147\u001b[0m  0.0202\n",
            "     19        \u001b[36m1.2046\u001b[0m  0.0189\n",
            "     20        \u001b[36m1.1867\u001b[0m  0.0158\n",
            "     21        \u001b[36m1.1589\u001b[0m  0.0141\n",
            "     22        1.1784  0.0157\n",
            "     23        1.1650  0.0141\n",
            "     24        \u001b[36m1.1524\u001b[0m  0.0154\n",
            "     25        \u001b[36m1.1455\u001b[0m  0.0152\n",
            "     26        \u001b[36m1.1388\u001b[0m  0.0159\n",
            "     27        \u001b[36m1.1322\u001b[0m  0.0154\n",
            "     28        \u001b[36m1.1258\u001b[0m  0.0160\n",
            "     29        \u001b[36m1.1194\u001b[0m  0.0159\n",
            "     30        \u001b[36m1.1131\u001b[0m  0.0163\n",
            "     31        \u001b[36m1.1070\u001b[0m  0.0168\n",
            "     32        \u001b[36m1.1010\u001b[0m  0.0179\n",
            "     33        \u001b[36m1.0950\u001b[0m  0.0187\n",
            "     34        \u001b[36m1.0892\u001b[0m  0.0223\n",
            "     35        \u001b[36m1.0835\u001b[0m  0.0165\n",
            "     36        \u001b[36m1.0778\u001b[0m  0.0139\n",
            "     37        \u001b[36m1.0723\u001b[0m  0.0181\n",
            "     38        \u001b[36m1.0668\u001b[0m  0.0157\n",
            "     39        \u001b[36m1.0615\u001b[0m  0.0168\n",
            "     40        \u001b[36m1.0562\u001b[0m  0.0135\n",
            "     41        \u001b[36m1.0510\u001b[0m  0.0187\n",
            "     42        \u001b[36m1.0459\u001b[0m  0.0197\n",
            "     43        \u001b[36m1.0409\u001b[0m  0.0188\n",
            "     44        \u001b[36m1.0360\u001b[0m  0.0134\n",
            "     45        \u001b[36m1.0311\u001b[0m  0.0168\n",
            "     46        \u001b[36m1.0264\u001b[0m  0.0176\n",
            "     47        \u001b[36m1.0217\u001b[0m  0.0225\n",
            "     48        \u001b[36m1.0170\u001b[0m  0.0151\n",
            "     49        \u001b[36m1.0125\u001b[0m  0.0194\n",
            "     50        \u001b[36m1.0080\u001b[0m  0.0177\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.5040\u001b[0m  0.0134\n",
            "      2        \u001b[36m1.4875\u001b[0m  0.0257\n",
            "      3        \u001b[36m1.4712\u001b[0m  0.0191\n",
            "      4        \u001b[36m1.4551\u001b[0m  0.0226\n",
            "      5        \u001b[36m1.4391\u001b[0m  0.0140\n",
            "      6        \u001b[36m1.4234\u001b[0m  0.0166\n",
            "      7        \u001b[36m1.4078\u001b[0m  0.0143\n",
            "      8        \u001b[36m1.3924\u001b[0m  0.0203\n",
            "      9        \u001b[36m1.3772\u001b[0m  0.0134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.3623\u001b[0m  0.0239\n",
            "     11        \u001b[36m1.3475\u001b[0m  0.0218\n",
            "     12        \u001b[36m1.3329\u001b[0m  0.0161\n",
            "     13        \u001b[36m1.3185\u001b[0m  0.0117\n",
            "     14        \u001b[36m1.3043\u001b[0m  0.0157\n",
            "     15        \u001b[36m1.2904\u001b[0m  0.0169\n",
            "     16        \u001b[36m1.2766\u001b[0m  0.0212\n",
            "     17        \u001b[36m1.2631\u001b[0m  0.0135\n",
            "     18        \u001b[36m1.2497\u001b[0m  0.0214\n",
            "     19        \u001b[36m1.2366\u001b[0m  0.0168\n",
            "     20        \u001b[36m1.2237\u001b[0m  0.0173\n",
            "     21        \u001b[36m1.2110\u001b[0m  0.0152\n",
            "     22        \u001b[36m1.1985\u001b[0m  0.0207\n",
            "     23        \u001b[36m1.1863\u001b[0m  0.0168\n",
            "     24        \u001b[36m1.1742\u001b[0m  0.0160\n",
            "     25        \u001b[36m1.1624\u001b[0m  0.0120\n",
            "     26        \u001b[36m1.1508\u001b[0m  0.0189\n",
            "     27        \u001b[36m1.1394\u001b[0m  0.0204\n",
            "     28        \u001b[36m1.1282\u001b[0m  0.0192\n",
            "     29        \u001b[36m1.1172\u001b[0m  0.0237\n",
            "     30        \u001b[36m1.1064\u001b[0m  0.0157\n",
            "     31        \u001b[36m1.0959\u001b[0m  0.0131\n",
            "     32        \u001b[36m1.0856\u001b[0m  0.0134\n",
            "     33        \u001b[36m1.0754\u001b[0m  0.0175\n",
            "     34        \u001b[36m1.0655\u001b[0m  0.0128\n",
            "     35        \u001b[36m1.0558\u001b[0m  0.0172\n",
            "     36        \u001b[36m1.0463\u001b[0m  0.0214\n",
            "     37        \u001b[36m1.0370\u001b[0m  0.0166\n",
            "     38        \u001b[36m1.0280\u001b[0m  0.0165\n",
            "     39        \u001b[36m1.0191\u001b[0m  0.0159\n",
            "     40        \u001b[36m1.0104\u001b[0m  0.0173\n",
            "     41        \u001b[36m1.0019\u001b[0m  0.0192\n",
            "     42        \u001b[36m0.9936\u001b[0m  0.0193\n",
            "     43        \u001b[36m0.9855\u001b[0m  0.0206\n",
            "     44        \u001b[36m0.9776\u001b[0m  0.0199\n",
            "     45        \u001b[36m0.9699\u001b[0m  0.0140\n",
            "     46        \u001b[36m0.9624\u001b[0m  0.0182\n",
            "     47        \u001b[36m0.9550\u001b[0m  0.0110\n",
            "     48        \u001b[36m0.9478\u001b[0m  0.0206\n",
            "     49        \u001b[36m0.9408\u001b[0m  0.0194\n",
            "     50        \u001b[36m0.9340\u001b[0m  0.0214\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.2320\u001b[0m  0.0251\n",
            "      2        \u001b[36m1.1348\u001b[0m  0.0358\n",
            "      3        \u001b[36m1.0741\u001b[0m  0.0210\n",
            "      4        \u001b[36m1.0203\u001b[0m  0.0275\n",
            "      5        \u001b[36m0.9713\u001b[0m  0.0241\n",
            "      6        \u001b[36m0.9270\u001b[0m  0.0157\n",
            "      7        \u001b[36m0.8879\u001b[0m  0.0234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.8540\u001b[0m  0.0264\n",
            "      9        \u001b[36m0.8223\u001b[0m  0.0215\n",
            "     10        \u001b[36m0.7960\u001b[0m  0.0269\n",
            "     11        \u001b[36m0.7730\u001b[0m  0.0155\n",
            "     12        \u001b[36m0.7571\u001b[0m  0.0298\n",
            "     13        \u001b[36m0.7257\u001b[0m  0.0258\n",
            "     14        \u001b[36m0.7171\u001b[0m  0.0223\n",
            "     15        \u001b[36m0.7051\u001b[0m  0.0249\n",
            "     16        \u001b[36m0.6948\u001b[0m  0.0231\n",
            "     17        \u001b[36m0.6920\u001b[0m  0.0188\n",
            "     18        \u001b[36m0.6837\u001b[0m  0.0184\n",
            "     19        \u001b[36m0.6781\u001b[0m  0.0160\n",
            "     20        \u001b[36m0.6714\u001b[0m  0.0201\n",
            "     21        \u001b[36m0.6648\u001b[0m  0.0300\n",
            "     22        \u001b[36m0.6449\u001b[0m  0.0211\n",
            "     23        \u001b[36m0.6403\u001b[0m  0.0191\n",
            "     24        \u001b[36m0.6367\u001b[0m  0.0200\n",
            "     25        \u001b[36m0.6316\u001b[0m  0.0230\n",
            "     26        \u001b[36m0.6247\u001b[0m  0.0177\n",
            "     27        0.6250  0.0318\n",
            "     28        \u001b[36m0.6202\u001b[0m  0.0226\n",
            "     29        \u001b[36m0.6161\u001b[0m  0.0214\n",
            "     30        \u001b[36m0.6130\u001b[0m  0.0161\n",
            "     31        \u001b[36m0.6104\u001b[0m  0.0217\n",
            "     32        0.6121  0.0195\n",
            "     33        \u001b[36m0.6062\u001b[0m  0.0252\n",
            "     34        \u001b[36m0.6039\u001b[0m  0.0291\n",
            "     35        \u001b[36m0.6019\u001b[0m  0.0258\n",
            "     36        \u001b[36m0.6000\u001b[0m  0.0249\n",
            "     37        \u001b[36m0.5981\u001b[0m  0.0242\n",
            "     38        0.5990  0.0241\n",
            "     39        \u001b[36m0.5972\u001b[0m  0.0338\n",
            "     40        \u001b[36m0.5954\u001b[0m  0.0254\n",
            "     41        \u001b[36m0.5937\u001b[0m  0.0213\n",
            "     42        \u001b[36m0.5920\u001b[0m  0.0211\n",
            "     43        \u001b[36m0.5904\u001b[0m  0.0239\n",
            "     44        \u001b[36m0.5888\u001b[0m  0.0220\n",
            "     45        \u001b[36m0.5872\u001b[0m  0.0199\n",
            "     46        \u001b[36m0.5858\u001b[0m  0.0157\n",
            "     47        \u001b[36m0.5843\u001b[0m  0.0287\n",
            "     48        \u001b[36m0.5829\u001b[0m  0.0243\n",
            "     49        \u001b[36m0.5816\u001b[0m  0.0205\n",
            "     50        \u001b[36m0.5803\u001b[0m  0.0174\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.4779\u001b[0m  0.0153\n",
            "      2        \u001b[36m1.4270\u001b[0m  0.0179\n",
            "      3        \u001b[36m1.3516\u001b[0m  0.0170\n",
            "      4        \u001b[36m1.2932\u001b[0m  0.0206\n",
            "      5        \u001b[36m1.2206\u001b[0m  0.0195\n",
            "      6        \u001b[36m1.1544\u001b[0m  0.0247\n",
            "      7        \u001b[36m1.1039\u001b[0m  0.0225\n",
            "      8        \u001b[36m1.0400\u001b[0m  0.0165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.9957\u001b[0m  0.0296\n",
            "     10        \u001b[36m0.9612\u001b[0m  0.0232\n",
            "     11        \u001b[36m0.9403\u001b[0m  0.0287\n",
            "     12        \u001b[36m0.9176\u001b[0m  0.0238\n",
            "     13        \u001b[36m0.8967\u001b[0m  0.0235\n",
            "     14        \u001b[36m0.8791\u001b[0m  0.0224\n",
            "     15        \u001b[36m0.8672\u001b[0m  0.0205\n",
            "     16        \u001b[36m0.8461\u001b[0m  0.0185\n",
            "     17        \u001b[36m0.8260\u001b[0m  0.0256\n",
            "     18        \u001b[36m0.8116\u001b[0m  0.0254\n",
            "     19        \u001b[36m0.7998\u001b[0m  0.0284\n",
            "     20        \u001b[36m0.7893\u001b[0m  0.0178\n",
            "     21        \u001b[36m0.7784\u001b[0m  0.0237\n",
            "     22        \u001b[36m0.7689\u001b[0m  0.0244\n",
            "     23        \u001b[36m0.7597\u001b[0m  0.0287\n",
            "     24        \u001b[36m0.7509\u001b[0m  0.0297\n",
            "     25        \u001b[36m0.7240\u001b[0m  0.0369\n",
            "     26        \u001b[36m0.7144\u001b[0m  0.0222\n",
            "     27        \u001b[36m0.7075\u001b[0m  0.0209\n",
            "     28        \u001b[36m0.6956\u001b[0m  0.0244\n",
            "     29        \u001b[36m0.6880\u001b[0m  0.0225\n",
            "     30        \u001b[36m0.6817\u001b[0m  0.0216\n",
            "     31        \u001b[36m0.6758\u001b[0m  0.0224\n",
            "     32        \u001b[36m0.6648\u001b[0m  0.0204\n",
            "     33        \u001b[36m0.6633\u001b[0m  0.0206\n",
            "     34        \u001b[36m0.6504\u001b[0m  0.0208\n",
            "     35        \u001b[36m0.6327\u001b[0m  0.0192\n",
            "     36        \u001b[36m0.6275\u001b[0m  0.0270\n",
            "     37        \u001b[36m0.6221\u001b[0m  0.0226\n",
            "     38        \u001b[36m0.6195\u001b[0m  0.0187\n",
            "     39        \u001b[36m0.6146\u001b[0m  0.0240\n",
            "     40        \u001b[36m0.6085\u001b[0m  0.0236\n",
            "     41        \u001b[36m0.6045\u001b[0m  0.0220\n",
            "     42        \u001b[36m0.6008\u001b[0m  0.0181\n",
            "     43        \u001b[36m0.5973\u001b[0m  0.0198\n",
            "     44        \u001b[36m0.5940\u001b[0m  0.0152\n",
            "     45        \u001b[36m0.5909\u001b[0m  0.0251\n",
            "     46        \u001b[36m0.5879\u001b[0m  0.0222\n",
            "     47        \u001b[36m0.5851\u001b[0m  0.0254\n",
            "     48        \u001b[36m0.5746\u001b[0m  0.0240\n",
            "     49        \u001b[36m0.5721\u001b[0m  0.0282\n",
            "     50        \u001b[36m0.5697\u001b[0m  0.0219\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8669\u001b[0m  0.0126\n",
            "      2        \u001b[36m0.8636\u001b[0m  0.0203\n",
            "      3        0.8735  0.0163\n",
            "      4        \u001b[36m0.8547\u001b[0m  0.0179\n",
            "      5        \u001b[36m0.8537\u001b[0m  0.0152\n",
            "      6        \u001b[36m0.8527\u001b[0m  0.0277\n",
            "      7        \u001b[36m0.8517\u001b[0m  0.0179\n",
            "      8        \u001b[36m0.8507\u001b[0m  0.0222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.8497\u001b[0m  0.0255\n",
            "     10        \u001b[36m0.8463\u001b[0m  0.0189\n",
            "     11        \u001b[36m0.8443\u001b[0m  0.0175\n",
            "     12        \u001b[36m0.8433\u001b[0m  0.0178\n",
            "     13        \u001b[36m0.8424\u001b[0m  0.0271\n",
            "     14        \u001b[36m0.8415\u001b[0m  0.0167\n",
            "     15        \u001b[36m0.8406\u001b[0m  0.0140\n",
            "     16        \u001b[36m0.8397\u001b[0m  0.0147\n",
            "     17        \u001b[36m0.8388\u001b[0m  0.0173\n",
            "     18        \u001b[36m0.8379\u001b[0m  0.0189\n",
            "     19        \u001b[36m0.8370\u001b[0m  0.0149\n",
            "     20        \u001b[36m0.8362\u001b[0m  0.0161\n",
            "     21        \u001b[36m0.8353\u001b[0m  0.0168\n",
            "     22        \u001b[36m0.8345\u001b[0m  0.0204\n",
            "     23        \u001b[36m0.8337\u001b[0m  0.0180\n",
            "     24        \u001b[36m0.8328\u001b[0m  0.0175\n",
            "     25        \u001b[36m0.8320\u001b[0m  0.0202\n",
            "     26        \u001b[36m0.8312\u001b[0m  0.0182\n",
            "     27        \u001b[36m0.8304\u001b[0m  0.0170\n",
            "     28        \u001b[36m0.8296\u001b[0m  0.0165\n",
            "     29        \u001b[36m0.8288\u001b[0m  0.0189\n",
            "     30        \u001b[36m0.8280\u001b[0m  0.0164\n",
            "     31        \u001b[36m0.8272\u001b[0m  0.0171\n",
            "     32        \u001b[36m0.8265\u001b[0m  0.0178\n",
            "     33        \u001b[36m0.8257\u001b[0m  0.0138\n",
            "     34        \u001b[36m0.8249\u001b[0m  0.0166\n",
            "     35        \u001b[36m0.8242\u001b[0m  0.0158\n",
            "     36        \u001b[36m0.8234\u001b[0m  0.0177\n",
            "     37        \u001b[36m0.8227\u001b[0m  0.0166\n",
            "     38        \u001b[36m0.8219\u001b[0m  0.0155\n",
            "     39        \u001b[36m0.8212\u001b[0m  0.0235\n",
            "     40        \u001b[36m0.8205\u001b[0m  0.0184\n",
            "     41        \u001b[36m0.8197\u001b[0m  0.0163\n",
            "     42        \u001b[36m0.8190\u001b[0m  0.0164\n",
            "     43        \u001b[36m0.8183\u001b[0m  0.0174\n",
            "     44        \u001b[36m0.8176\u001b[0m  0.0171\n",
            "     45        \u001b[36m0.8169\u001b[0m  0.0217\n",
            "     46        \u001b[36m0.8162\u001b[0m  0.0287\n",
            "     47        \u001b[36m0.8155\u001b[0m  0.0228\n",
            "     48        \u001b[36m0.8148\u001b[0m  0.0147\n",
            "     49        \u001b[36m0.8141\u001b[0m  0.0183\n",
            "     50        \u001b[36m0.8134\u001b[0m  0.0217\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.7413\u001b[0m  0.0191\n",
            "      2        \u001b[36m1.6886\u001b[0m  0.0177\n",
            "      3        \u001b[36m1.6388\u001b[0m  0.0126\n",
            "      4        \u001b[36m1.5919\u001b[0m  0.0172\n",
            "      5        \u001b[36m1.5482\u001b[0m  0.0137\n",
            "      6        \u001b[36m1.5077\u001b[0m  0.0149\n",
            "      7        \u001b[36m1.4703\u001b[0m  0.0157\n",
            "      8        \u001b[36m1.4364\u001b[0m  0.0236\n",
            "      9        \u001b[36m1.4050\u001b[0m  0.0163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.3767\u001b[0m  0.0179\n",
            "     11        \u001b[36m1.3512\u001b[0m  0.0160\n",
            "     12        \u001b[36m1.3282\u001b[0m  0.0216\n",
            "     13        \u001b[36m1.3076\u001b[0m  0.0174\n",
            "     14        \u001b[36m1.2891\u001b[0m  0.0123\n",
            "     15        \u001b[36m1.2726\u001b[0m  0.0136\n",
            "     16        \u001b[36m1.2578\u001b[0m  0.0186\n",
            "     17        \u001b[36m1.2446\u001b[0m  0.0161\n",
            "     18        \u001b[36m1.2328\u001b[0m  0.0151\n",
            "     19        \u001b[36m1.2221\u001b[0m  0.0152\n",
            "     20        \u001b[36m1.2126\u001b[0m  0.0145\n",
            "     21        \u001b[36m1.2040\u001b[0m  0.0164\n",
            "     22        \u001b[36m1.1962\u001b[0m  0.0121\n",
            "     23        \u001b[36m1.1891\u001b[0m  0.0150\n",
            "     24        \u001b[36m1.1826\u001b[0m  0.0155\n",
            "     25        \u001b[36m1.1766\u001b[0m  0.0164\n",
            "     26        \u001b[36m1.1711\u001b[0m  0.0179\n",
            "     27        \u001b[36m1.1660\u001b[0m  0.0162\n",
            "     28        \u001b[36m1.1612\u001b[0m  0.0121\n",
            "     29        \u001b[36m1.1567\u001b[0m  0.0178\n",
            "     30        \u001b[36m1.1524\u001b[0m  0.0157\n",
            "     31        \u001b[36m1.1484\u001b[0m  0.0163\n",
            "     32        \u001b[36m1.1446\u001b[0m  0.0155\n",
            "     33        \u001b[36m1.1409\u001b[0m  0.0166\n",
            "     34        \u001b[36m1.1374\u001b[0m  0.0178\n",
            "     35        \u001b[36m1.1340\u001b[0m  0.0167\n",
            "     36        \u001b[36m1.1307\u001b[0m  0.0180\n",
            "     37        \u001b[36m1.1275\u001b[0m  0.0224\n",
            "     38        \u001b[36m1.1244\u001b[0m  0.0185\n",
            "     39        \u001b[36m1.1214\u001b[0m  0.0189\n",
            "     40        \u001b[36m1.1185\u001b[0m  0.0200\n",
            "     41        \u001b[36m1.1156\u001b[0m  0.0167\n",
            "     42        \u001b[36m1.1127\u001b[0m  0.0171\n",
            "     43        \u001b[36m1.1100\u001b[0m  0.0145\n",
            "     44        \u001b[36m1.1072\u001b[0m  0.0141\n",
            "     45        \u001b[36m1.1045\u001b[0m  0.0145\n",
            "     46        \u001b[36m1.1019\u001b[0m  0.0186\n",
            "     47        \u001b[36m1.0993\u001b[0m  0.0185\n",
            "     48        \u001b[36m1.0967\u001b[0m  0.0183\n",
            "     49        \u001b[36m1.0941\u001b[0m  0.0215\n",
            "     50        \u001b[36m1.0916\u001b[0m  0.0193\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0235\n",
            "      2       37.3239  0.0237\n",
            "      3       37.3239  0.0203\n",
            "      4       37.3239  0.0211\n",
            "      5       37.3239  0.0175\n",
            "      6       37.3239  0.0195\n",
            "      7       37.3239  0.0169\n",
            "      8       37.3239  0.0224\n",
            "      9       37.3239  0.0168\n",
            "     10       37.3239  0.0202\n",
            "     11       37.3239  0.0232\n",
            "     12       37.3239  0.0226\n",
            "     13       37.3239  0.0243\n",
            "     14       37.3239  0.0245\n",
            "     15       37.3239  0.0202\n",
            "     16       37.3239  0.0214\n",
            "     17       37.3239  0.0214\n",
            "     18       37.3239  0.0236\n",
            "     19       37.3239  0.0176\n",
            "     20       37.3239  0.0179\n",
            "     21       37.3239  0.0174\n",
            "     22       37.3239  0.0210\n",
            "     23       37.3239  0.0251\n",
            "     24       37.3239  0.0180\n",
            "     25       37.3239  0.0262\n",
            "     26       37.3239  0.0207\n",
            "     27       37.3239  0.0187\n",
            "     28       37.3239  0.0257\n",
            "     29       37.3239  0.0254\n",
            "     30       37.3239  0.0238\n",
            "     31       37.3239  0.0184\n",
            "     32       37.3239  0.0192\n",
            "     33       37.3239  0.0210\n",
            "     34       37.3239  0.0285\n",
            "     35       37.3239  0.0226\n",
            "     36       37.3239  0.0194\n",
            "     37       37.3239  0.0281\n",
            "     38       37.3239  0.0217\n",
            "     39       37.3239  0.0207\n",
            "     40       37.3239  0.0256\n",
            "     41       37.3239  0.0204\n",
            "     42       37.3239  0.0227\n",
            "     43       37.3239  0.0165\n",
            "     44       37.3239  0.0213\n",
            "     45       37.3239  0.0251\n",
            "     46       37.3239  0.0220\n",
            "     47       37.3239  0.0204\n",
            "     48       37.3239  0.0202\n",
            "     49       37.3239  0.0241\n",
            "     50       37.3239  0.0285\n",
            "     51       37.3239  0.0227\n",
            "     52       37.3239  0.0213\n",
            "     53       37.3239  0.0180\n",
            "     54       37.3239  0.0204\n",
            "     55       37.3239  0.0194\n",
            "     56       37.3239  0.0186\n",
            "     57       37.3239  0.0211\n",
            "     58       37.3239  0.0224\n",
            "     59       37.3239  0.0214\n",
            "     60       37.3239  0.0138\n",
            "     61       37.3239  0.0197\n",
            "     62       37.3239  0.0150\n",
            "     63       37.3239  0.0222\n",
            "     64       37.3239  0.0210\n",
            "     65       37.3239  0.0217\n",
            "     66       37.3239  0.0233\n",
            "     67       37.3239  0.0196\n",
            "     68       37.3239  0.0224\n",
            "     69       37.3239  0.0315\n",
            "     70       37.3239  0.0280\n",
            "     71       37.3239  0.0246\n",
            "     72       37.3239  0.0248\n",
            "     73       37.3239  0.0223\n",
            "     74       37.3239  0.0292\n",
            "     75       37.3239  0.0196\n",
            "     76       37.3239  0.0254\n",
            "     77       37.3239  0.0237\n",
            "     78       37.3239  0.0195\n",
            "     79       37.3239  0.0231\n",
            "     80       37.3239  0.0167\n",
            "     81       37.3239  0.0199\n",
            "     82       37.3239  0.0251\n",
            "     83       37.3239  0.0243\n",
            "     84       37.3239  0.0250\n",
            "     85       37.3239  0.0188\n",
            "     86       37.3239  0.0217\n",
            "     87       37.3239  0.0211\n",
            "     88       37.3239  0.0173\n",
            "     89       37.3239  0.0252\n",
            "     90       37.3239  0.0163\n",
            "     91       37.3239  0.0217\n",
            "     92       37.3239  0.0281\n",
            "     93       37.3239  0.0188\n",
            "     94       37.3239  0.0206\n",
            "     95       37.3239  0.0158\n",
            "     96       37.3239  0.0205\n",
            "     97       37.3239  0.0225\n",
            "     98       37.3239  0.0230\n",
            "     99       37.3239  0.0145\n",
            "    100       37.3239  0.0187\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0212\n",
            "      2       37.1930  0.0201\n",
            "      3       37.1930  0.0208\n",
            "      4       37.1930  0.0190\n",
            "      5       37.1930  0.0186\n",
            "      6       37.1930  0.0205\n",
            "      7       37.1930  0.0219\n",
            "      8       37.1930  0.0205\n",
            "      9       37.1930  0.0346\n",
            "     10       37.1930  0.0239\n",
            "     11       37.1930  0.0242\n",
            "     12       37.1930  0.0246\n",
            "     13       37.1930  0.0269\n",
            "     14       37.1930  0.0288\n",
            "     15       37.1930  0.0250\n",
            "     16       37.1930  0.0227\n",
            "     17       37.1930  0.0173\n",
            "     18       37.1930  0.0216\n",
            "     19       37.1930  0.0272\n",
            "     20       37.1930  0.0220\n",
            "     21       37.1930  0.0223\n",
            "     22       37.1930  0.0173\n",
            "     23       37.1930  0.0173\n",
            "     24       37.1930  0.0191\n",
            "     25       37.1930  0.0207\n",
            "     26       37.1930  0.0204\n",
            "     27       37.1930  0.0211\n",
            "     28       37.1930  0.0232\n",
            "     29       37.1930  0.0210\n",
            "     30       37.1930  0.0269\n",
            "     31       37.1930  0.0185\n",
            "     32       37.1930  0.0182\n",
            "     33       37.1930  0.0195\n",
            "     34       37.1930  0.0214\n",
            "     35       37.1930  0.0201\n",
            "     36       37.1930  0.0202\n",
            "     37       37.1930  0.0190\n",
            "     38       37.1930  0.0193\n",
            "     39       37.1930  0.0201\n",
            "     40       37.1930  0.0191\n",
            "     41       37.1930  0.0210\n",
            "     42       37.1930  0.0194\n",
            "     43       37.1930  0.0205\n",
            "     44       37.1930  0.0181\n",
            "     45       37.1930  0.0212\n",
            "     46       37.1930  0.0200\n",
            "     47       37.1930  0.0191\n",
            "     48       37.1930  0.0168\n",
            "     49       37.1930  0.0214\n",
            "     50       37.1930  0.0217\n",
            "     51       37.1930  0.0252\n",
            "     52       37.1930  0.0260\n",
            "     53       37.1930  0.0255\n",
            "     54       37.1930  0.0215\n",
            "     55       37.1930  0.0223\n",
            "     56       37.1930  0.0218\n",
            "     57       37.1930  0.0208\n",
            "     58       37.1930  0.0213\n",
            "     59       37.1930  0.0241\n",
            "     60       37.1930  0.0234\n",
            "     61       37.1930  0.0149\n",
            "     62       37.1930  0.0240\n",
            "     63       37.1930  0.0182\n",
            "     64       37.1930  0.0204\n",
            "     65       37.1930  0.0152\n",
            "     66       37.1930  0.0181\n",
            "     67       37.1930  0.0223\n",
            "     68       37.1930  0.0185\n",
            "     69       37.1930  0.0222\n",
            "     70       37.1930  0.0193\n",
            "     71       37.1930  0.0262\n",
            "     72       37.1930  0.0155\n",
            "     73       37.1930  0.0183\n",
            "     74       37.1930  0.0174\n",
            "     75       37.1930  0.0218\n",
            "     76       37.1930  0.0204\n",
            "     77       37.1930  0.0180\n",
            "     78       37.1930  0.0208\n",
            "     79       37.1930  0.0163\n",
            "     80       37.1930  0.0209\n",
            "     81       37.1930  0.0189\n",
            "     82       37.1930  0.0223\n",
            "     83       37.1930  0.0178\n",
            "     84       37.1930  0.0204\n",
            "     85       37.1930  0.0226\n",
            "     86       37.1930  0.0184\n",
            "     87       37.1930  0.0231\n",
            "     88       37.1930  0.0202\n",
            "     89       37.1930  0.0164\n",
            "     90       37.1930  0.0268\n",
            "     91       37.1930  0.0255\n",
            "     92       37.1930  0.0292\n",
            "     93       37.1930  0.0237\n",
            "     94       37.1930  0.0267\n",
            "     95       37.1930  0.0197\n",
            "     96       37.1930  0.0192\n",
            "     97       37.1930  0.0229\n",
            "     98       37.1930  0.0206\n",
            "     99       37.1930  0.0200\n",
            "    100       37.1930  0.0197\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0126\n",
            "      2       37.3239  0.0168\n",
            "      3       37.3239  0.0160\n",
            "      4       37.3239  0.0234\n",
            "      5       37.3239  0.0158\n",
            "      6       37.3239  0.0153\n",
            "      7       37.3239  0.0140\n",
            "      8       37.3239  0.0152\n",
            "      9       37.3239  0.0128\n",
            "     10       37.3239  0.0211\n",
            "     11       37.3239  0.0117\n",
            "     12       37.3239  0.0150\n",
            "     13       37.3239  0.0156\n",
            "     14       37.3239  0.0156\n",
            "     15       37.3239  0.0175\n",
            "     16       37.3239  0.0147\n",
            "     17       37.3239  0.0174\n",
            "     18       37.3239  0.0155\n",
            "     19       37.3239  0.0173\n",
            "     20       37.3239  0.0151\n",
            "     21       37.3239  0.0156\n",
            "     22       37.3239  0.0151\n",
            "     23       37.3239  0.0130\n",
            "     24       37.3239  0.0196\n",
            "     25       37.3239  0.0182\n",
            "     26       37.3239  0.0171\n",
            "     27       37.3239  0.0165\n",
            "     28       37.3239  0.0170\n",
            "     29       37.3239  0.0148\n",
            "     30       37.3239  0.0182\n",
            "     31       37.3239  0.0140\n",
            "     32       37.3239  0.0216\n",
            "     33       37.3239  0.0161\n",
            "     34       37.3239  0.0188\n",
            "     35       37.3239  0.0177\n",
            "     36       37.3239  0.0179\n",
            "     37       37.3239  0.0187\n",
            "     38       37.3239  0.0207\n",
            "     39       37.3239  0.0217\n",
            "     40       37.3239  0.0132\n",
            "     41       37.3239  0.0254\n",
            "     42       37.3239  0.0181\n",
            "     43       37.3239  0.0182\n",
            "     44       37.3239  0.0210\n",
            "     45       37.3239  0.0116\n",
            "     46       37.3239  0.0243\n",
            "     47       37.3239  0.0141\n",
            "     48       37.3239  0.0176\n",
            "     49       37.3239  0.0139\n",
            "     50       37.3239  0.0202\n",
            "     51       37.3239  0.0120\n",
            "     52       37.3239  0.0298\n",
            "     53       37.3239  0.0187\n",
            "     54       37.3239  0.0154\n",
            "     55       37.3239  0.0211\n",
            "     56       37.3239  0.0160\n",
            "     57       37.3239  0.0170\n",
            "     58       37.3239  0.0146\n",
            "     59       37.3239  0.0129\n",
            "     60       37.3239  0.0183\n",
            "     61       37.3239  0.0156\n",
            "     62       37.3239  0.0164\n",
            "     63       37.3239  0.0126\n",
            "     64       37.3239  0.0185\n",
            "     65       37.3239  0.0116\n",
            "     66       37.3239  0.0185\n",
            "     67       37.3239  0.0166\n",
            "     68       37.3239  0.0172\n",
            "     69       37.3239  0.0174\n",
            "     70       37.3239  0.0205\n",
            "     71       37.3239  0.0132\n",
            "     72       37.3239  0.0183\n",
            "     73       37.3239  0.0165\n",
            "     74       37.3239  0.0188\n",
            "     75       37.3239  0.0159\n",
            "     76       37.3239  0.0176\n",
            "     77       37.3239  0.0150\n",
            "     78       37.3239  0.0194\n",
            "     79       37.3239  0.0147\n",
            "     80       37.3239  0.0212\n",
            "     81       37.3239  0.0169\n",
            "     82       37.3239  0.0203\n",
            "     83       37.3239  0.0197\n",
            "     84       37.3239  0.0242\n",
            "     85       37.3239  0.0185\n",
            "     86       37.3239  0.0203\n",
            "     87       37.3239  0.0273\n",
            "     88       37.3239  0.0281\n",
            "     89       37.3239  0.0171\n",
            "     90       37.3239  0.0218\n",
            "     91       37.3239  0.0139\n",
            "     92       37.3239  0.0187\n",
            "     93       37.3239  0.0174\n",
            "     94       37.3239  0.0208\n",
            "     95       37.3239  0.0142\n",
            "     96       37.3239  0.0160\n",
            "     97       37.3239  0.0179\n",
            "     98       37.3239  0.0215\n",
            "     99       37.3239  0.0343\n",
            "    100       37.3239  0.0188\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0157\n",
            "      2       37.1930  0.0154\n",
            "      3       37.1930  0.0131\n",
            "      4       37.1930  0.0160\n",
            "      5       37.1930  0.0161\n",
            "      6       37.1930  0.0172\n",
            "      7       37.1930  0.0287\n",
            "      8       37.1930  0.0185\n",
            "      9       37.1930  0.0171\n",
            "     10       37.1930  0.0174\n",
            "     11       37.1930  0.0143\n",
            "     12       37.1930  0.0154\n",
            "     13       37.1930  0.0206\n",
            "     14       37.1930  0.0163\n",
            "     15       37.1930  0.0149\n",
            "     16       37.1930  0.0186\n",
            "     17       37.1930  0.0179\n",
            "     18       37.1930  0.0177\n",
            "     19       37.1930  0.0205\n",
            "     20       37.1930  0.0190\n",
            "     21       37.1930  0.0210\n",
            "     22       37.1930  0.0168\n",
            "     23       37.1930  0.0170\n",
            "     24       37.1930  0.0155\n",
            "     25       37.1930  0.0264\n",
            "     26       37.1930  0.0194\n",
            "     27       37.1930  0.0187\n",
            "     28       37.1930  0.0221\n",
            "     29       37.1930  0.0243\n",
            "     30       37.1930  0.0223\n",
            "     31       37.1930  0.0227\n",
            "     32       37.1930  0.0235\n",
            "     33       37.1930  0.0219\n",
            "     34       37.1930  0.0143\n",
            "     35       37.1930  0.0196\n",
            "     36       37.1930  0.0166\n",
            "     37       37.1930  0.0173\n",
            "     38       37.1930  0.0224\n",
            "     39       37.1930  0.0177\n",
            "     40       37.1930  0.0209\n",
            "     41       37.1930  0.0147\n",
            "     42       37.1930  0.0157\n",
            "     43       37.1930  0.0129\n",
            "     44       37.1930  0.0180\n",
            "     45       37.1930  0.0156\n",
            "     46       37.1930  0.0200\n",
            "     47       37.1930  0.0228\n",
            "     48       37.1930  0.0140\n",
            "     49       37.1930  0.0151\n",
            "     50       37.1930  0.0188\n",
            "     51       37.1930  0.0151\n",
            "     52       37.1930  0.0221\n",
            "     53       37.1930  0.0169\n",
            "     54       37.1930  0.0160\n",
            "     55       37.1930  0.0158\n",
            "     56       37.1930  0.0162\n",
            "     57       37.1930  0.0189\n",
            "     58       37.1930  0.0175\n",
            "     59       37.1930  0.0260\n",
            "     60       37.1930  0.0200\n",
            "     61       37.1930  0.0236\n",
            "     62       37.1930  0.0137\n",
            "     63       37.1930  0.0153\n",
            "     64       37.1930  0.0149\n",
            "     65       37.1930  0.0262\n",
            "     66       37.1930  0.0182\n",
            "     67       37.1930  0.0202\n",
            "     68       37.1930  0.0233\n",
            "     69       37.1930  0.0280\n",
            "     70       37.1930  0.0213\n",
            "     71       37.1930  0.0189\n",
            "     72       37.1930  0.0173\n",
            "     73       37.1930  0.0185\n",
            "     74       37.1930  0.0218\n",
            "     75       37.1930  0.0173\n",
            "     76       37.1930  0.0277\n",
            "     77       37.1930  0.0232\n",
            "     78       37.1930  0.0129\n",
            "     79       37.1930  0.0134\n",
            "     80       37.1930  0.0203\n",
            "     81       37.1930  0.0183\n",
            "     82       37.1930  0.0233\n",
            "     83       37.1930  0.0163\n",
            "     84       37.1930  0.0142\n",
            "     85       37.1930  0.0160\n",
            "     86       37.1930  0.0196\n",
            "     87       37.1930  0.0246\n",
            "     88       37.1930  0.0125\n",
            "     89       37.1930  0.0146\n",
            "     90       37.1930  0.0168\n",
            "     91       37.1930  0.0209\n",
            "     92       37.1930  0.0208\n",
            "     93       37.1930  0.0173\n",
            "     94       37.1930  0.0174\n",
            "     95       37.1930  0.0110\n",
            "     96       37.1930  0.0216\n",
            "     97       37.1930  0.0156\n",
            "     98       37.1930  0.0177\n",
            "     99       37.1930  0.0208\n",
            "    100       37.1930  0.0197\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0244\n",
            "      2       37.3239  0.0217\n",
            "      3       37.3239  0.0241\n",
            "      4       37.3239  0.0223\n",
            "      5       37.3239  0.0153\n",
            "      6       37.3239  0.0194\n",
            "      7       37.3239  0.0157\n",
            "      8       37.3239  0.0275\n",
            "      9       37.3239  0.0316\n",
            "     10       37.3239  0.0265\n",
            "     11       37.3239  0.0299\n",
            "     12       37.3239  0.0218\n",
            "     13       37.3239  0.0230\n",
            "     14       37.3239  0.0207\n",
            "     15       37.3239  0.0260\n",
            "     16       37.3239  0.0212\n",
            "     17       37.3239  0.0322\n",
            "     18       37.3239  0.0211\n",
            "     19       37.3239  0.0173\n",
            "     20       37.3239  0.0220\n",
            "     21       37.3239  0.0157\n",
            "     22       37.3239  0.0295\n",
            "     23       37.3239  0.0217\n",
            "     24       37.3239  0.0216\n",
            "     25       37.3239  0.0208\n",
            "     26       37.3239  0.0215\n",
            "     27       37.3239  0.0184\n",
            "     28       37.3239  0.0219\n",
            "     29       37.3239  0.0204\n",
            "     30       37.3239  0.0214\n",
            "     31       37.3239  0.0212\n",
            "     32       37.3239  0.0221\n",
            "     33       37.3239  0.0215\n",
            "     34       37.3239  0.0209\n",
            "     35       37.3239  0.0218\n",
            "     36       37.3239  0.0230\n",
            "     37       37.3239  0.0240\n",
            "     38       37.3239  0.0237\n",
            "     39       37.3239  0.0192\n",
            "     40       37.3239  0.0192\n",
            "     41       37.3239  0.0248\n",
            "     42       37.3239  0.0189\n",
            "     43       37.3239  0.0244\n",
            "     44       37.3239  0.0227\n",
            "     45       37.3239  0.0173\n",
            "     46       37.3239  0.0309\n",
            "     47       37.3239  0.0317\n",
            "     48       37.3239  0.0275\n",
            "     49       37.3239  0.0304\n",
            "     50       37.3239  0.0257\n",
            "     51       37.3239  0.0247\n",
            "     52       37.3239  0.0246\n",
            "     53       37.3239  0.0253\n",
            "     54       37.3239  0.0200\n",
            "     55       37.3239  0.0303\n",
            "     56       37.3239  0.0265\n",
            "     57       37.3239  0.0196\n",
            "     58       37.3239  0.0220\n",
            "     59       37.3239  0.0206\n",
            "     60       37.3239  0.0198\n",
            "     61       37.3239  0.0172\n",
            "     62       37.3239  0.0217\n",
            "     63       37.3239  0.0172\n",
            "     64       37.3239  0.0214\n",
            "     65       37.3239  0.0197\n",
            "     66       37.3239  0.0217\n",
            "     67       37.3239  0.0236\n",
            "     68       37.3239  0.0216\n",
            "     69       37.3239  0.0228\n",
            "     70       37.3239  0.0226\n",
            "     71       37.3239  0.0222\n",
            "     72       37.3239  0.0280\n",
            "     73       37.3239  0.0264\n",
            "     74       37.3239  0.0321\n",
            "     75       37.3239  0.0201\n",
            "     76       37.3239  0.0239\n",
            "     77       37.3239  0.0229\n",
            "     78       37.3239  0.0237\n",
            "     79       37.3239  0.0186\n",
            "     80       37.3239  0.0216\n",
            "     81       37.3239  0.0229\n",
            "     82       37.3239  0.0244\n",
            "     83       37.3239  0.0259\n",
            "     84       37.3239  0.0197\n",
            "     85       37.3239  0.0298\n",
            "     86       37.3239  0.0258\n",
            "     87       37.3239  0.0223\n",
            "     88       37.3239  0.0218\n",
            "     89       37.3239  0.0254\n",
            "     90       37.3239  0.0244\n",
            "     91       37.3239  0.0221\n",
            "     92       37.3239  0.0172\n",
            "     93       37.3239  0.0212\n",
            "     94       37.3239  0.0266\n",
            "     95       37.3239  0.0231\n",
            "     96       37.3239  0.0218\n",
            "     97       37.3239  0.0207\n",
            "     98       37.3239  0.0172\n",
            "     99       37.3239  0.0165\n",
            "    100       37.3239  0.0230\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0200\n",
            "      2       37.1930  0.0190\n",
            "      3       37.1930  0.0246\n",
            "      4       37.1930  0.0217\n",
            "      5       37.1930  0.0148\n",
            "      6       37.1930  0.0195\n",
            "      7       37.1930  0.0145\n",
            "      8       37.1930  0.0216\n",
            "      9       37.1930  0.0240\n",
            "     10       37.1930  0.0260\n",
            "     11       37.1930  0.0233\n",
            "     12       37.1930  0.0198\n",
            "     13       37.1930  0.0192\n",
            "     14       37.1930  0.0250\n",
            "     15       37.1930  0.0186\n",
            "     16       37.1930  0.0246\n",
            "     17       37.1930  0.0226\n",
            "     18       37.1930  0.0207\n",
            "     19       37.1930  0.0222\n",
            "     20       37.1930  0.0239\n",
            "     21       37.1930  0.0275\n",
            "     22       37.1930  0.0253\n",
            "     23       37.1930  0.0198\n",
            "     24       37.1930  0.0228\n",
            "     25       37.1930  0.0226\n",
            "     26       37.1930  0.0270\n",
            "     27       37.1930  0.0263\n",
            "     28       37.1930  0.0236\n",
            "     29       37.1930  0.0166\n",
            "     30       37.1930  0.0204\n",
            "     31       37.1930  0.0173\n",
            "     32       37.1930  0.0200\n",
            "     33       37.1930  0.0223\n",
            "     34       37.1930  0.0273\n",
            "     35       37.1930  0.0212\n",
            "     36       37.1930  0.0189\n",
            "     37       37.1930  0.0195\n",
            "     38       37.1930  0.0216\n",
            "     39       37.1930  0.0211\n",
            "     40       37.1930  0.0232\n",
            "     41       37.1930  0.0205\n",
            "     42       37.1930  0.0214\n",
            "     43       37.1930  0.0183\n",
            "     44       37.1930  0.0202\n",
            "     45       37.1930  0.0165\n",
            "     46       37.1930  0.0226\n",
            "     47       37.1930  0.0221\n",
            "     48       37.1930  0.0182\n",
            "     49       37.1930  0.0267\n",
            "     50       37.1930  0.0173\n",
            "     51       37.1930  0.0199\n",
            "     52       37.1930  0.0208\n",
            "     53       37.1930  0.0227\n",
            "     54       37.1930  0.0232\n",
            "     55       37.1930  0.0188\n",
            "     56       37.1930  0.0198\n",
            "     57       37.1930  0.0196\n",
            "     58       37.1930  0.0209\n",
            "     59       37.1930  0.0256\n",
            "     60       37.1930  0.0218\n",
            "     61       37.1930  0.0283\n",
            "     62       37.1930  0.0228\n",
            "     63       37.1930  0.0200\n",
            "     64       37.1930  0.0201\n",
            "     65       37.1930  0.0211\n",
            "     66       37.1930  0.0165\n",
            "     67       37.1930  0.0217\n",
            "     68       37.1930  0.0156\n",
            "     69       37.1930  0.0250\n",
            "     70       37.1930  0.0163\n",
            "     71       37.1930  0.0256\n",
            "     72       37.1930  0.0235\n",
            "     73       37.1930  0.0147\n",
            "     74       37.1930  0.0266\n",
            "     75       37.1930  0.0287\n",
            "     76       37.1930  0.0208\n",
            "     77       37.1930  0.0203\n",
            "     78       37.1930  0.0199\n",
            "     79       37.1930  0.0201\n",
            "     80       37.1930  0.0202\n",
            "     81       37.1930  0.0159\n",
            "     82       37.1930  0.0253\n",
            "     83       37.1930  0.0215\n",
            "     84       37.1930  0.0237\n",
            "     85       37.1930  0.0202\n",
            "     86       37.1930  0.0151\n",
            "     87       37.1930  0.0196\n",
            "     88       37.1930  0.0177\n",
            "     89       37.1930  0.0221\n",
            "     90       37.1930  0.0200\n",
            "     91       37.1930  0.0233\n",
            "     92       37.1930  0.0216\n",
            "     93       37.1930  0.0232\n",
            "     94       37.1930  0.0190\n",
            "     95       37.1930  0.0239\n",
            "     96       37.1930  0.0261\n",
            "     97       37.1930  0.0205\n",
            "     98       37.1930  0.0192\n",
            "     99       37.1930  0.0242\n",
            "    100       37.1930  0.0251\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3239\u001b[0m  0.0221\n",
            "      2       37.3239  0.0203\n",
            "      3       37.3239  0.0185\n",
            "      4       37.3239  0.0207\n",
            "      5       37.3239  0.0148\n",
            "      6       37.3239  0.0237\n",
            "      7       37.3239  0.0159\n",
            "      8       37.3239  0.0228\n",
            "      9       37.3239  0.0233\n",
            "     10       37.3239  0.0142\n",
            "     11       37.3239  0.0164\n",
            "     12       37.3239  0.0154\n",
            "     13       37.3239  0.0163\n",
            "     14       37.3239  0.0119\n",
            "     15       37.3239  0.0175\n",
            "     16       37.3239  0.0170\n",
            "     17       37.3239  0.0191\n",
            "     18       37.3239  0.0203\n",
            "     19       37.3239  0.0178\n",
            "     20       37.3239  0.0155\n",
            "     21       37.3239  0.0163\n",
            "     22       37.3239  0.0135\n",
            "     23       37.3239  0.0148\n",
            "     24       37.3239  0.0143\n",
            "     25       37.3239  0.0183\n",
            "     26       37.3239  0.0155\n",
            "     27       37.3239  0.0152\n",
            "     28       37.3239  0.0199\n",
            "     29       37.3239  0.0155\n",
            "     30       37.3239  0.0152\n",
            "     31       37.3239  0.0180\n",
            "     32       37.3239  0.0149\n",
            "     33       37.3239  0.0154\n",
            "     34       37.3239  0.0151\n",
            "     35       37.3239  0.0148\n",
            "     36       37.3239  0.0169\n",
            "     37       37.3239  0.0277\n",
            "     38       37.3239  0.0189\n",
            "     39       37.3239  0.0176\n",
            "     40       37.3239  0.0181\n",
            "     41       37.3239  0.0206\n",
            "     42       37.3239  0.0174\n",
            "     43       37.3239  0.0178\n",
            "     44       37.3239  0.0188\n",
            "     45       37.3239  0.0176\n",
            "     46       37.3239  0.0165\n",
            "     47       37.3239  0.0178\n",
            "     48       37.3239  0.0165\n",
            "     49       37.3239  0.0211\n",
            "     50       37.3239  0.0165\n",
            "     51       37.3239  0.0162\n",
            "     52       37.3239  0.0151\n",
            "     53       37.3239  0.0147\n",
            "     54       37.3239  0.0195\n",
            "     55       37.3239  0.0165\n",
            "     56       37.3239  0.0191\n",
            "     57       37.3239  0.0168\n",
            "     58       37.3239  0.0160\n",
            "     59       37.3239  0.0158\n",
            "     60       37.3239  0.0156\n",
            "     61       37.3239  0.0160\n",
            "     62       37.3239  0.0157\n",
            "     63       37.3239  0.0163\n",
            "     64       37.3239  0.0160\n",
            "     65       37.3239  0.0168\n",
            "     66       37.3239  0.0175\n",
            "     67       37.3239  0.0156\n",
            "     68       37.3239  0.0157\n",
            "     69       37.3239  0.0147\n",
            "     70       37.3239  0.0165\n",
            "     71       37.3239  0.0155\n",
            "     72       37.3239  0.0158\n",
            "     73       37.3239  0.0158\n",
            "     74       37.3239  0.0155\n",
            "     75       37.3239  0.0187\n",
            "     76       37.3239  0.0176\n",
            "     77       37.3239  0.0160\n",
            "     78       37.3239  0.0185\n",
            "     79       37.3239  0.0217\n",
            "     80       37.3239  0.0155\n",
            "     81       37.3239  0.0154\n",
            "     82       37.3239  0.0156\n",
            "     83       37.3239  0.0175\n",
            "     84       37.3239  0.0163\n",
            "     85       37.3239  0.0174\n",
            "     86       37.3239  0.0171\n",
            "     87       37.3239  0.0168\n",
            "     88       37.3239  0.0165\n",
            "     89       37.3239  0.0172\n",
            "     90       37.3239  0.0168\n",
            "     91       37.3239  0.0195\n",
            "     92       37.3239  0.0194\n",
            "     93       37.3239  0.0187\n",
            "     94       37.3239  0.0192\n",
            "     95       37.3239  0.0184\n",
            "     96       37.3239  0.0175\n",
            "     97       37.3239  0.0204\n",
            "     98       37.3239  0.0193\n",
            "     99       37.3239  0.0189\n",
            "    100       37.3239  0.0199\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0165\n",
            "      2       37.1930  0.0160\n",
            "      3       37.1930  0.0155\n",
            "      4       37.1930  0.0147\n",
            "      5       37.1930  0.0154\n",
            "      6       37.1930  0.0200\n",
            "      7       37.1930  0.0132\n",
            "      8       37.1930  0.0185\n",
            "      9       37.1930  0.0143\n",
            "     10       37.1930  0.0166\n",
            "     11       37.1930  0.0160\n",
            "     12       37.1930  0.0174\n",
            "     13       37.1930  0.0190\n",
            "     14       37.1930  0.0160\n",
            "     15       37.1930  0.0156\n",
            "     16       37.1930  0.0189\n",
            "     17       37.1930  0.0163\n",
            "     18       37.1930  0.0168\n",
            "     19       37.1930  0.0168\n",
            "     20       37.1930  0.0161\n",
            "     21       37.1930  0.0167\n",
            "     22       37.1930  0.0171\n",
            "     23       37.1930  0.0177\n",
            "     24       37.1930  0.0175\n",
            "     25       37.1930  0.0215\n",
            "     26       37.1930  0.0171\n",
            "     27       37.1930  0.0169\n",
            "     28       37.1930  0.0166\n",
            "     29       37.1930  0.0175\n",
            "     30       37.1930  0.0204\n",
            "     31       37.1930  0.0208\n",
            "     32       37.1930  0.0221\n",
            "     33       37.1930  0.0197\n",
            "     34       37.1930  0.0256\n",
            "     35       37.1930  0.0248\n",
            "     36       37.1930  0.0201\n",
            "     37       37.1930  0.0237\n",
            "     38       37.1930  0.0206\n",
            "     39       37.1930  0.0197\n",
            "     40       37.1930  0.0181\n",
            "     41       37.1930  0.0141\n",
            "     42       37.1930  0.0235\n",
            "     43       37.1930  0.0198\n",
            "     44       37.1930  0.0206\n",
            "     45       37.1930  0.0174\n",
            "     46       37.1930  0.0212\n",
            "     47       37.1930  0.0156\n",
            "     48       37.1930  0.0111\n",
            "     49       37.1930  0.0178\n",
            "     50       37.1930  0.0158\n",
            "     51       37.1930  0.0183\n",
            "     52       37.1930  0.0166\n",
            "     53       37.1930  0.0153\n",
            "     54       37.1930  0.0170\n",
            "     55       37.1930  0.0150\n",
            "     56       37.1930  0.0154\n",
            "     57       37.1930  0.0145\n",
            "     58       37.1930  0.0231\n",
            "     59       37.1930  0.0234\n",
            "     60       37.1930  0.0150\n",
            "     61       37.1930  0.0156\n",
            "     62       37.1930  0.0152\n",
            "     63       37.1930  0.0169\n",
            "     64       37.1930  0.0180\n",
            "     65       37.1930  0.0182\n",
            "     66       37.1930  0.0199\n",
            "     67       37.1930  0.0157\n",
            "     68       37.1930  0.0156\n",
            "     69       37.1930  0.0146\n",
            "     70       37.1930  0.0150\n",
            "     71       37.1930  0.0156\n",
            "     72       37.1930  0.0190\n",
            "     73       37.1930  0.0131\n",
            "     74       37.1930  0.0152\n",
            "     75       37.1930  0.0154\n",
            "     76       37.1930  0.0161\n",
            "     77       37.1930  0.0139\n",
            "     78       37.1930  0.0167\n",
            "     79       37.1930  0.0159\n",
            "     80       37.1930  0.0179\n",
            "     81       37.1930  0.0182\n",
            "     82       37.1930  0.0176\n",
            "     83       37.1930  0.0182\n",
            "     84       37.1930  0.0173\n",
            "     85       37.1930  0.0189\n",
            "     86       37.1930  0.0175\n",
            "     87       37.1930  0.0181\n",
            "     88       37.1930  0.0171\n",
            "     89       37.1930  0.0163\n",
            "     90       37.1930  0.0165\n",
            "     91       37.1930  0.0155\n",
            "     92       37.1930  0.0159\n",
            "     93       37.1930  0.0159\n",
            "     94       37.1930  0.0158\n",
            "     95       37.1930  0.0150\n",
            "     96       37.1930  0.0158\n",
            "     97       37.1930  0.0155\n",
            "     98       37.1930  0.0182\n",
            "     99       37.1930  0.0154\n",
            "    100       37.1930  0.0154\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m59.1549\u001b[0m  0.0197\n",
            "      2       59.1549  0.0293\n",
            "      3       59.1549  0.0233\n",
            "      4       59.1549  0.0286\n",
            "      5       59.1549  0.0221\n",
            "      6       59.1549  0.0213\n",
            "      7       59.1549  0.0225\n",
            "      8       59.1549  0.0212\n",
            "      9       59.1549  0.0212\n",
            "     10       59.1549  0.0213\n",
            "     11       59.1549  0.0249\n",
            "     12       59.1549  0.0216\n",
            "     13       \u001b[36m59.0881\u001b[0m  0.0151\n",
            "     14       \u001b[36m59.0472\u001b[0m  0.0226\n",
            "     15       59.0483  0.0243\n",
            "     16       59.0475  0.0239\n",
            "     17       \u001b[36m59.0450\u001b[0m  0.0244\n",
            "     18       \u001b[36m58.9939\u001b[0m  0.0289\n",
            "     19       \u001b[36m58.9323\u001b[0m  0.0266\n",
            "     20       \u001b[36m58.8929\u001b[0m  0.0188\n",
            "     21       \u001b[36m58.8025\u001b[0m  0.0190\n",
            "     22       \u001b[36m58.7483\u001b[0m  0.0236\n",
            "     23       \u001b[36m58.6986\u001b[0m  0.0248\n",
            "     24       \u001b[36m58.6695\u001b[0m  0.0255\n",
            "     25       \u001b[36m58.5688\u001b[0m  0.0224\n",
            "     26       58.5761  0.0175\n",
            "     27       \u001b[36m58.4889\u001b[0m  0.0257\n",
            "     28       \u001b[36m58.2178\u001b[0m  0.0179\n",
            "     29       \u001b[36m55.1569\u001b[0m  0.0217\n",
            "     30       56.2525  0.0281\n",
            "     31       \u001b[36m48.1751\u001b[0m  0.0167\n",
            "     32       \u001b[36m38.3359\u001b[0m  0.0213\n",
            "     33       39.9390  0.0233\n",
            "     34       42.4455  0.0210\n",
            "     35       \u001b[36m38.0209\u001b[0m  0.0220\n",
            "     36       \u001b[36m35.5221\u001b[0m  0.0194\n",
            "     37       37.4729  0.0173\n",
            "     38       38.6287  0.0212\n",
            "     39       35.8949  0.0228\n",
            "     40       36.8103  0.0241\n",
            "     41       36.6293  0.0180\n",
            "     42       36.8792  0.0164\n",
            "     43       36.8159  0.0178\n",
            "     44       35.9694  0.0220\n",
            "     45       36.3251  0.0244\n",
            "     46       36.3697  0.0258\n",
            "     47       36.1493  0.0201\n",
            "     48       35.6384  0.0181\n",
            "     49       36.0011  0.0252\n",
            "     50       \u001b[36m35.4314\u001b[0m  0.0161\n",
            "     51       35.9773  0.0232\n",
            "     52       36.0223  0.0206\n",
            "     53       36.0700  0.0239\n",
            "     54       35.6222  0.0200\n",
            "     55       35.7622  0.0241\n",
            "     56       35.8492  0.0222\n",
            "     57       \u001b[36m34.9221\u001b[0m  0.0292\n",
            "     58       37.0562  0.0292\n",
            "     59       35.8655  0.0268\n",
            "     60       35.5326  0.0307\n",
            "     61       35.6925  0.0242\n",
            "     62       35.5872  0.0230\n",
            "     63       35.4738  0.0224\n",
            "     64       35.7053  0.0232\n",
            "     65       35.7954  0.0206\n",
            "     66       \u001b[36m34.8430\u001b[0m  0.0254\n",
            "     67       35.1691  0.0184\n",
            "     68       \u001b[36m34.7432\u001b[0m  0.0172\n",
            "     69       34.7756  0.0186\n",
            "     70       34.9953  0.0243\n",
            "     71       34.8057  0.0198\n",
            "     72       \u001b[36m34.6109\u001b[0m  0.0206\n",
            "     73       \u001b[36m34.5538\u001b[0m  0.0183\n",
            "     74       34.5977  0.0165\n",
            "     75       \u001b[36m34.5115\u001b[0m  0.0212\n",
            "     76       34.6813  0.0197\n",
            "     77       34.6093  0.0218\n",
            "     78       34.6272  0.0205\n",
            "     79       \u001b[36m34.4062\u001b[0m  0.0173\n",
            "     80       34.5108  0.0184\n",
            "     81       \u001b[36m33.8409\u001b[0m  0.0201\n",
            "     82       \u001b[36m33.1608\u001b[0m  0.0233\n",
            "     83       \u001b[36m31.8574\u001b[0m  0.0192\n",
            "     84       31.8630  0.0204\n",
            "     85       \u001b[36m28.6618\u001b[0m  0.0228\n",
            "     86       28.9540  0.0206\n",
            "     87       \u001b[36m26.7479\u001b[0m  0.0284\n",
            "     88       \u001b[36m24.4943\u001b[0m  0.0219\n",
            "     89       \u001b[36m23.5319\u001b[0m  0.0215\n",
            "     90       \u001b[36m22.5556\u001b[0m  0.0174\n",
            "     91       \u001b[36m22.0294\u001b[0m  0.0249\n",
            "     92       \u001b[36m21.3002\u001b[0m  0.0275\n",
            "     93       \u001b[36m21.0764\u001b[0m  0.0251\n",
            "     94       \u001b[36m20.2066\u001b[0m  0.0273\n",
            "     95       \u001b[36m20.0789\u001b[0m  0.0263\n",
            "     96       \u001b[36m19.5833\u001b[0m  0.0256\n",
            "     97       \u001b[36m19.2811\u001b[0m  0.0272\n",
            "     98       \u001b[36m19.2209\u001b[0m  0.0258\n",
            "     99       \u001b[36m18.7824\u001b[0m  0.0232\n",
            "    100       \u001b[36m17.9511\u001b[0m  0.0223\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m47.0575\u001b[0m  0.0164\n",
            "      2       \u001b[36m46.6566\u001b[0m  0.0246\n",
            "      3       \u001b[36m45.0445\u001b[0m  0.0219\n",
            "      4       46.2208  0.0242\n",
            "      5       47.0438  0.0189\n",
            "      6       47.6730  0.0269\n",
            "      7       48.2746  0.0203\n",
            "      8       48.2327  0.0158\n",
            "      9       48.2155  0.0196\n",
            "     10       48.2102  0.0199\n",
            "     11       48.2118  0.0202\n",
            "     12       48.2153  0.0181\n",
            "     13       48.2178  0.0257\n",
            "     14       47.6313  0.0162\n",
            "     15       47.4320  0.0155\n",
            "     16       47.1669  0.0216\n",
            "     17       47.1675  0.0223\n",
            "     18       47.1680  0.0254\n",
            "     19       47.1686  0.0176\n",
            "     20       47.1690  0.0259\n",
            "     21       47.1694  0.0252\n",
            "     22       47.1698  0.0178\n",
            "     23       47.1702  0.0235\n",
            "     24       46.2180  0.0337\n",
            "     25       46.9041  0.0224\n",
            "     26       \u001b[36m44.3487\u001b[0m  0.0247\n",
            "     27       \u001b[36m43.0249\u001b[0m  0.0256\n",
            "     28       43.4012  0.0239\n",
            "     29       43.8665  0.0257\n",
            "     30       45.4179  0.0244\n",
            "     31       45.7050  0.0208\n",
            "     32       45.6140  0.0213\n",
            "     33       45.0579  0.0274\n",
            "     34       44.4977  0.0242\n",
            "     35       \u001b[36m42.6246\u001b[0m  0.0271\n",
            "     36       43.6253  0.0288\n",
            "     37       44.5415  0.0236\n",
            "     38       44.0558  0.0267\n",
            "     39       44.9325  0.0171\n",
            "     40       43.5755  0.0185\n",
            "     41       \u001b[36m30.6830\u001b[0m  0.0169\n",
            "     42       32.3049  0.0233\n",
            "     43       32.6444  0.0207\n",
            "     44       \u001b[36m27.7730\u001b[0m  0.0233\n",
            "     45       31.5431  0.0235\n",
            "     46       35.0885  0.0263\n",
            "     47       35.4734  0.0224\n",
            "     48       34.7368  0.0190\n",
            "     49       34.4274  0.0269\n",
            "     50       34.7368  0.0264\n",
            "     51       34.7368  0.0225\n",
            "     52       34.7368  0.0200\n",
            "     53       34.7368  0.0165\n",
            "     54       34.7368  0.0204\n",
            "     55       34.7368  0.0147\n",
            "     56       34.7368  0.0271\n",
            "     57       34.7368  0.0238\n",
            "     58       34.7368  0.0236\n",
            "     59       34.7368  0.0238\n",
            "     60       34.7368  0.0218\n",
            "     61       34.7368  0.0221\n",
            "     62       34.7368  0.0277\n",
            "     63       34.7368  0.0249\n",
            "     64       34.7368  0.0252\n",
            "     65       34.7368  0.0258\n",
            "     66       34.7368  0.0316\n",
            "     67       34.7368  0.0160\n",
            "     68       34.7368  0.0224\n",
            "     69       34.7368  0.0197\n",
            "     70       34.7368  0.0220\n",
            "     71       34.7368  0.0256\n",
            "     72       34.7368  0.0206\n",
            "     73       34.7368  0.0208\n",
            "     74       34.7368  0.0235\n",
            "     75       34.7368  0.0214\n",
            "     76       34.7368  0.0204\n",
            "     77       34.7368  0.0169\n",
            "     78       34.7368  0.0181\n",
            "     79       34.7368  0.0245\n",
            "     80       34.7368  0.0213\n",
            "     81       34.7815  0.0210\n",
            "     82       35.4386  0.0251\n",
            "     83       35.4386  0.0221\n",
            "     84       35.4386  0.0209\n",
            "     85       35.4386  0.0194\n",
            "     86       35.4386  0.0216\n",
            "     87       35.4386  0.0205\n",
            "     88       35.4386  0.0205\n",
            "     89       35.4386  0.0193\n",
            "     90       35.4386  0.0174\n",
            "     91       35.4386  0.0247\n",
            "     92       35.4386  0.0225\n",
            "     93       35.4386  0.0215\n",
            "     94       35.4386  0.0210\n",
            "     95       35.4386  0.0149\n",
            "     96       35.4386  0.0264\n",
            "     97       35.4386  0.0262\n",
            "     98       35.4386  0.0221\n",
            "     99       35.4386  0.0207\n",
            "    100       35.4386  0.0165\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m40.2326\u001b[0m  0.0223\n",
            "      2       \u001b[36m32.4366\u001b[0m  0.0227\n",
            "      3       36.1535  0.0172\n",
            "      4       38.0282  0.0180\n",
            "      5       38.0282  0.0168\n",
            "      6       38.0282  0.0157\n",
            "      7       38.0282  0.0178\n",
            "      8       38.0282  0.0204\n",
            "      9       38.0282  0.0203\n",
            "     10       38.0282  0.0200\n",
            "     11       38.0282  0.0201\n",
            "     12       38.0282  0.0130\n",
            "     13       38.0282  0.0232\n",
            "     14       38.0282  0.0230\n",
            "     15       38.0282  0.0195\n",
            "     16       38.0282  0.0170\n",
            "     17       38.0282  0.0159\n",
            "     18       38.0282  0.0162\n",
            "     19       38.0282  0.0152\n",
            "     20       38.0282  0.0172\n",
            "     21       38.0282  0.0175\n",
            "     22       38.0282  0.0152\n",
            "     23       38.0282  0.0192\n",
            "     24       38.0282  0.0144\n",
            "     25       38.0282  0.0171\n",
            "     26       38.0282  0.0161\n",
            "     27       38.0282  0.0160\n",
            "     28       38.0282  0.0168\n",
            "     29       38.0282  0.0159\n",
            "     30       38.0282  0.0157\n",
            "     31       38.0282  0.0155\n",
            "     32       38.0282  0.0172\n",
            "     33       38.0282  0.0214\n",
            "     34       38.0282  0.0220\n",
            "     35       38.0282  0.0193\n",
            "     36       38.0282  0.0164\n",
            "     37       38.0282  0.0139\n",
            "     38       38.0282  0.0152\n",
            "     39       38.0282  0.0159\n",
            "     40       38.0282  0.0152\n",
            "     41       38.0282  0.0154\n",
            "     42       38.0282  0.0160\n",
            "     43       38.0282  0.0181\n",
            "     44       38.0282  0.0150\n",
            "     45       38.0282  0.0157\n",
            "     46       38.0282  0.0165\n",
            "     47       38.0282  0.0167\n",
            "     48       38.0282  0.0199\n",
            "     49       38.0282  0.0178\n",
            "     50       38.0282  0.0186\n",
            "     51       38.0282  0.0162\n",
            "     52       38.0282  0.0162\n",
            "     53       38.0282  0.0161\n",
            "     54       38.0282  0.0161\n",
            "     55       38.0282  0.0182\n",
            "     56       38.0282  0.0182\n",
            "     57       38.0282  0.0184\n",
            "     58       38.0282  0.0198\n",
            "     59       38.0282  0.0187\n",
            "     60       38.0282  0.0173\n",
            "     61       38.0282  0.0236\n",
            "     62       38.0282  0.0211\n",
            "     63       38.0282  0.0232\n",
            "     64       38.0282  0.0158\n",
            "     65       38.0282  0.0145\n",
            "     66       38.0282  0.0117\n",
            "     67       38.0282  0.0187\n",
            "     68       38.0282  0.0178\n",
            "     69       38.0282  0.0146\n",
            "     70       38.0282  0.0246\n",
            "     71       38.0282  0.0153\n",
            "     72       38.0282  0.0117\n",
            "     73       38.0282  0.0153\n",
            "     74       38.0282  0.0153\n",
            "     75       38.0282  0.0219\n",
            "     76       38.0282  0.0171\n",
            "     77       38.0282  0.0170\n",
            "     78       38.0282  0.0160\n",
            "     79       38.0282  0.0156\n",
            "     80       38.0282  0.0164\n",
            "     81       38.0282  0.0155\n",
            "     82       38.0282  0.0144\n",
            "     83       38.0282  0.0147\n",
            "     84       38.0282  0.0147\n",
            "     85       38.0282  0.0143\n",
            "     86       38.0282  0.0149\n",
            "     87       38.0282  0.0148\n",
            "     88       38.0282  0.0144\n",
            "     89       38.0282  0.0151\n",
            "     90       38.0282  0.0179\n",
            "     91       38.0282  0.0152\n",
            "     92       38.0282  0.0166\n",
            "     93       38.0282  0.0176\n",
            "     94       38.0282  0.0150\n",
            "     95       38.0282  0.0148\n",
            "     96       38.0282  0.0152\n",
            "     97       38.0282  0.0155\n",
            "     98       38.0282  0.0144\n",
            "     99       38.0282  0.0245\n",
            "    100       38.0282  0.0170\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0159\n",
            "      2       37.1930  0.0173\n",
            "      3       37.1930  0.0179\n",
            "      4       37.1930  0.0182\n",
            "      5       37.1930  0.0200\n",
            "      6       37.1930  0.0251\n",
            "      7       37.1930  0.0171\n",
            "      8       37.1930  0.0186\n",
            "      9       37.1930  0.0177\n",
            "     10       37.1930  0.0178\n",
            "     11       37.1930  0.0167\n",
            "     12       37.1930  0.0200\n",
            "     13       37.1930  0.0166\n",
            "     14       37.1930  0.0147\n",
            "     15       37.1930  0.0146\n",
            "     16       37.1930  0.0160\n",
            "     17       37.1930  0.0153\n",
            "     18       37.1930  0.0217\n",
            "     19       37.1930  0.0157\n",
            "     20       37.1930  0.0154\n",
            "     21       37.1930  0.0146\n",
            "     22       37.1930  0.0155\n",
            "     23       37.1930  0.0153\n",
            "     24       37.1930  0.0152\n",
            "     25       37.1930  0.0158\n",
            "     26       37.1930  0.0152\n",
            "     27       37.1930  0.0174\n",
            "     28       37.1930  0.0169\n",
            "     29       37.1930  0.0163\n",
            "     30       37.1930  0.0146\n",
            "     31       37.1930  0.0150\n",
            "     32       37.1930  0.0148\n",
            "     33       37.1930  0.0155\n",
            "     34       37.1930  0.0151\n",
            "     35       37.1930  0.0155\n",
            "     36       37.1930  0.0152\n",
            "     37       37.1930  0.0152\n",
            "     38       37.1930  0.0188\n",
            "     39       37.1930  0.0152\n",
            "     40       37.1930  0.0167\n",
            "     41       37.1930  0.0164\n",
            "     42       37.1930  0.0164\n",
            "     43       37.1930  0.0159\n",
            "     44       37.1930  0.0159\n",
            "     45       37.1930  0.0156\n",
            "     46       37.1930  0.0196\n",
            "     47       37.1930  0.0190\n",
            "     48       37.1930  0.0173\n",
            "     49       37.1930  0.0169\n",
            "     50       37.1930  0.0196\n",
            "     51       37.1930  0.0175\n",
            "     52       37.1930  0.0168\n",
            "     53       37.1930  0.0172\n",
            "     54       37.1930  0.0168\n",
            "     55       37.1930  0.0171\n",
            "     56       37.1930  0.0160\n",
            "     57       37.1930  0.0170\n",
            "     58       37.1930  0.0147\n",
            "     59       37.1930  0.0156\n",
            "     60       37.1930  0.0159\n",
            "     61       37.1930  0.0150\n",
            "     62       37.1930  0.0148\n",
            "     63       37.1930  0.0220\n",
            "     64       37.1930  0.0149\n",
            "     65       37.1930  0.0152\n",
            "     66       37.1930  0.0193\n",
            "     67       37.1930  0.0150\n",
            "     68       37.1930  0.0146\n",
            "     69       37.1930  0.0149\n",
            "     70       37.1930  0.0165\n",
            "     71       37.1930  0.0190\n",
            "     72       37.1930  0.0170\n",
            "     73       37.1930  0.0214\n",
            "     74       37.1930  0.0147\n",
            "     75       37.1930  0.0165\n",
            "     76       37.1930  0.0150\n",
            "     77       37.1930  0.0157\n",
            "     78       37.1930  0.0153\n",
            "     79       37.1930  0.0150\n",
            "     80       37.1930  0.0161\n",
            "     81       37.1930  0.0205\n",
            "     82       37.1930  0.0165\n",
            "     83       37.1930  0.0163\n",
            "     84       37.1930  0.0161\n",
            "     85       37.1930  0.0181\n",
            "     86       37.1930  0.0161\n",
            "     87       37.1930  0.0180\n",
            "     88       37.1930  0.0169\n",
            "     89       37.1930  0.0156\n",
            "     90       37.1930  0.0164\n",
            "     91       37.1930  0.0173\n",
            "     92       37.1930  0.0177\n",
            "     93       37.1930  0.0177\n",
            "     94       37.1930  0.0184\n",
            "     95       37.1930  0.0177\n",
            "     96       37.1930  0.0180\n",
            "     97       37.1930  0.0189\n",
            "     98       37.1930  0.0260\n",
            "     99       37.1930  0.0181\n",
            "    100       37.1930  0.0203\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.6761\u001b[0m  0.0233\n",
            "      2       \u001b[36m37.3239\u001b[0m  0.0248\n",
            "      3       \u001b[36m36.8357\u001b[0m  0.0243\n",
            "      4       37.2360  0.0211\n",
            "      5       36.9718  0.0222\n",
            "      6       \u001b[36m36.6342\u001b[0m  0.0208\n",
            "      7       36.9728  0.0214\n",
            "      8       \u001b[36m35.5634\u001b[0m  0.0253\n",
            "      9       \u001b[36m35.2113\u001b[0m  0.0234\n",
            "     10       \u001b[36m34.9358\u001b[0m  0.0205\n",
            "     11       \u001b[36m34.8592\u001b[0m  0.0206\n",
            "     12       35.2416  0.0205\n",
            "     13       35.5634  0.0194\n",
            "     14       \u001b[36m33.6088\u001b[0m  0.0248\n",
            "     15       \u001b[36m32.7465\u001b[0m  0.0198\n",
            "     16       32.9207  0.0212\n",
            "     17       33.0172  0.0211\n",
            "     18       32.7467  0.0209\n",
            "     19       32.7669  0.0207\n",
            "     20       33.3470  0.0204\n",
            "     21       34.5070  0.0216\n",
            "     22       35.2113  0.0208\n",
            "     23       35.5634  0.0231\n",
            "     24       35.5634  0.0254\n",
            "     25       35.5634  0.0208\n",
            "     26       35.5634  0.0234\n",
            "     27       35.5634  0.0223\n",
            "     28       35.5634  0.0223\n",
            "     29       35.5634  0.0199\n",
            "     30       35.5634  0.0202\n",
            "     31       35.5634  0.0206\n",
            "     32       35.5634  0.0206\n",
            "     33       35.5634  0.0246\n",
            "     34       35.5634  0.0219\n",
            "     35       35.5634  0.0227\n",
            "     36       35.5634  0.0239\n",
            "     37       35.2113  0.0365\n",
            "     38       35.2113  0.0226\n",
            "     39       35.2113  0.0222\n",
            "     40       35.2113  0.0171\n",
            "     41       35.2113  0.0208\n",
            "     42       35.2113  0.0226\n",
            "     43       35.2113  0.0221\n",
            "     44       35.2113  0.0221\n",
            "     45       35.2113  0.0193\n",
            "     46       35.2113  0.0217\n",
            "     47       35.2113  0.0309\n",
            "     48       35.2113  0.0199\n",
            "     49       35.2113  0.0203\n",
            "     50       35.2113  0.0170\n",
            "     51       35.5634  0.0193\n",
            "     52       35.5634  0.0188\n",
            "     53       35.5634  0.0207\n",
            "     54       35.5634  0.0209\n",
            "     55       35.5634  0.0185\n",
            "     56       35.5634  0.0246\n",
            "     57       35.9155  0.0240\n",
            "     58       35.9155  0.0258\n",
            "     59       35.9155  0.0168\n",
            "     60       35.9155  0.0236\n",
            "     61       35.9155  0.0203\n",
            "     62       35.9155  0.0233\n",
            "     63       35.8290  0.0229\n",
            "     64       35.7492  0.0225\n",
            "     65       35.6704  0.0159\n",
            "     66       35.5634  0.0199\n",
            "     67       35.5634  0.0252\n",
            "     68       35.5634  0.0248\n",
            "     69       35.5634  0.0279\n",
            "     70       35.5634  0.0165\n",
            "     71       35.5634  0.0306\n",
            "     72       35.5634  0.0249\n",
            "     73       35.5634  0.0255\n",
            "     74       35.5634  0.0148\n",
            "     75       35.5634  0.0238\n",
            "     76       35.7225  0.0208\n",
            "     77       35.9155  0.0244\n",
            "     78       35.9155  0.0268\n",
            "     79       35.9155  0.0232\n",
            "     80       35.9155  0.0236\n",
            "     81       35.9155  0.0228\n",
            "     82       35.2390  0.0240\n",
            "     83       36.9718  0.0238\n",
            "     84       37.3239  0.0227\n",
            "     85       37.6761  0.0201\n",
            "     86       37.6761  0.0199\n",
            "     87       37.6761  0.0198\n",
            "     88       37.6761  0.0202\n",
            "     89       37.6761  0.0200\n",
            "     90       37.6761  0.0191\n",
            "     91       37.6761  0.0234\n",
            "     92       37.6761  0.0214\n",
            "     93       37.6761  0.0222\n",
            "     94       37.6761  0.0203\n",
            "     95       37.6761  0.0196\n",
            "     96       37.6761  0.0211\n",
            "     97       37.6761  0.0191\n",
            "     98       37.6761  0.0210\n",
            "     99       37.6761  0.0205\n",
            "    100       37.6761  0.0202\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m47.7193\u001b[0m  0.0186\n",
            "      2       47.7193  0.0204\n",
            "      3       48.0702  0.0197\n",
            "      4       47.9249  0.0212\n",
            "      5       \u001b[36m47.3684\u001b[0m  0.0192\n",
            "      6       47.3684  0.0193\n",
            "      7       \u001b[36m47.0175\u001b[0m  0.0168\n",
            "      8       47.0175  0.0288\n",
            "      9       \u001b[36m46.8435\u001b[0m  0.0226\n",
            "     10       \u001b[36m46.6667\u001b[0m  0.0275\n",
            "     11       46.6667  0.0239\n",
            "     12       46.6667  0.0178\n",
            "     13       46.6667  0.0336\n",
            "     14       46.6667  0.0203\n",
            "     15       46.6667  0.0205\n",
            "     16       46.6667  0.0258\n",
            "     17       46.6667  0.0294\n",
            "     18       46.6667  0.0280\n",
            "     19       46.6667  0.0168\n",
            "     20       46.7492  0.0192\n",
            "     21       \u001b[36m43.1579\u001b[0m  0.0171\n",
            "     22       43.1579  0.0197\n",
            "     23       \u001b[36m42.8070\u001b[0m  0.0214\n",
            "     24       \u001b[36m42.1053\u001b[0m  0.0190\n",
            "     25       \u001b[36m41.4035\u001b[0m  0.0203\n",
            "     26       \u001b[36m41.0526\u001b[0m  0.0193\n",
            "     27       41.0526  0.0200\n",
            "     28       41.4035  0.0192\n",
            "     29       \u001b[36m40.7018\u001b[0m  0.0343\n",
            "     30       \u001b[36m39.8170\u001b[0m  0.0217\n",
            "     31       \u001b[36m39.7018\u001b[0m  0.0197\n",
            "     32       \u001b[36m38.8425\u001b[0m  0.0150\n",
            "     33       \u001b[36m38.5965\u001b[0m  0.0209\n",
            "     34       \u001b[36m37.7891\u001b[0m  0.0175\n",
            "     35       37.9365  0.0206\n",
            "     36       38.2456  0.0211\n",
            "     37       38.9078  0.0197\n",
            "     38       38.7333  0.0198\n",
            "     39       \u001b[36m37.7891\u001b[0m  0.0199\n",
            "     40       \u001b[36m37.1930\u001b[0m  0.0197\n",
            "     41       38.2456  0.0211\n",
            "     42       38.2456  0.0237\n",
            "     43       38.5965  0.0239\n",
            "     44       38.5965  0.0213\n",
            "     45       38.5965  0.0217\n",
            "     46       38.5965  0.0180\n",
            "     47       38.5965  0.0211\n",
            "     48       38.5965  0.0254\n",
            "     49       38.5965  0.0256\n",
            "     50       38.5965  0.0234\n",
            "     51       38.5965  0.0269\n",
            "     52       38.5965  0.0159\n",
            "     53       38.5965  0.0231\n",
            "     54       38.5965  0.0206\n",
            "     55       38.5965  0.0225\n",
            "     56       38.5965  0.0287\n",
            "     57       38.5965  0.0332\n",
            "     58       38.5965  0.0240\n",
            "     59       38.5965  0.0220\n",
            "     60       38.5965  0.0211\n",
            "     61       38.5965  0.0185\n",
            "     62       38.5965  0.0191\n",
            "     63       38.5965  0.0219\n",
            "     64       38.5965  0.0206\n",
            "     65       38.5965  0.0222\n",
            "     66       38.5965  0.0214\n",
            "     67       38.5965  0.0209\n",
            "     68       38.5965  0.0212\n",
            "     69       38.5965  0.0194\n",
            "     70       38.5965  0.0234\n",
            "     71       38.5965  0.0205\n",
            "     72       38.5965  0.0199\n",
            "     73       38.5965  0.0187\n",
            "     74       38.5965  0.0199\n",
            "     75       38.5965  0.0188\n",
            "     76       38.5965  0.0263\n",
            "     77       38.5965  0.0214\n",
            "     78       38.5965  0.0221\n",
            "     79       38.5965  0.0205\n",
            "     80       38.5965  0.0185\n",
            "     81       38.5965  0.0145\n",
            "     82       38.5965  0.0213\n",
            "     83       38.5965  0.0203\n",
            "     84       38.5965  0.0257\n",
            "     85       38.5965  0.0217\n",
            "     86       38.5965  0.0224\n",
            "     87       38.5965  0.0214\n",
            "     88       38.5965  0.0199\n",
            "     89       38.5965  0.0266\n",
            "     90       38.5965  0.0231\n",
            "     91       38.5965  0.0267\n",
            "     92       38.5965  0.0156\n",
            "     93       38.5965  0.0185\n",
            "     94       38.5965  0.0193\n",
            "     95       38.5965  0.0255\n",
            "     96       38.5965  0.0268\n",
            "     97       38.5965  0.0225\n",
            "     98       38.5965  0.0241\n",
            "     99       38.5965  0.0202\n",
            "    100       38.5965  0.0194\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m42.2535\u001b[0m  0.0174\n",
            "      2       42.2535  0.0161\n",
            "      3       42.2535  0.0202\n",
            "      4       42.2535  0.0171\n",
            "      5       42.2535  0.0112\n",
            "      6       42.2535  0.0174\n",
            "      7       42.2535  0.0127\n",
            "      8       42.2535  0.0165\n",
            "      9       42.2535  0.0167\n",
            "     10       42.2535  0.0161\n",
            "     11       42.2535  0.0170\n",
            "     12       42.2535  0.0161\n",
            "     13       42.2535  0.0163\n",
            "     14       42.2535  0.0157\n",
            "     15       42.2535  0.0160\n",
            "     16       42.2535  0.0155\n",
            "     17       42.2535  0.0171\n",
            "     18       42.2535  0.0211\n",
            "     19       42.2535  0.0186\n",
            "     20       42.2535  0.0194\n",
            "     21       42.2535  0.0179\n",
            "     22       42.2535  0.0184\n",
            "     23       42.2535  0.0179\n",
            "     24       42.2535  0.0167\n",
            "     25       42.2535  0.0175\n",
            "     26       42.2535  0.0189\n",
            "     27       42.2535  0.0181\n",
            "     28       42.2535  0.0182\n",
            "     29       42.2535  0.0189\n",
            "     30       42.2535  0.0192\n",
            "     31       42.2535  0.0186\n",
            "     32       42.2535  0.0173\n",
            "     33       42.2535  0.0164\n",
            "     34       42.2535  0.0232\n",
            "     35       42.2535  0.0203\n",
            "     36       42.2535  0.0188\n",
            "     37       42.2535  0.0173\n",
            "     38       42.2535  0.0158\n",
            "     39       42.2535  0.0184\n",
            "     40       42.2535  0.0207\n",
            "     41       42.2535  0.0246\n",
            "     42       42.2535  0.0214\n",
            "     43       42.2535  0.0253\n",
            "     44       42.2535  0.0188\n",
            "     45       42.2535  0.0171\n",
            "     46       42.2535  0.0155\n",
            "     47       42.2535  0.0159\n",
            "     48       42.2535  0.0150\n",
            "     49       42.2535  0.0176\n",
            "     50       42.2535  0.0170\n",
            "     51       42.2535  0.0202\n",
            "     52       42.2535  0.0164\n",
            "     53       42.2535  0.0150\n",
            "     54       42.2535  0.0137\n",
            "     55       42.2535  0.0185\n",
            "     56       42.2535  0.0120\n",
            "     57       42.2535  0.0164\n",
            "     58       42.2535  0.0150\n",
            "     59       42.2535  0.0159\n",
            "     60       42.2535  0.0193\n",
            "     61       42.2535  0.0193\n",
            "     62       42.2535  0.0184\n",
            "     63       42.2535  0.0150\n",
            "     64       42.2535  0.0150\n",
            "     65       42.2535  0.0162\n",
            "     66       42.2535  0.0153\n",
            "     67       42.2535  0.0157\n",
            "     68       42.2535  0.0158\n",
            "     69       42.2535  0.0158\n",
            "     70       42.2535  0.0156\n",
            "     71       42.2535  0.0180\n",
            "     72       42.2535  0.0198\n",
            "     73       42.2535  0.0167\n",
            "     74       42.2535  0.0206\n",
            "     75       42.2535  0.0184\n",
            "     76       42.2535  0.0177\n",
            "     77       42.2535  0.0177\n",
            "     78       42.2535  0.0222\n",
            "     79       42.2535  0.0201\n",
            "     80       42.2535  0.0190\n",
            "     81       42.2535  0.0177\n",
            "     82       42.2535  0.0176\n",
            "     83       42.2535  0.0170\n",
            "     84       42.2535  0.0153\n",
            "     85       42.2535  0.0176\n",
            "     86       42.2535  0.0181\n",
            "     87       42.2535  0.0161\n",
            "     88       42.2535  0.0161\n",
            "     89       42.2535  0.0153\n",
            "     90       42.2535  0.0227\n",
            "     91       42.2535  0.0225\n",
            "     92       42.2535  0.0199\n",
            "     93       42.2535  0.0158\n",
            "     94       42.2535  0.0185\n",
            "     95       42.2535  0.0169\n",
            "     96       42.2535  0.0159\n",
            "     97       42.2535  0.0167\n",
            "     98       42.2535  0.0164\n",
            "     99       42.2535  0.0158\n",
            "    100       42.2535  0.0166\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1930\u001b[0m  0.0173\n",
            "      2       37.1930  0.0165\n",
            "      3       37.1930  0.0179\n",
            "      4       37.1930  0.0165\n",
            "      5       37.1930  0.0161\n",
            "      6       37.1930  0.0198\n",
            "      7       37.1930  0.0200\n",
            "      8       37.1930  0.0160\n",
            "      9       37.1930  0.0198\n",
            "     10       37.1930  0.0185\n",
            "     11       37.1930  0.0189\n",
            "     12       37.1930  0.0178\n",
            "     13       37.1930  0.0171\n",
            "     14       37.1930  0.0178\n",
            "     15       37.1930  0.0198\n",
            "     16       37.1930  0.0204\n",
            "     17       37.1930  0.0178\n",
            "     18       37.1930  0.0182\n",
            "     19       37.1930  0.0180\n",
            "     20       37.1930  0.0175\n",
            "     21       37.1930  0.0178\n",
            "     22       37.1930  0.0192\n",
            "     23       37.1930  0.0176\n",
            "     24       37.1930  0.0185\n",
            "     25       37.1930  0.0186\n",
            "     26       37.1930  0.0162\n",
            "     27       37.1930  0.0165\n",
            "     28       37.1930  0.0177\n",
            "     29       37.1930  0.0165\n",
            "     30       37.1930  0.0158\n",
            "     31       37.1930  0.0158\n",
            "     32       37.1930  0.0154\n",
            "     33       37.1930  0.0258\n",
            "     34       37.1930  0.0151\n",
            "     35       37.1930  0.0155\n",
            "     36       37.1930  0.0171\n",
            "     37       37.1930  0.0166\n",
            "     38       37.1930  0.0179\n",
            "     39       37.1930  0.0159\n",
            "     40       37.1930  0.0201\n",
            "     41       37.1930  0.0173\n",
            "     42       37.1930  0.0159\n",
            "     43       37.1930  0.0158\n",
            "     44       37.1930  0.0169\n",
            "     45       37.1930  0.0177\n",
            "     46       37.1930  0.0169\n",
            "     47       37.1930  0.0169\n",
            "     48       37.1930  0.0178\n",
            "     49       37.1930  0.0167\n",
            "     50       37.1930  0.0155\n",
            "     51       37.1930  0.0154\n",
            "     52       37.1930  0.0155\n",
            "     53       37.1930  0.0228\n",
            "     54       37.1930  0.0174\n",
            "     55       37.1930  0.0173\n",
            "     56       37.1930  0.0163\n",
            "     57       37.1930  0.0161\n",
            "     58       37.1930  0.0168\n",
            "     59       37.1930  0.0158\n",
            "     60       37.1930  0.0159\n",
            "     61       37.1930  0.0155\n",
            "     62       37.1930  0.0156\n",
            "     63       37.1930  0.0155\n",
            "     64       37.1930  0.0161\n",
            "     65       37.1930  0.0161\n",
            "     66       37.1930  0.0167\n",
            "     67       37.1930  0.0177\n",
            "     68       37.1930  0.0185\n",
            "     69       37.1930  0.0166\n",
            "     70       37.1930  0.0209\n",
            "     71       37.1930  0.0210\n",
            "     72       37.1930  0.0222\n",
            "     73       37.1930  0.0177\n",
            "     74       37.1930  0.0163\n",
            "     75       37.1930  0.0155\n",
            "     76       37.1930  0.0162\n",
            "     77       37.1930  0.0182\n",
            "     78       37.1930  0.0178\n",
            "     79       37.1930  0.0160\n",
            "     80       37.1930  0.0168\n",
            "     81       37.1930  0.0187\n",
            "     82       37.1930  0.0158\n",
            "     83       37.1930  0.0167\n",
            "     84       37.1930  0.0160\n",
            "     85       37.1930  0.0155\n",
            "     86       37.1930  0.0163\n",
            "     87       37.1930  0.0154\n",
            "     88       37.1930  0.0170\n",
            "     89       37.1930  0.0154\n",
            "     90       37.1930  0.0241\n",
            "     91       37.1930  0.0167\n",
            "     92       37.1930  0.0167\n",
            "     93       37.1930  0.0157\n",
            "     94       37.1930  0.0160\n",
            "     95       37.1930  0.0165\n",
            "     96       37.1930  0.0165\n",
            "     97       37.1930  0.0158\n",
            "     98       37.1930  0.0190\n",
            "     99       37.1930  0.0177\n",
            "    100       37.1930  0.0185\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.9734\u001b[0m  0.0235\n",
            "      2        \u001b[36m1.9496\u001b[0m  0.0229\n",
            "      3        \u001b[36m1.9262\u001b[0m  0.0229\n",
            "      4        \u001b[36m1.9026\u001b[0m  0.0221\n",
            "      5        \u001b[36m1.8788\u001b[0m  0.0246\n",
            "      6        \u001b[36m1.8549\u001b[0m  0.0244\n",
            "      7        \u001b[36m1.8308\u001b[0m  0.0242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m1.8066\u001b[0m  0.0359\n",
            "      9        \u001b[36m1.7823\u001b[0m  0.0246\n",
            "     10        \u001b[36m1.7578\u001b[0m  0.0255\n",
            "     11        \u001b[36m1.7331\u001b[0m  0.0286\n",
            "     12        \u001b[36m1.7081\u001b[0m  0.0381\n",
            "     13        \u001b[36m1.6827\u001b[0m  0.0258\n",
            "     14        \u001b[36m1.6568\u001b[0m  0.0309\n",
            "     15        \u001b[36m1.6302\u001b[0m  0.0245\n",
            "     16        \u001b[36m1.6026\u001b[0m  0.0239\n",
            "     17        \u001b[36m1.5737\u001b[0m  0.0230\n",
            "     18        \u001b[36m1.5430\u001b[0m  0.0205\n",
            "     19        \u001b[36m1.5097\u001b[0m  0.0218\n",
            "     20        \u001b[36m1.4730\u001b[0m  0.0210\n",
            "     21        \u001b[36m1.4319\u001b[0m  0.0228\n",
            "     22        \u001b[36m1.3852\u001b[0m  0.0215\n",
            "     23        \u001b[36m1.3315\u001b[0m  0.0273\n",
            "     24        \u001b[36m1.2701\u001b[0m  0.0215\n",
            "     25        \u001b[36m1.2005\u001b[0m  0.0216\n",
            "     26        \u001b[36m1.1235\u001b[0m  0.0208\n",
            "     27        \u001b[36m1.0413\u001b[0m  0.0216\n",
            "     28        \u001b[36m0.9573\u001b[0m  0.0282\n",
            "     29        \u001b[36m0.8763\u001b[0m  0.0211\n",
            "     30        \u001b[36m0.8043\u001b[0m  0.0204\n",
            "     31        \u001b[36m0.7471\u001b[0m  0.0203\n",
            "     32        \u001b[36m0.7076\u001b[0m  0.0217\n",
            "     33        \u001b[36m0.6845\u001b[0m  0.0208\n",
            "     34        \u001b[36m0.6727\u001b[0m  0.0214\n",
            "     35        \u001b[36m0.6671\u001b[0m  0.0227\n",
            "     36        \u001b[36m0.6644\u001b[0m  0.0209\n",
            "     37        \u001b[36m0.6631\u001b[0m  0.0192\n",
            "     38        \u001b[36m0.6624\u001b[0m  0.0270\n",
            "     39        \u001b[36m0.6621\u001b[0m  0.0229\n",
            "     40        \u001b[36m0.6620\u001b[0m  0.0228\n",
            "     41        \u001b[36m0.6620\u001b[0m  0.0232\n",
            "     42        0.6620  0.0234\n",
            "     43        0.6620  0.0232\n",
            "     44        0.6620  0.0241\n",
            "     45        0.6620  0.0215\n",
            "     46        0.6620  0.0242\n",
            "     47        0.6620  0.0223\n",
            "     48        \u001b[36m0.6620\u001b[0m  0.0263\n",
            "     49        \u001b[36m0.6619\u001b[0m  0.0231\n",
            "     50        \u001b[36m0.6619\u001b[0m  0.0214\n",
            "     51        \u001b[36m0.6619\u001b[0m  0.0218\n",
            "     52        \u001b[36m0.6619\u001b[0m  0.0231\n",
            "     53        \u001b[36m0.6619\u001b[0m  0.0195\n",
            "     54        \u001b[36m0.6619\u001b[0m  0.0202\n",
            "     55        \u001b[36m0.6619\u001b[0m  0.0173\n",
            "     56        \u001b[36m0.6619\u001b[0m  0.0188\n",
            "     57        \u001b[36m0.6619\u001b[0m  0.0281\n",
            "     58        \u001b[36m0.6619\u001b[0m  0.0180\n",
            "     59        \u001b[36m0.6619\u001b[0m  0.0207\n",
            "     60        \u001b[36m0.6618\u001b[0m  0.0218\n",
            "     61        \u001b[36m0.6618\u001b[0m  0.0215\n",
            "     62        \u001b[36m0.6618\u001b[0m  0.0177\n",
            "     63        \u001b[36m0.6618\u001b[0m  0.0193\n",
            "     64        \u001b[36m0.6618\u001b[0m  0.0223\n",
            "     65        \u001b[36m0.6618\u001b[0m  0.0182\n",
            "     66        \u001b[36m0.6618\u001b[0m  0.0202\n",
            "     67        \u001b[36m0.6618\u001b[0m  0.0182\n",
            "     68        \u001b[36m0.6618\u001b[0m  0.0262\n",
            "     69        \u001b[36m0.6618\u001b[0m  0.0285\n",
            "     70        \u001b[36m0.6618\u001b[0m  0.0214\n",
            "     71        \u001b[36m0.6618\u001b[0m  0.0170\n",
            "     72        \u001b[36m0.6618\u001b[0m  0.0171\n",
            "     73        \u001b[36m0.6618\u001b[0m  0.0167\n",
            "     74        \u001b[36m0.6618\u001b[0m  0.0233\n",
            "     75        \u001b[36m0.6618\u001b[0m  0.0248\n",
            "     76        \u001b[36m0.6618\u001b[0m  0.0164\n",
            "     77        \u001b[36m0.6618\u001b[0m  0.0153\n",
            "     78        \u001b[36m0.6618\u001b[0m  0.0220\n",
            "     79        \u001b[36m0.6618\u001b[0m  0.0206\n",
            "     80        \u001b[36m0.6618\u001b[0m  0.0231\n",
            "     81        \u001b[36m0.6618\u001b[0m  0.0171\n",
            "     82        \u001b[36m0.6618\u001b[0m  0.0159\n",
            "     83        \u001b[36m0.6618\u001b[0m  0.0199\n",
            "     84        \u001b[36m0.6618\u001b[0m  0.0217\n",
            "     85        \u001b[36m0.6618\u001b[0m  0.0277\n",
            "     86        \u001b[36m0.6618\u001b[0m  0.0262\n",
            "     87        \u001b[36m0.6618\u001b[0m  0.0180\n",
            "     88        \u001b[36m0.6618\u001b[0m  0.0304\n",
            "     89        \u001b[36m0.6618\u001b[0m  0.0215\n",
            "     90        \u001b[36m0.6618\u001b[0m  0.0267\n",
            "     91        \u001b[36m0.6618\u001b[0m  0.0240\n",
            "     92        \u001b[36m0.6617\u001b[0m  0.0159\n",
            "     93        \u001b[36m0.6617\u001b[0m  0.0241\n",
            "     94        \u001b[36m0.6617\u001b[0m  0.0159\n",
            "     95        \u001b[36m0.6617\u001b[0m  0.0228\n",
            "     96        \u001b[36m0.6617\u001b[0m  0.0199\n",
            "     97        \u001b[36m0.6617\u001b[0m  0.0203\n",
            "     98        \u001b[36m0.6617\u001b[0m  0.0228\n",
            "     99        \u001b[36m0.6617\u001b[0m  0.0153\n",
            "    100        \u001b[36m0.6617\u001b[0m  0.0229\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.7546\u001b[0m  0.0197\n",
            "      2        \u001b[36m1.7302\u001b[0m  0.0169\n",
            "      3        \u001b[36m1.7073\u001b[0m  0.0216\n",
            "      4        \u001b[36m1.6846\u001b[0m  0.0149\n",
            "      5        \u001b[36m1.6618\u001b[0m  0.0346\n",
            "      6        \u001b[36m1.6389\u001b[0m  0.0230\n",
            "      7        \u001b[36m1.6160\u001b[0m  0.0189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m1.5930\u001b[0m  0.0199\n",
            "      9        \u001b[36m1.5700\u001b[0m  0.0265\n",
            "     10        \u001b[36m1.5469\u001b[0m  0.0254\n",
            "     11        \u001b[36m1.5237\u001b[0m  0.0221\n",
            "     12        \u001b[36m1.5004\u001b[0m  0.0166\n",
            "     13        \u001b[36m1.4769\u001b[0m  0.0262\n",
            "     14        \u001b[36m1.4532\u001b[0m  0.0248\n",
            "     15        \u001b[36m1.4292\u001b[0m  0.0193\n",
            "     16        \u001b[36m1.4046\u001b[0m  0.0238\n",
            "     17        \u001b[36m1.3795\u001b[0m  0.0170\n",
            "     18        \u001b[36m1.3534\u001b[0m  0.0274\n",
            "     19        \u001b[36m1.3261\u001b[0m  0.0214\n",
            "     20        \u001b[36m1.2972\u001b[0m  0.0198\n",
            "     21        \u001b[36m1.2662\u001b[0m  0.0241\n",
            "     22        \u001b[36m1.2327\u001b[0m  0.0167\n",
            "     23        \u001b[36m1.1960\u001b[0m  0.0239\n",
            "     24        \u001b[36m1.1557\u001b[0m  0.0270\n",
            "     25        \u001b[36m1.1116\u001b[0m  0.0254\n",
            "     26        \u001b[36m1.0637\u001b[0m  0.0224\n",
            "     27        \u001b[36m1.0122\u001b[0m  0.0201\n",
            "     28        \u001b[36m0.9579\u001b[0m  0.0206\n",
            "     29        \u001b[36m0.9022\u001b[0m  0.0174\n",
            "     30        \u001b[36m0.8470\u001b[0m  0.0214\n",
            "     31        \u001b[36m0.7956\u001b[0m  0.0211\n",
            "     32        \u001b[36m0.7514\u001b[0m  0.0195\n",
            "     33        \u001b[36m0.7172\u001b[0m  0.0197\n",
            "     34        \u001b[36m0.6936\u001b[0m  0.0193\n",
            "     35        \u001b[36m0.6790\u001b[0m  0.0200\n",
            "     36        \u001b[36m0.6708\u001b[0m  0.0233\n",
            "     37        \u001b[36m0.6663\u001b[0m  0.0198\n",
            "     38        \u001b[36m0.6638\u001b[0m  0.0222\n",
            "     39        \u001b[36m0.6625\u001b[0m  0.0278\n",
            "     40        \u001b[36m0.6618\u001b[0m  0.0203\n",
            "     41        \u001b[36m0.6614\u001b[0m  0.0152\n",
            "     42        \u001b[36m0.6612\u001b[0m  0.0198\n",
            "     43        \u001b[36m0.6611\u001b[0m  0.0154\n",
            "     44        \u001b[36m0.6611\u001b[0m  0.0268\n",
            "     45        \u001b[36m0.6610\u001b[0m  0.0243\n",
            "     46        \u001b[36m0.6610\u001b[0m  0.0202\n",
            "     47        \u001b[36m0.6610\u001b[0m  0.0221\n",
            "     48        \u001b[36m0.6610\u001b[0m  0.0248\n",
            "     49        \u001b[36m0.6610\u001b[0m  0.0212\n",
            "     50        \u001b[36m0.6610\u001b[0m  0.0229\n",
            "     51        \u001b[36m0.6610\u001b[0m  0.0202\n",
            "     52        \u001b[36m0.6610\u001b[0m  0.0182\n",
            "     53        \u001b[36m0.6610\u001b[0m  0.0178\n",
            "     54        \u001b[36m0.6610\u001b[0m  0.0209\n",
            "     55        \u001b[36m0.6610\u001b[0m  0.0219\n",
            "     56        \u001b[36m0.6609\u001b[0m  0.0223\n",
            "     57        \u001b[36m0.6609\u001b[0m  0.0259\n",
            "     58        \u001b[36m0.6609\u001b[0m  0.0172\n",
            "     59        \u001b[36m0.6609\u001b[0m  0.0191\n",
            "     60        \u001b[36m0.6609\u001b[0m  0.0240\n",
            "     61        \u001b[36m0.6609\u001b[0m  0.0205\n",
            "     62        \u001b[36m0.6609\u001b[0m  0.0278\n",
            "     63        \u001b[36m0.6609\u001b[0m  0.0258\n",
            "     64        \u001b[36m0.6609\u001b[0m  0.0184\n",
            "     65        \u001b[36m0.6609\u001b[0m  0.0318\n",
            "     66        \u001b[36m0.6609\u001b[0m  0.0218\n",
            "     67        \u001b[36m0.6609\u001b[0m  0.0206\n",
            "     68        \u001b[36m0.6609\u001b[0m  0.0174\n",
            "     69        \u001b[36m0.6609\u001b[0m  0.0224\n",
            "     70        \u001b[36m0.6609\u001b[0m  0.0150\n",
            "     71        \u001b[36m0.6609\u001b[0m  0.0204\n",
            "     72        \u001b[36m0.6609\u001b[0m  0.0178\n",
            "     73        \u001b[36m0.6609\u001b[0m  0.0215\n",
            "     74        \u001b[36m0.6609\u001b[0m  0.0238\n",
            "     75        \u001b[36m0.6609\u001b[0m  0.0215\n",
            "     76        \u001b[36m0.6609\u001b[0m  0.0233\n",
            "     77        \u001b[36m0.6609\u001b[0m  0.0178\n",
            "     78        \u001b[36m0.6609\u001b[0m  0.0172\n",
            "     79        \u001b[36m0.6609\u001b[0m  0.0219\n",
            "     80        \u001b[36m0.6608\u001b[0m  0.0196\n",
            "     81        \u001b[36m0.6608\u001b[0m  0.0236\n",
            "     82        \u001b[36m0.6608\u001b[0m  0.0206\n",
            "     83        \u001b[36m0.6608\u001b[0m  0.0204\n",
            "     84        \u001b[36m0.6608\u001b[0m  0.0192\n",
            "     85        \u001b[36m0.6608\u001b[0m  0.0171\n",
            "     86        \u001b[36m0.6608\u001b[0m  0.0205\n",
            "     87        \u001b[36m0.6608\u001b[0m  0.0194\n",
            "     88        \u001b[36m0.6608\u001b[0m  0.0228\n",
            "     89        \u001b[36m0.6608\u001b[0m  0.0253\n",
            "     90        \u001b[36m0.6608\u001b[0m  0.0336\n",
            "     91        \u001b[36m0.6608\u001b[0m  0.0213\n",
            "     92        \u001b[36m0.6608\u001b[0m  0.0196\n",
            "     93        \u001b[36m0.6608\u001b[0m  0.0173\n",
            "     94        \u001b[36m0.6608\u001b[0m  0.0215\n",
            "     95        \u001b[36m0.6608\u001b[0m  0.0191\n",
            "     96        \u001b[36m0.6608\u001b[0m  0.0232\n",
            "     97        \u001b[36m0.6608\u001b[0m  0.0243\n",
            "     98        \u001b[36m0.6608\u001b[0m  0.0184\n",
            "     99        \u001b[36m0.6608\u001b[0m  0.0232\n",
            "    100        \u001b[36m0.6608\u001b[0m  0.0222\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.7439\u001b[0m  0.0122\n",
            "      2        \u001b[36m1.7332\u001b[0m  0.0198\n",
            "      3        \u001b[36m1.7226\u001b[0m  0.0145\n",
            "      4        \u001b[36m1.7120\u001b[0m  0.0183\n",
            "      5        \u001b[36m1.7014\u001b[0m  0.0154\n",
            "      6        \u001b[36m1.6908\u001b[0m  0.0205\n",
            "      7        \u001b[36m1.6803\u001b[0m  0.0160\n",
            "      8        \u001b[36m1.6698\u001b[0m  0.0125\n",
            "      9        \u001b[36m1.6592\u001b[0m  0.0168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.6487\u001b[0m  0.0214\n",
            "     11        \u001b[36m1.6383\u001b[0m  0.0253\n",
            "     12        \u001b[36m1.6278\u001b[0m  0.0180\n",
            "     13        \u001b[36m1.6174\u001b[0m  0.0148\n",
            "     14        \u001b[36m1.6070\u001b[0m  0.0158\n",
            "     15        \u001b[36m1.5966\u001b[0m  0.0156\n",
            "     16        \u001b[36m1.5863\u001b[0m  0.0124\n",
            "     17        \u001b[36m1.5759\u001b[0m  0.0185\n",
            "     18        \u001b[36m1.5656\u001b[0m  0.0156\n",
            "     19        \u001b[36m1.5553\u001b[0m  0.0154\n",
            "     20        \u001b[36m1.5451\u001b[0m  0.0150\n",
            "     21        \u001b[36m1.5348\u001b[0m  0.0168\n",
            "     22        \u001b[36m1.5246\u001b[0m  0.0174\n",
            "     23        \u001b[36m1.5145\u001b[0m  0.0158\n",
            "     24        \u001b[36m1.5043\u001b[0m  0.0163\n",
            "     25        \u001b[36m1.4942\u001b[0m  0.0157\n",
            "     26        \u001b[36m1.4841\u001b[0m  0.0155\n",
            "     27        \u001b[36m1.4740\u001b[0m  0.0157\n",
            "     28        \u001b[36m1.4640\u001b[0m  0.0160\n",
            "     29        \u001b[36m1.4540\u001b[0m  0.0156\n",
            "     30        \u001b[36m1.4441\u001b[0m  0.0157\n",
            "     31        \u001b[36m1.4341\u001b[0m  0.0163\n",
            "     32        \u001b[36m1.4242\u001b[0m  0.0167\n",
            "     33        \u001b[36m1.4144\u001b[0m  0.0172\n",
            "     34        \u001b[36m1.4046\u001b[0m  0.0160\n",
            "     35        \u001b[36m1.3948\u001b[0m  0.0165\n",
            "     36        \u001b[36m1.3850\u001b[0m  0.0167\n",
            "     37        \u001b[36m1.3753\u001b[0m  0.0222\n",
            "     38        \u001b[36m1.3657\u001b[0m  0.0171\n",
            "     39        \u001b[36m1.3561\u001b[0m  0.0177\n",
            "     40        \u001b[36m1.3465\u001b[0m  0.0176\n",
            "     41        \u001b[36m1.3369\u001b[0m  0.0180\n",
            "     42        \u001b[36m1.3274\u001b[0m  0.0174\n",
            "     43        \u001b[36m1.3180\u001b[0m  0.0222\n",
            "     44        \u001b[36m1.3086\u001b[0m  0.0194\n",
            "     45        \u001b[36m1.2992\u001b[0m  0.0189\n",
            "     46        \u001b[36m1.2899\u001b[0m  0.0187\n",
            "     47        \u001b[36m1.2807\u001b[0m  0.0185\n",
            "     48        \u001b[36m1.2715\u001b[0m  0.0190\n",
            "     49        \u001b[36m1.2623\u001b[0m  0.0169\n",
            "     50        \u001b[36m1.2532\u001b[0m  0.0182\n",
            "     51        \u001b[36m1.2441\u001b[0m  0.0135\n",
            "     52        \u001b[36m1.2351\u001b[0m  0.0184\n",
            "     53        \u001b[36m1.2262\u001b[0m  0.0177\n",
            "     54        \u001b[36m1.2173\u001b[0m  0.0221\n",
            "     55        \u001b[36m1.2084\u001b[0m  0.0173\n",
            "     56        \u001b[36m1.1997\u001b[0m  0.0165\n",
            "     57        \u001b[36m1.1909\u001b[0m  0.0133\n",
            "     58        \u001b[36m1.1823\u001b[0m  0.0201\n",
            "     59        \u001b[36m1.1737\u001b[0m  0.0154\n",
            "     60        \u001b[36m1.1651\u001b[0m  0.0166\n",
            "     61        \u001b[36m1.1567\u001b[0m  0.0163\n",
            "     62        \u001b[36m1.1482\u001b[0m  0.0159\n",
            "     63        \u001b[36m1.1399\u001b[0m  0.0166\n",
            "     64        \u001b[36m1.1316\u001b[0m  0.0158\n",
            "     65        \u001b[36m1.1234\u001b[0m  0.0202\n",
            "     66        \u001b[36m1.1153\u001b[0m  0.0253\n",
            "     67        \u001b[36m1.1072\u001b[0m  0.0147\n",
            "     68        \u001b[36m1.0992\u001b[0m  0.0188\n",
            "     69        \u001b[36m1.0912\u001b[0m  0.0110\n",
            "     70        \u001b[36m1.0834\u001b[0m  0.0166\n",
            "     71        \u001b[36m1.0756\u001b[0m  0.0157\n",
            "     72        \u001b[36m1.0678\u001b[0m  0.0173\n",
            "     73        \u001b[36m1.0602\u001b[0m  0.0166\n",
            "     74        \u001b[36m1.0526\u001b[0m  0.0156\n",
            "     75        \u001b[36m1.0451\u001b[0m  0.0160\n",
            "     76        \u001b[36m1.0377\u001b[0m  0.0151\n",
            "     77        \u001b[36m1.0304\u001b[0m  0.0160\n",
            "     78        \u001b[36m1.0231\u001b[0m  0.0157\n",
            "     79        \u001b[36m1.0159\u001b[0m  0.0203\n",
            "     80        \u001b[36m1.0088\u001b[0m  0.0154\n",
            "     81        \u001b[36m1.0018\u001b[0m  0.0197\n",
            "     82        \u001b[36m0.9949\u001b[0m  0.0169\n",
            "     83        \u001b[36m0.9880\u001b[0m  0.0217\n",
            "     84        \u001b[36m0.9812\u001b[0m  0.0172\n",
            "     85        \u001b[36m0.9745\u001b[0m  0.0240\n",
            "     86        \u001b[36m0.9679\u001b[0m  0.0185\n",
            "     87        \u001b[36m0.9614\u001b[0m  0.0172\n",
            "     88        \u001b[36m0.9550\u001b[0m  0.0165\n",
            "     89        \u001b[36m0.9486\u001b[0m  0.0177\n",
            "     90        \u001b[36m0.9423\u001b[0m  0.0169\n",
            "     91        \u001b[36m0.9362\u001b[0m  0.0169\n",
            "     92        \u001b[36m0.9301\u001b[0m  0.0180\n",
            "     93        \u001b[36m0.9241\u001b[0m  0.0174\n",
            "     94        \u001b[36m0.9181\u001b[0m  0.0181\n",
            "     95        \u001b[36m0.9123\u001b[0m  0.0183\n",
            "     96        \u001b[36m0.9065\u001b[0m  0.0195\n",
            "     97        \u001b[36m0.9009\u001b[0m  0.0183\n",
            "     98        \u001b[36m0.8953\u001b[0m  0.0200\n",
            "     99        \u001b[36m0.8898\u001b[0m  0.0199\n",
            "    100        \u001b[36m0.8844\u001b[0m  0.0171\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.4683\u001b[0m  0.0163\n",
            "      2        \u001b[36m1.4584\u001b[0m  0.0158\n",
            "      3        \u001b[36m1.4486\u001b[0m  0.0152\n",
            "      4        \u001b[36m1.4389\u001b[0m  0.0168\n",
            "      5        \u001b[36m1.4291\u001b[0m  0.0200\n",
            "      6        \u001b[36m1.4194\u001b[0m  0.0236\n",
            "      7        \u001b[36m1.4097\u001b[0m  0.0163\n",
            "      8        \u001b[36m1.4001\u001b[0m  0.0154\n",
            "      9        \u001b[36m1.3905\u001b[0m  0.0165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.3810\u001b[0m  0.0189\n",
            "     11        \u001b[36m1.3714\u001b[0m  0.0169\n",
            "     12        \u001b[36m1.3620\u001b[0m  0.0159\n",
            "     13        \u001b[36m1.3525\u001b[0m  0.0152\n",
            "     14        \u001b[36m1.3431\u001b[0m  0.0161\n",
            "     15        \u001b[36m1.3338\u001b[0m  0.0163\n",
            "     16        \u001b[36m1.3244\u001b[0m  0.0166\n",
            "     17        \u001b[36m1.3152\u001b[0m  0.0151\n",
            "     18        \u001b[36m1.3059\u001b[0m  0.0166\n",
            "     19        \u001b[36m1.2967\u001b[0m  0.0172\n",
            "     20        \u001b[36m1.2876\u001b[0m  0.0161\n",
            "     21        \u001b[36m1.2785\u001b[0m  0.0180\n",
            "     22        \u001b[36m1.2695\u001b[0m  0.0167\n",
            "     23        \u001b[36m1.2605\u001b[0m  0.0202\n",
            "     24        \u001b[36m1.2515\u001b[0m  0.0160\n",
            "     25        \u001b[36m1.2426\u001b[0m  0.0171\n",
            "     26        \u001b[36m1.2338\u001b[0m  0.0177\n",
            "     27        \u001b[36m1.2250\u001b[0m  0.0157\n",
            "     28        \u001b[36m1.2163\u001b[0m  0.0164\n",
            "     29        \u001b[36m1.2076\u001b[0m  0.0152\n",
            "     30        \u001b[36m1.1990\u001b[0m  0.0162\n",
            "     31        \u001b[36m1.1904\u001b[0m  0.0156\n",
            "     32        \u001b[36m1.1819\u001b[0m  0.0164\n",
            "     33        \u001b[36m1.1734\u001b[0m  0.0236\n",
            "     34        \u001b[36m1.1650\u001b[0m  0.0241\n",
            "     35        \u001b[36m1.1567\u001b[0m  0.0198\n",
            "     36        \u001b[36m1.1484\u001b[0m  0.0172\n",
            "     37        \u001b[36m1.1402\u001b[0m  0.0179\n",
            "     38        \u001b[36m1.1321\u001b[0m  0.0200\n",
            "     39        \u001b[36m1.1240\u001b[0m  0.0216\n",
            "     40        \u001b[36m1.1160\u001b[0m  0.0215\n",
            "     41        \u001b[36m1.1080\u001b[0m  0.0178\n",
            "     42        \u001b[36m1.1001\u001b[0m  0.0158\n",
            "     43        \u001b[36m1.0923\u001b[0m  0.0150\n",
            "     44        \u001b[36m1.0846\u001b[0m  0.0161\n",
            "     45        \u001b[36m1.0769\u001b[0m  0.0123\n",
            "     46        \u001b[36m1.0693\u001b[0m  0.0187\n",
            "     47        \u001b[36m1.0618\u001b[0m  0.0190\n",
            "     48        \u001b[36m1.0543\u001b[0m  0.0173\n",
            "     49        \u001b[36m1.0469\u001b[0m  0.0128\n",
            "     50        \u001b[36m1.0396\u001b[0m  0.0174\n",
            "     51        \u001b[36m1.0324\u001b[0m  0.0170\n",
            "     52        \u001b[36m1.0252\u001b[0m  0.0177\n",
            "     53        \u001b[36m1.0181\u001b[0m  0.0191\n",
            "     54        \u001b[36m1.0111\u001b[0m  0.0158\n",
            "     55        \u001b[36m1.0042\u001b[0m  0.0161\n",
            "     56        \u001b[36m0.9973\u001b[0m  0.0177\n",
            "     57        \u001b[36m0.9906\u001b[0m  0.0170\n",
            "     58        \u001b[36m0.9839\u001b[0m  0.0166\n",
            "     59        \u001b[36m0.9773\u001b[0m  0.0153\n",
            "     60        \u001b[36m0.9707\u001b[0m  0.0152\n",
            "     61        \u001b[36m0.9643\u001b[0m  0.0166\n",
            "     62        \u001b[36m0.9579\u001b[0m  0.0153\n",
            "     63        \u001b[36m0.9516\u001b[0m  0.0162\n",
            "     64        \u001b[36m0.9454\u001b[0m  0.0170\n",
            "     65        \u001b[36m0.9393\u001b[0m  0.0174\n",
            "     66        \u001b[36m0.9332\u001b[0m  0.0187\n",
            "     67        \u001b[36m0.9273\u001b[0m  0.0173\n",
            "     68        \u001b[36m0.9214\u001b[0m  0.0159\n",
            "     69        \u001b[36m0.9156\u001b[0m  0.0160\n",
            "     70        \u001b[36m0.9099\u001b[0m  0.0190\n",
            "     71        \u001b[36m0.9043\u001b[0m  0.0203\n",
            "     72        \u001b[36m0.8988\u001b[0m  0.0177\n",
            "     73        \u001b[36m0.8933\u001b[0m  0.0233\n",
            "     74        \u001b[36m0.8880\u001b[0m  0.0214\n",
            "     75        \u001b[36m0.8827\u001b[0m  0.0228\n",
            "     76        \u001b[36m0.8775\u001b[0m  0.0209\n",
            "     77        \u001b[36m0.8724\u001b[0m  0.0148\n",
            "     78        \u001b[36m0.8674\u001b[0m  0.0177\n",
            "     79        \u001b[36m0.8624\u001b[0m  0.0283\n",
            "     80        \u001b[36m0.8576\u001b[0m  0.0336\n",
            "     81        \u001b[36m0.8528\u001b[0m  0.0126\n",
            "     82        \u001b[36m0.8481\u001b[0m  0.0270\n",
            "     83        \u001b[36m0.8435\u001b[0m  0.0222\n",
            "     84        \u001b[36m0.8390\u001b[0m  0.0285\n",
            "     85        \u001b[36m0.8345\u001b[0m  0.0214\n",
            "     86        \u001b[36m0.8302\u001b[0m  0.0123\n",
            "     87        \u001b[36m0.8259\u001b[0m  0.0171\n",
            "     88        \u001b[36m0.8217\u001b[0m  0.0149\n",
            "     89        \u001b[36m0.8176\u001b[0m  0.0194\n",
            "     90        \u001b[36m0.8136\u001b[0m  0.0170\n",
            "     91        \u001b[36m0.8096\u001b[0m  0.0197\n",
            "     92        \u001b[36m0.8058\u001b[0m  0.0151\n",
            "     93        \u001b[36m0.8020\u001b[0m  0.0129\n",
            "     94        \u001b[36m0.7982\u001b[0m  0.0169\n",
            "     95        \u001b[36m0.7946\u001b[0m  0.0140\n",
            "     96        \u001b[36m0.7910\u001b[0m  0.0155\n",
            "     97        \u001b[36m0.7876\u001b[0m  0.0117\n",
            "     98        \u001b[36m0.7841\u001b[0m  0.0162\n",
            "     99        \u001b[36m0.7808\u001b[0m  0.0162\n",
            "    100        \u001b[36m0.7775\u001b[0m  0.0228\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.9542\u001b[0m  0.0204\n",
            "      2        \u001b[36m2.9089\u001b[0m  0.0232\n",
            "      3        \u001b[36m2.8642\u001b[0m  0.0231\n",
            "      4        \u001b[36m2.8192\u001b[0m  0.0204\n",
            "      5        \u001b[36m2.7739\u001b[0m  0.0165\n",
            "      6        \u001b[36m2.7283\u001b[0m  0.0163\n",
            "      7        \u001b[36m2.6826\u001b[0m  0.0189\n",
            "      8        \u001b[36m2.6367\u001b[0m  0.0232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m2.5908\u001b[0m  0.0277\n",
            "     10        \u001b[36m2.5447\u001b[0m  0.0302\n",
            "     11        \u001b[36m2.4986\u001b[0m  0.0256\n",
            "     12        \u001b[36m2.4525\u001b[0m  0.0243\n",
            "     13        \u001b[36m2.4063\u001b[0m  0.0221\n",
            "     14        \u001b[36m2.3602\u001b[0m  0.0267\n",
            "     15        \u001b[36m2.3140\u001b[0m  0.0262\n",
            "     16        \u001b[36m2.2678\u001b[0m  0.0253\n",
            "     17        \u001b[36m2.2216\u001b[0m  0.0206\n",
            "     18        \u001b[36m2.1754\u001b[0m  0.0248\n",
            "     19        \u001b[36m2.1289\u001b[0m  0.0271\n",
            "     20        \u001b[36m2.0822\u001b[0m  0.0312\n",
            "     21        \u001b[36m2.0345\u001b[0m  0.0237\n",
            "     22        \u001b[36m1.9850\u001b[0m  0.0251\n",
            "     23        \u001b[36m1.9312\u001b[0m  0.0178\n",
            "     24        \u001b[36m1.8687\u001b[0m  0.0284\n",
            "     25        \u001b[36m1.7884\u001b[0m  0.0219\n",
            "     26        \u001b[36m1.6763\u001b[0m  0.0211\n",
            "     27        \u001b[36m1.5191\u001b[0m  0.0223\n",
            "     28        \u001b[36m1.3189\u001b[0m  0.0204\n",
            "     29        \u001b[36m1.0990\u001b[0m  0.0183\n",
            "     30        \u001b[36m0.8998\u001b[0m  0.0250\n",
            "     31        \u001b[36m0.7645\u001b[0m  0.0210\n",
            "     32        \u001b[36m0.6994\u001b[0m  0.0305\n",
            "     33        \u001b[36m0.6756\u001b[0m  0.0232\n",
            "     34        \u001b[36m0.6674\u001b[0m  0.0257\n",
            "     35        \u001b[36m0.6639\u001b[0m  0.0167\n",
            "     36        \u001b[36m0.6624\u001b[0m  0.0222\n",
            "     37        \u001b[36m0.6618\u001b[0m  0.0192\n",
            "     38        \u001b[36m0.6617\u001b[0m  0.0213\n",
            "     39        0.6619  0.0222\n",
            "     40        0.6620  0.0240\n",
            "     41        0.6621  0.0226\n",
            "     42        0.6622  0.0230\n",
            "     43        0.6622  0.0202\n",
            "     44        0.6622  0.0198\n",
            "     45        0.6622  0.0263\n",
            "     46        0.6622  0.0216\n",
            "     47        0.6622  0.0162\n",
            "     48        0.6622  0.0256\n",
            "     49        0.6621  0.0292\n",
            "     50        0.6621  0.0261\n",
            "     51        0.6621  0.0226\n",
            "     52        0.6621  0.0202\n",
            "     53        0.6621  0.0347\n",
            "     54        0.6621  0.0289\n",
            "     55        0.6621  0.0251\n",
            "     56        0.6621  0.0183\n",
            "     57        0.6621  0.0277\n",
            "     58        0.6621  0.0314\n",
            "     59        0.6621  0.0272\n",
            "     60        0.6621  0.0254\n",
            "     61        0.6621  0.0187\n",
            "     62        0.6621  0.0187\n",
            "     63        0.6621  0.0191\n",
            "     64        0.6621  0.0207\n",
            "     65        0.6621  0.0294\n",
            "     66        0.6621  0.0188\n",
            "     67        0.6621  0.0263\n",
            "     68        0.6621  0.0185\n",
            "     69        0.6620  0.0209\n",
            "     70        0.6620  0.0271\n",
            "     71        0.6620  0.0245\n",
            "     72        0.6620  0.0227\n",
            "     73        0.6620  0.0208\n",
            "     74        0.6620  0.0197\n",
            "     75        0.6620  0.0217\n",
            "     76        0.6620  0.0234\n",
            "     77        0.6620  0.0280\n",
            "     78        0.6620  0.0216\n",
            "     79        0.6620  0.0216\n",
            "     80        0.6620  0.0233\n",
            "     81        0.6620  0.0226\n",
            "     82        0.6620  0.0184\n",
            "     83        0.6620  0.0230\n",
            "     84        0.6620  0.0201\n",
            "     85        0.6620  0.0181\n",
            "     86        0.6620  0.0180\n",
            "     87        0.6620  0.0268\n",
            "     88        0.6620  0.0264\n",
            "     89        0.6620  0.0253\n",
            "     90        0.6620  0.0251\n",
            "     91        0.6620  0.0201\n",
            "     92        0.6620  0.0251\n",
            "     93        0.6620  0.0342\n",
            "     94        0.6620  0.0249\n",
            "     95        0.6620  0.0207\n",
            "     96        0.6620  0.0307\n",
            "     97        0.6620  0.0249\n",
            "     98        0.6620  0.0174\n",
            "     99        0.6620  0.0248\n",
            "    100        0.6620  0.0150\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.2733\u001b[0m  0.0217\n",
            "      2        \u001b[36m2.2264\u001b[0m  0.0245\n",
            "      3        \u001b[36m2.1824\u001b[0m  0.0198\n",
            "      4        \u001b[36m2.1385\u001b[0m  0.0244\n",
            "      5        \u001b[36m2.0946\u001b[0m  0.0166\n",
            "      6        \u001b[36m2.0505\u001b[0m  0.0219\n",
            "      7        \u001b[36m2.0064\u001b[0m  0.0212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m1.9623\u001b[0m  0.0236\n",
            "      9        \u001b[36m1.9182\u001b[0m  0.0236\n",
            "     10        \u001b[36m1.8741\u001b[0m  0.0231\n",
            "     11        \u001b[36m1.8302\u001b[0m  0.0151\n",
            "     12        \u001b[36m1.7863\u001b[0m  0.0248\n",
            "     13        \u001b[36m1.7426\u001b[0m  0.0154\n",
            "     14        \u001b[36m1.6990\u001b[0m  0.0213\n",
            "     15        \u001b[36m1.6557\u001b[0m  0.0197\n",
            "     16        \u001b[36m1.6125\u001b[0m  0.0208\n",
            "     17        \u001b[36m1.5694\u001b[0m  0.0204\n",
            "     18        \u001b[36m1.5264\u001b[0m  0.0237\n",
            "     19        \u001b[36m1.4831\u001b[0m  0.0194\n",
            "     20        \u001b[36m1.4391\u001b[0m  0.0254\n",
            "     21        \u001b[36m1.3930\u001b[0m  0.0266\n",
            "     22        \u001b[36m1.3425\u001b[0m  0.0269\n",
            "     23        \u001b[36m1.2833\u001b[0m  0.0238\n",
            "     24        \u001b[36m1.2084\u001b[0m  0.0207\n",
            "     25        \u001b[36m1.1101\u001b[0m  0.0218\n",
            "     26        \u001b[36m0.9892\u001b[0m  0.0264\n",
            "     27        \u001b[36m0.8650\u001b[0m  0.0230\n",
            "     28        \u001b[36m0.7675\u001b[0m  0.0234\n",
            "     29        \u001b[36m0.7099\u001b[0m  0.0269\n",
            "     30        \u001b[36m0.6824\u001b[0m  0.0232\n",
            "     31        \u001b[36m0.6705\u001b[0m  0.0250\n",
            "     32        \u001b[36m0.6654\u001b[0m  0.0287\n",
            "     33        \u001b[36m0.6630\u001b[0m  0.0330\n",
            "     34        \u001b[36m0.6618\u001b[0m  0.0233\n",
            "     35        \u001b[36m0.6613\u001b[0m  0.0225\n",
            "     36        \u001b[36m0.6611\u001b[0m  0.0222\n",
            "     37        \u001b[36m0.6610\u001b[0m  0.0230\n",
            "     38        \u001b[36m0.6610\u001b[0m  0.0235\n",
            "     39        0.6610  0.0262\n",
            "     40        0.6610  0.0213\n",
            "     41        0.6610  0.0249\n",
            "     42        0.6610  0.0182\n",
            "     43        0.6610  0.0226\n",
            "     44        0.6611  0.0218\n",
            "     45        0.6611  0.0198\n",
            "     46        0.6611  0.0225\n",
            "     47        0.6610  0.0207\n",
            "     48        0.6610  0.0174\n",
            "     49        0.6610  0.0208\n",
            "     50        0.6610  0.0214\n",
            "     51        0.6610  0.0242\n",
            "     52        0.6610  0.0231\n",
            "     53        0.6610  0.0261\n",
            "     54        0.6610  0.0241\n",
            "     55        0.6610  0.0233\n",
            "     56        0.6610  0.0227\n",
            "     57        0.6610  0.0184\n",
            "     58        0.6610  0.0300\n",
            "     59        0.6610  0.0242\n",
            "     60        0.6610  0.0316\n",
            "     61        0.6610  0.0234\n",
            "     62        0.6610  0.0195\n",
            "     63        0.6610  0.0179\n",
            "     64        0.6610  0.0291\n",
            "     65        0.6610  0.0249\n",
            "     66        0.6610  0.0243\n",
            "     67        0.6610  0.0250\n",
            "     68        0.6610  0.0175\n",
            "     69        0.6610  0.0250\n",
            "     70        0.6610  0.0163\n",
            "     71        0.6610  0.0221\n",
            "     72        0.6610  0.0218\n",
            "     73        0.6610  0.0203\n",
            "     74        0.6610  0.0254\n",
            "     75        0.6610  0.0217\n",
            "     76        0.6614  0.0206\n",
            "     77        0.6610  0.0195\n",
            "     78        0.6610  0.0213\n",
            "     79        0.6610  0.0277\n",
            "     80        0.6610  0.0206\n",
            "     81        0.6610  0.0169\n",
            "     82        0.6618  0.0212\n",
            "     83        0.6610  0.0184\n",
            "     84        0.6610  0.0224\n",
            "     85        0.6610  0.0209\n",
            "     86        0.6610  0.0209\n",
            "     87        0.6610  0.0206\n",
            "     88        0.6610  0.0226\n",
            "     89        0.6610  0.0249\n",
            "     90        0.6610  0.0235\n",
            "     91        0.6610  0.0236\n",
            "     92        0.6610  0.0233\n",
            "     93        0.6610  0.0211\n",
            "     94        0.6610  0.0217\n",
            "     95        0.6610  0.0235\n",
            "     96        0.6610  0.0211\n",
            "     97        0.6610  0.0233\n",
            "     98        0.6610  0.0171\n",
            "     99        0.6610  0.0215\n",
            "    100        0.6610  0.0197\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.7450\u001b[0m  0.0192\n",
            "      2        \u001b[36m2.7239\u001b[0m  0.0174\n",
            "      3        \u001b[36m2.7028\u001b[0m  0.0171\n",
            "      4        \u001b[36m2.6817\u001b[0m  0.0187\n",
            "      5        \u001b[36m2.6606\u001b[0m  0.0131\n",
            "      6        \u001b[36m2.6395\u001b[0m  0.0181\n",
            "      7        \u001b[36m2.6184\u001b[0m  0.0186\n",
            "      8        \u001b[36m2.5973\u001b[0m  0.0185\n",
            "      9        \u001b[36m2.5762\u001b[0m  0.0224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m2.5551\u001b[0m  0.0177\n",
            "     11        \u001b[36m2.5341\u001b[0m  0.0177\n",
            "     12        \u001b[36m2.5130\u001b[0m  0.0218\n",
            "     13        \u001b[36m2.4920\u001b[0m  0.0231\n",
            "     14        \u001b[36m2.4709\u001b[0m  0.0197\n",
            "     15        \u001b[36m2.4499\u001b[0m  0.0169\n",
            "     16        \u001b[36m2.4289\u001b[0m  0.0131\n",
            "     17        \u001b[36m2.4079\u001b[0m  0.0154\n",
            "     18        \u001b[36m2.3869\u001b[0m  0.0152\n",
            "     19        \u001b[36m2.3659\u001b[0m  0.0154\n",
            "     20        \u001b[36m2.3449\u001b[0m  0.0157\n",
            "     21        \u001b[36m2.3239\u001b[0m  0.0153\n",
            "     22        \u001b[36m2.3030\u001b[0m  0.0160\n",
            "     23        \u001b[36m2.2820\u001b[0m  0.0177\n",
            "     24        \u001b[36m2.2611\u001b[0m  0.0253\n",
            "     25        \u001b[36m2.2402\u001b[0m  0.0166\n",
            "     26        \u001b[36m2.2193\u001b[0m  0.0167\n",
            "     27        \u001b[36m2.1984\u001b[0m  0.0167\n",
            "     28        \u001b[36m2.1775\u001b[0m  0.0166\n",
            "     29        \u001b[36m2.1567\u001b[0m  0.0170\n",
            "     30        \u001b[36m2.1358\u001b[0m  0.0160\n",
            "     31        \u001b[36m2.1150\u001b[0m  0.0160\n",
            "     32        \u001b[36m2.0943\u001b[0m  0.0152\n",
            "     33        \u001b[36m2.0735\u001b[0m  0.0156\n",
            "     34        \u001b[36m2.0528\u001b[0m  0.0186\n",
            "     35        \u001b[36m2.0321\u001b[0m  0.0154\n",
            "     36        \u001b[36m2.0114\u001b[0m  0.0171\n",
            "     37        \u001b[36m1.9907\u001b[0m  0.0168\n",
            "     38        \u001b[36m1.9701\u001b[0m  0.0156\n",
            "     39        \u001b[36m1.9495\u001b[0m  0.0191\n",
            "     40        \u001b[36m1.9290\u001b[0m  0.0158\n",
            "     41        \u001b[36m1.9084\u001b[0m  0.0183\n",
            "     42        \u001b[36m1.8880\u001b[0m  0.0171\n",
            "     43        \u001b[36m1.8675\u001b[0m  0.0188\n",
            "     44        \u001b[36m1.8471\u001b[0m  0.0196\n",
            "     45        \u001b[36m1.8268\u001b[0m  0.0187\n",
            "     46        \u001b[36m1.8065\u001b[0m  0.0191\n",
            "     47        \u001b[36m1.7862\u001b[0m  0.0187\n",
            "     48        \u001b[36m1.7660\u001b[0m  0.0190\n",
            "     49        \u001b[36m1.7458\u001b[0m  0.0189\n",
            "     50        \u001b[36m1.7258\u001b[0m  0.0166\n",
            "     51        \u001b[36m1.7057\u001b[0m  0.0181\n",
            "     52        \u001b[36m1.6858\u001b[0m  0.0200\n",
            "     53        \u001b[36m1.6658\u001b[0m  0.0187\n",
            "     54        \u001b[36m1.6460\u001b[0m  0.0169\n",
            "     55        \u001b[36m1.6263\u001b[0m  0.0136\n",
            "     56        \u001b[36m1.6066\u001b[0m  0.0176\n",
            "     57        \u001b[36m1.5870\u001b[0m  0.0171\n",
            "     58        \u001b[36m1.5675\u001b[0m  0.0161\n",
            "     59        \u001b[36m1.5481\u001b[0m  0.0212\n",
            "     60        \u001b[36m1.5287\u001b[0m  0.0213\n",
            "     61        \u001b[36m1.5095\u001b[0m  0.0199\n",
            "     62        \u001b[36m1.4904\u001b[0m  0.0178\n",
            "     63        \u001b[36m1.4714\u001b[0m  0.0192\n",
            "     64        \u001b[36m1.4525\u001b[0m  0.0248\n",
            "     65        \u001b[36m1.4337\u001b[0m  0.0197\n",
            "     66        \u001b[36m1.4151\u001b[0m  0.0159\n",
            "     67        \u001b[36m1.3965\u001b[0m  0.0146\n",
            "     68        \u001b[36m1.3781\u001b[0m  0.0171\n",
            "     69        \u001b[36m1.3599\u001b[0m  0.0162\n",
            "     70        \u001b[36m1.3418\u001b[0m  0.0161\n",
            "     71        \u001b[36m1.3239\u001b[0m  0.0165\n",
            "     72        \u001b[36m1.3061\u001b[0m  0.0153\n",
            "     73        \u001b[36m1.2885\u001b[0m  0.0195\n",
            "     74        \u001b[36m1.2710\u001b[0m  0.0169\n",
            "     75        \u001b[36m1.2538\u001b[0m  0.0166\n",
            "     76        \u001b[36m1.2367\u001b[0m  0.0169\n",
            "     77        \u001b[36m1.2198\u001b[0m  0.0167\n",
            "     78        \u001b[36m1.2032\u001b[0m  0.0185\n",
            "     79        \u001b[36m1.1867\u001b[0m  0.0163\n",
            "     80        \u001b[36m1.1704\u001b[0m  0.0169\n",
            "     81        \u001b[36m1.1544\u001b[0m  0.0162\n",
            "     82        \u001b[36m1.1386\u001b[0m  0.0172\n",
            "     83        \u001b[36m1.1230\u001b[0m  0.0162\n",
            "     84        \u001b[36m1.1077\u001b[0m  0.0152\n",
            "     85        \u001b[36m1.0926\u001b[0m  0.0155\n",
            "     86        \u001b[36m1.0778\u001b[0m  0.0158\n",
            "     87        \u001b[36m1.0633\u001b[0m  0.0167\n",
            "     88        \u001b[36m1.0490\u001b[0m  0.0176\n",
            "     89        \u001b[36m1.0350\u001b[0m  0.0177\n",
            "     90        \u001b[36m1.0212\u001b[0m  0.0167\n",
            "     91        \u001b[36m1.0078\u001b[0m  0.0180\n",
            "     92        \u001b[36m0.9946\u001b[0m  0.0192\n",
            "     93        \u001b[36m0.9817\u001b[0m  0.0193\n",
            "     94        \u001b[36m0.9692\u001b[0m  0.0177\n",
            "     95        \u001b[36m0.9569\u001b[0m  0.0170\n",
            "     96        \u001b[36m0.9449\u001b[0m  0.0184\n",
            "     97        \u001b[36m0.9333\u001b[0m  0.0184\n",
            "     98        \u001b[36m0.9219\u001b[0m  0.0239\n",
            "     99        \u001b[36m0.9109\u001b[0m  0.0200\n",
            "    100        \u001b[36m0.9001\u001b[0m  0.0194\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.9134\u001b[0m  0.0209\n",
            "      2        \u001b[36m2.8925\u001b[0m  0.0168\n",
            "      3        \u001b[36m2.8716\u001b[0m  0.0171\n",
            "      4        \u001b[36m2.8507\u001b[0m  0.0256\n",
            "      5        \u001b[36m2.8298\u001b[0m  0.0265\n",
            "      6        \u001b[36m2.8089\u001b[0m  0.0206\n",
            "      7        \u001b[36m2.7880\u001b[0m  0.0167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m2.7671\u001b[0m  0.0198\n",
            "      9        \u001b[36m2.7463\u001b[0m  0.0168\n",
            "     10        \u001b[36m2.7254\u001b[0m  0.0168\n",
            "     11        \u001b[36m2.7045\u001b[0m  0.0150\n",
            "     12        \u001b[36m2.6836\u001b[0m  0.0151\n",
            "     13        \u001b[36m2.6628\u001b[0m  0.0151\n",
            "     14        \u001b[36m2.6419\u001b[0m  0.0160\n",
            "     15        \u001b[36m2.6211\u001b[0m  0.0149\n",
            "     16        \u001b[36m2.6003\u001b[0m  0.0150\n",
            "     17        \u001b[36m2.5794\u001b[0m  0.0154\n",
            "     18        \u001b[36m2.5586\u001b[0m  0.0145\n",
            "     19        \u001b[36m2.5377\u001b[0m  0.0149\n",
            "     20        \u001b[36m2.5169\u001b[0m  0.0159\n",
            "     21        \u001b[36m2.4961\u001b[0m  0.0164\n",
            "     22        \u001b[36m2.4753\u001b[0m  0.0165\n",
            "     23        \u001b[36m2.4545\u001b[0m  0.0201\n",
            "     24        \u001b[36m2.4337\u001b[0m  0.0174\n",
            "     25        \u001b[36m2.4129\u001b[0m  0.0174\n",
            "     26        \u001b[36m2.3922\u001b[0m  0.0175\n",
            "     27        \u001b[36m2.3714\u001b[0m  0.0164\n",
            "     28        \u001b[36m2.3507\u001b[0m  0.0168\n",
            "     29        \u001b[36m2.3299\u001b[0m  0.0198\n",
            "     30        \u001b[36m2.3092\u001b[0m  0.0171\n",
            "     31        \u001b[36m2.2885\u001b[0m  0.0159\n",
            "     32        \u001b[36m2.2678\u001b[0m  0.0178\n",
            "     33        \u001b[36m2.2471\u001b[0m  0.0175\n",
            "     34        \u001b[36m2.2265\u001b[0m  0.0167\n",
            "     35        \u001b[36m2.2058\u001b[0m  0.0167\n",
            "     36        \u001b[36m2.1852\u001b[0m  0.0170\n",
            "     37        \u001b[36m2.1646\u001b[0m  0.0200\n",
            "     38        \u001b[36m2.1440\u001b[0m  0.0197\n",
            "     39        \u001b[36m2.1234\u001b[0m  0.0180\n",
            "     40        \u001b[36m2.1028\u001b[0m  0.0209\n",
            "     41        \u001b[36m2.0823\u001b[0m  0.0177\n",
            "     42        \u001b[36m2.0618\u001b[0m  0.0188\n",
            "     43        \u001b[36m2.0413\u001b[0m  0.0187\n",
            "     44        \u001b[36m2.0208\u001b[0m  0.0177\n",
            "     45        \u001b[36m2.0004\u001b[0m  0.0236\n",
            "     46        \u001b[36m1.9800\u001b[0m  0.0225\n",
            "     47        \u001b[36m1.9596\u001b[0m  0.0226\n",
            "     48        \u001b[36m1.9393\u001b[0m  0.0157\n",
            "     49        \u001b[36m1.9190\u001b[0m  0.0165\n",
            "     50        \u001b[36m1.8987\u001b[0m  0.0171\n",
            "     51        \u001b[36m1.8785\u001b[0m  0.0160\n",
            "     52        \u001b[36m1.8583\u001b[0m  0.0160\n",
            "     53        \u001b[36m1.8381\u001b[0m  0.0216\n",
            "     54        \u001b[36m1.8180\u001b[0m  0.0215\n",
            "     55        \u001b[36m1.7979\u001b[0m  0.0198\n",
            "     56        \u001b[36m1.7779\u001b[0m  0.0171\n",
            "     57        \u001b[36m1.7580\u001b[0m  0.0161\n",
            "     58        \u001b[36m1.7381\u001b[0m  0.0235\n",
            "     59        \u001b[36m1.7182\u001b[0m  0.0164\n",
            "     60        \u001b[36m1.6984\u001b[0m  0.0155\n",
            "     61        \u001b[36m1.6787\u001b[0m  0.0152\n",
            "     62        \u001b[36m1.6590\u001b[0m  0.0150\n",
            "     63        \u001b[36m1.6394\u001b[0m  0.0151\n",
            "     64        \u001b[36m1.6199\u001b[0m  0.0144\n",
            "     65        \u001b[36m1.6005\u001b[0m  0.0150\n",
            "     66        \u001b[36m1.5811\u001b[0m  0.0148\n",
            "     67        \u001b[36m1.5619\u001b[0m  0.0153\n",
            "     68        \u001b[36m1.5427\u001b[0m  0.0187\n",
            "     69        \u001b[36m1.5236\u001b[0m  0.0147\n",
            "     70        \u001b[36m1.5046\u001b[0m  0.0175\n",
            "     71        \u001b[36m1.4857\u001b[0m  0.0185\n",
            "     72        \u001b[36m1.4669\u001b[0m  0.0184\n",
            "     73        \u001b[36m1.4483\u001b[0m  0.0190\n",
            "     74        \u001b[36m1.4297\u001b[0m  0.0171\n",
            "     75        \u001b[36m1.4113\u001b[0m  0.0172\n",
            "     76        \u001b[36m1.3930\u001b[0m  0.0168\n",
            "     77        \u001b[36m1.3748\u001b[0m  0.0188\n",
            "     78        \u001b[36m1.3568\u001b[0m  0.0189\n",
            "     79        \u001b[36m1.3389\u001b[0m  0.0174\n",
            "     80        \u001b[36m1.3212\u001b[0m  0.0200\n",
            "     81        \u001b[36m1.3036\u001b[0m  0.0174\n",
            "     82        \u001b[36m1.2862\u001b[0m  0.0195\n",
            "     83        \u001b[36m1.2690\u001b[0m  0.0195\n",
            "     84        \u001b[36m1.2519\u001b[0m  0.0174\n",
            "     85        \u001b[36m1.2350\u001b[0m  0.0166\n",
            "     86        \u001b[36m1.2184\u001b[0m  0.0231\n",
            "     87        \u001b[36m1.2019\u001b[0m  0.0174\n",
            "     88        \u001b[36m1.1856\u001b[0m  0.0164\n",
            "     89        \u001b[36m1.1695\u001b[0m  0.0183\n",
            "     90        \u001b[36m1.1537\u001b[0m  0.0257\n",
            "     91        \u001b[36m1.1380\u001b[0m  0.0199\n",
            "     92        \u001b[36m1.1226\u001b[0m  0.0291\n",
            "     93        \u001b[36m1.1075\u001b[0m  0.0250\n",
            "     94        \u001b[36m1.0926\u001b[0m  0.0213\n",
            "     95        \u001b[36m1.0779\u001b[0m  0.0208\n",
            "     96        \u001b[36m1.0635\u001b[0m  0.0214\n",
            "     97        \u001b[36m1.0493\u001b[0m  0.0303\n",
            "     98        \u001b[36m1.0354\u001b[0m  0.0207\n",
            "     99        \u001b[36m1.0218\u001b[0m  0.0237\n",
            "    100        \u001b[36m1.0085\u001b[0m  0.0271\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.4538\u001b[0m  0.0221\n",
            "      2        \u001b[36m1.4004\u001b[0m  0.0192\n",
            "      3        \u001b[36m1.3573\u001b[0m  0.0227\n",
            "      4        \u001b[36m1.3117\u001b[0m  0.0207\n",
            "      5        \u001b[36m1.2675\u001b[0m  0.0211\n",
            "      6        \u001b[36m1.2263\u001b[0m  0.0228\n",
            "      7        \u001b[36m1.1843\u001b[0m  0.0254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m1.1428\u001b[0m  0.0243\n",
            "      9        \u001b[36m1.1018\u001b[0m  0.0217\n",
            "     10        \u001b[36m1.0576\u001b[0m  0.0218\n",
            "     11        \u001b[36m1.0200\u001b[0m  0.0217\n",
            "     12        \u001b[36m0.9841\u001b[0m  0.0223\n",
            "     13        \u001b[36m0.9497\u001b[0m  0.0221\n",
            "     14        \u001b[36m0.9186\u001b[0m  0.0223\n",
            "     15        \u001b[36m0.8905\u001b[0m  0.0283\n",
            "     16        \u001b[36m0.8593\u001b[0m  0.0290\n",
            "     17        \u001b[36m0.8374\u001b[0m  0.0226\n",
            "     18        \u001b[36m0.8148\u001b[0m  0.0245\n",
            "     19        \u001b[36m0.7994\u001b[0m  0.0277\n",
            "     20        \u001b[36m0.7868\u001b[0m  0.0248\n",
            "     21        \u001b[36m0.7698\u001b[0m  0.0234\n",
            "     22        \u001b[36m0.7486\u001b[0m  0.0249\n",
            "     23        \u001b[36m0.7418\u001b[0m  0.0216\n",
            "     24        \u001b[36m0.7355\u001b[0m  0.0209\n",
            "     25        \u001b[36m0.7309\u001b[0m  0.0213\n",
            "     26        \u001b[36m0.7270\u001b[0m  0.0232\n",
            "     27        \u001b[36m0.7258\u001b[0m  0.0222\n",
            "     28        \u001b[36m0.7227\u001b[0m  0.0237\n",
            "     29        \u001b[36m0.7202\u001b[0m  0.0272\n",
            "     30        \u001b[36m0.7184\u001b[0m  0.0249\n",
            "     31        \u001b[36m0.7160\u001b[0m  0.0207\n",
            "     32        \u001b[36m0.7139\u001b[0m  0.0197\n",
            "     33        \u001b[36m0.7104\u001b[0m  0.0245\n",
            "     34        \u001b[36m0.7083\u001b[0m  0.0215\n",
            "     35        \u001b[36m0.7064\u001b[0m  0.0276\n",
            "     36        \u001b[36m0.7046\u001b[0m  0.0242\n",
            "     37        \u001b[36m0.7028\u001b[0m  0.0224\n",
            "     38        \u001b[36m0.7011\u001b[0m  0.0274\n",
            "     39        \u001b[36m0.6995\u001b[0m  0.0233\n",
            "     40        \u001b[36m0.6979\u001b[0m  0.0213\n",
            "     41        \u001b[36m0.6964\u001b[0m  0.0162\n",
            "     42        \u001b[36m0.6949\u001b[0m  0.0262\n",
            "     43        \u001b[36m0.6935\u001b[0m  0.0246\n",
            "     44        \u001b[36m0.6921\u001b[0m  0.0315\n",
            "     45        \u001b[36m0.6907\u001b[0m  0.0225\n",
            "     46        \u001b[36m0.6894\u001b[0m  0.0253\n",
            "     47        \u001b[36m0.6881\u001b[0m  0.0244\n",
            "     48        \u001b[36m0.6853\u001b[0m  0.0237\n",
            "     49        \u001b[36m0.6842\u001b[0m  0.0220\n",
            "     50        \u001b[36m0.6819\u001b[0m  0.0230\n",
            "     51        \u001b[36m0.6817\u001b[0m  0.0222\n",
            "     52        \u001b[36m0.6801\u001b[0m  0.0158\n",
            "     53        \u001b[36m0.6750\u001b[0m  0.0248\n",
            "     54        \u001b[36m0.6735\u001b[0m  0.0269\n",
            "     55        \u001b[36m0.6722\u001b[0m  0.0272\n",
            "     56        \u001b[36m0.6707\u001b[0m  0.0248\n",
            "     57        \u001b[36m0.6693\u001b[0m  0.0211\n",
            "     58        \u001b[36m0.6680\u001b[0m  0.0205\n",
            "     59        \u001b[36m0.6667\u001b[0m  0.0292\n",
            "     60        \u001b[36m0.6654\u001b[0m  0.0280\n",
            "     61        \u001b[36m0.6642\u001b[0m  0.0272\n",
            "     62        \u001b[36m0.6630\u001b[0m  0.0238\n",
            "     63        \u001b[36m0.6619\u001b[0m  0.0229\n",
            "     64        \u001b[36m0.6607\u001b[0m  0.0177\n",
            "     65        \u001b[36m0.6597\u001b[0m  0.0208\n",
            "     66        \u001b[36m0.6586\u001b[0m  0.0220\n",
            "     67        \u001b[36m0.6576\u001b[0m  0.0244\n",
            "     68        \u001b[36m0.6566\u001b[0m  0.0236\n",
            "     69        \u001b[36m0.6557\u001b[0m  0.0185\n",
            "     70        \u001b[36m0.6548\u001b[0m  0.0185\n",
            "     71        \u001b[36m0.6539\u001b[0m  0.0185\n",
            "     72        \u001b[36m0.6530\u001b[0m  0.0203\n",
            "     73        \u001b[36m0.6467\u001b[0m  0.0348\n",
            "     74        0.6557  0.0264\n",
            "     75        0.6610  0.0216\n",
            "     76        \u001b[36m0.6414\u001b[0m  0.0186\n",
            "     77        0.6953  0.0214\n",
            "     78        0.7100  0.0264\n",
            "     79        0.6469  0.0252\n",
            "     80        0.6425  0.0184\n",
            "     81        0.6468  0.0208\n",
            "     82        0.6472  0.0213\n",
            "     83        \u001b[36m0.6370\u001b[0m  0.0220\n",
            "     84        \u001b[36m0.6366\u001b[0m  0.0347\n",
            "     85        0.6387  0.0242\n",
            "     86        \u001b[36m0.6355\u001b[0m  0.0169\n",
            "     87        0.6379  0.0241\n",
            "     88        \u001b[36m0.6312\u001b[0m  0.0293\n",
            "     89        0.6347  0.0227\n",
            "     90        0.6453  0.0171\n",
            "     91        0.6635  0.0280\n",
            "     92        0.6812  0.0207\n",
            "     93        0.6855  0.0182\n",
            "     94        0.6859  0.0220\n",
            "     95        0.7101  0.0239\n",
            "     96        0.7137  0.0246\n",
            "     97        0.7134  0.0197\n",
            "     98        0.6892  0.0197\n",
            "     99        0.6646  0.0263\n",
            "    100        0.6613  0.0251\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7571\u001b[0m  0.0238\n",
            "      2        \u001b[36m0.7330\u001b[0m  0.0255\n",
            "      3        \u001b[36m0.7185\u001b[0m  0.0183\n",
            "      4        \u001b[36m0.7042\u001b[0m  0.0265\n",
            "      5        \u001b[36m0.6932\u001b[0m  0.0329\n",
            "      6        \u001b[36m0.6846\u001b[0m  0.0181\n",
            "      7        \u001b[36m0.6768\u001b[0m  0.0225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.6699\u001b[0m  0.0228\n",
            "      9        \u001b[36m0.6637\u001b[0m  0.0238\n",
            "     10        \u001b[36m0.6581\u001b[0m  0.0324\n",
            "     11        \u001b[36m0.6529\u001b[0m  0.0209\n",
            "     12        \u001b[36m0.6454\u001b[0m  0.0280\n",
            "     13        \u001b[36m0.6409\u001b[0m  0.0225\n",
            "     14        \u001b[36m0.6366\u001b[0m  0.0244\n",
            "     15        \u001b[36m0.6325\u001b[0m  0.0252\n",
            "     16        \u001b[36m0.6286\u001b[0m  0.0215\n",
            "     17        \u001b[36m0.6239\u001b[0m  0.0206\n",
            "     18        \u001b[36m0.6202\u001b[0m  0.0254\n",
            "     19        \u001b[36m0.6165\u001b[0m  0.0256\n",
            "     20        \u001b[36m0.6129\u001b[0m  0.0217\n",
            "     21        \u001b[36m0.6095\u001b[0m  0.0237\n",
            "     22        \u001b[36m0.6059\u001b[0m  0.0206\n",
            "     23        \u001b[36m0.5939\u001b[0m  0.0221\n",
            "     24        \u001b[36m0.5908\u001b[0m  0.0351\n",
            "     25        \u001b[36m0.5879\u001b[0m  0.0217\n",
            "     26        \u001b[36m0.5850\u001b[0m  0.0282\n",
            "     27        \u001b[36m0.5823\u001b[0m  0.0257\n",
            "     28        \u001b[36m0.5795\u001b[0m  0.0202\n",
            "     29        \u001b[36m0.5769\u001b[0m  0.0252\n",
            "     30        \u001b[36m0.5743\u001b[0m  0.0280\n",
            "     31        \u001b[36m0.5718\u001b[0m  0.0286\n",
            "     32        \u001b[36m0.5684\u001b[0m  0.0303\n",
            "     33        \u001b[36m0.5659\u001b[0m  0.0235\n",
            "     34        \u001b[36m0.5601\u001b[0m  0.0194\n",
            "     35        \u001b[36m0.5579\u001b[0m  0.0286\n",
            "     36        \u001b[36m0.5567\u001b[0m  0.0326\n",
            "     37        \u001b[36m0.5551\u001b[0m  0.0248\n",
            "     38        \u001b[36m0.5526\u001b[0m  0.0240\n",
            "     39        \u001b[36m0.5506\u001b[0m  0.0239\n",
            "     40        \u001b[36m0.5482\u001b[0m  0.0252\n",
            "     41        \u001b[36m0.5463\u001b[0m  0.0279\n",
            "     42        \u001b[36m0.5444\u001b[0m  0.0266\n",
            "     43        \u001b[36m0.5429\u001b[0m  0.0213\n",
            "     44        \u001b[36m0.5408\u001b[0m  0.0221\n",
            "     45        \u001b[36m0.5392\u001b[0m  0.0305\n",
            "     46        \u001b[36m0.5376\u001b[0m  0.0339\n",
            "     47        \u001b[36m0.5361\u001b[0m  0.0248\n",
            "     48        \u001b[36m0.5346\u001b[0m  0.0217\n",
            "     49        \u001b[36m0.5332\u001b[0m  0.0314\n",
            "     50        0.5351  0.0275\n",
            "     51        \u001b[36m0.5305\u001b[0m  0.0247\n",
            "     52        \u001b[36m0.5292\u001b[0m  0.0235\n",
            "     53        \u001b[36m0.5280\u001b[0m  0.0219\n",
            "     54        \u001b[36m0.5268\u001b[0m  0.0185\n",
            "     55        \u001b[36m0.5256\u001b[0m  0.0236\n",
            "     56        \u001b[36m0.5245\u001b[0m  0.0283\n",
            "     57        \u001b[36m0.5234\u001b[0m  0.0236\n",
            "     58        \u001b[36m0.5224\u001b[0m  0.0255\n",
            "     59        \u001b[36m0.5214\u001b[0m  0.0185\n",
            "     60        \u001b[36m0.5204\u001b[0m  0.0247\n",
            "     61        \u001b[36m0.5199\u001b[0m  0.0248\n",
            "     62        \u001b[36m0.5168\u001b[0m  0.0261\n",
            "     63        0.5343  0.0208\n",
            "     64        0.5340  0.0236\n",
            "     65        0.5352  0.0256\n",
            "     66        0.5343  0.0177\n",
            "     67        0.5334  0.0314\n",
            "     68        0.5325  0.0243\n",
            "     69        0.5317  0.0255\n",
            "     70        0.5309  0.0208\n",
            "     71        0.5301  0.0182\n",
            "     72        0.5293  0.0206\n",
            "     73        0.5286  0.0239\n",
            "     74        0.5279  0.0249\n",
            "     75        0.5273  0.0212\n",
            "     76        0.5258  0.0198\n",
            "     77        0.5251  0.0200\n",
            "     78        0.5245  0.0254\n",
            "     79        0.5239  0.0253\n",
            "     80        0.5233  0.0167\n",
            "     81        0.5227  0.0201\n",
            "     82        0.5222  0.0225\n",
            "     83        0.5216  0.0223\n",
            "     84        0.5211  0.0234\n",
            "     85        0.5204  0.0237\n",
            "     86        0.5199  0.0154\n",
            "     87        0.5194  0.0181\n",
            "     88        0.5190  0.0228\n",
            "     89        0.5185  0.0289\n",
            "     90        0.5181  0.0280\n",
            "     91        0.5176  0.0201\n",
            "     92        0.5172  0.0261\n",
            "     93        \u001b[36m0.5168\u001b[0m  0.0285\n",
            "     94        \u001b[36m0.5164\u001b[0m  0.0247\n",
            "     95        \u001b[36m0.5160\u001b[0m  0.0248\n",
            "     96        \u001b[36m0.5157\u001b[0m  0.0197\n",
            "     97        \u001b[36m0.5151\u001b[0m  0.0246\n",
            "     98        \u001b[36m0.5149\u001b[0m  0.0318\n",
            "     99        \u001b[36m0.5146\u001b[0m  0.0257\n",
            "    100        \u001b[36m0.5141\u001b[0m  0.0254\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.1728\u001b[0m  0.0181\n",
            "      2        \u001b[36m1.1654\u001b[0m  0.0127\n",
            "      3        \u001b[36m1.1581\u001b[0m  0.0158\n",
            "      4        \u001b[36m1.1509\u001b[0m  0.0132\n",
            "      5        \u001b[36m1.1437\u001b[0m  0.0191\n",
            "      6        \u001b[36m1.1367\u001b[0m  0.0142\n",
            "      7        \u001b[36m1.1297\u001b[0m  0.0165\n",
            "      8        \u001b[36m1.1229\u001b[0m  0.0168\n",
            "      9        \u001b[36m1.1161\u001b[0m  0.0170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.1094\u001b[0m  0.0148\n",
            "     11        \u001b[36m1.1028\u001b[0m  0.0170\n",
            "     12        \u001b[36m1.0963\u001b[0m  0.0176\n",
            "     13        \u001b[36m1.0899\u001b[0m  0.0193\n",
            "     14        \u001b[36m1.0836\u001b[0m  0.0236\n",
            "     15        \u001b[36m1.0774\u001b[0m  0.0180\n",
            "     16        \u001b[36m1.0713\u001b[0m  0.0167\n",
            "     17        \u001b[36m1.0652\u001b[0m  0.0158\n",
            "     18        \u001b[36m1.0593\u001b[0m  0.0160\n",
            "     19        \u001b[36m1.0534\u001b[0m  0.0156\n",
            "     20        \u001b[36m1.0477\u001b[0m  0.0154\n",
            "     21        \u001b[36m1.0420\u001b[0m  0.0157\n",
            "     22        \u001b[36m1.0365\u001b[0m  0.0162\n",
            "     23        \u001b[36m1.0310\u001b[0m  0.0173\n",
            "     24        \u001b[36m1.0256\u001b[0m  0.0234\n",
            "     25        \u001b[36m1.0203\u001b[0m  0.0237\n",
            "     26        \u001b[36m1.0150\u001b[0m  0.0165\n",
            "     27        \u001b[36m1.0099\u001b[0m  0.0182\n",
            "     28        \u001b[36m1.0048\u001b[0m  0.0192\n",
            "     29        \u001b[36m0.9999\u001b[0m  0.0206\n",
            "     30        \u001b[36m0.9950\u001b[0m  0.0215\n",
            "     31        \u001b[36m0.9902\u001b[0m  0.0179\n",
            "     32        \u001b[36m0.9855\u001b[0m  0.0184\n",
            "     33        \u001b[36m0.9809\u001b[0m  0.0189\n",
            "     34        \u001b[36m0.9764\u001b[0m  0.0205\n",
            "     35        \u001b[36m0.9719\u001b[0m  0.0181\n",
            "     36        \u001b[36m0.9675\u001b[0m  0.0176\n",
            "     37        \u001b[36m0.9632\u001b[0m  0.0170\n",
            "     38        \u001b[36m0.9590\u001b[0m  0.0180\n",
            "     39        \u001b[36m0.9548\u001b[0m  0.0202\n",
            "     40        \u001b[36m0.9508\u001b[0m  0.0186\n",
            "     41        \u001b[36m0.9467\u001b[0m  0.0192\n",
            "     42        \u001b[36m0.9428\u001b[0m  0.0241\n",
            "     43        \u001b[36m0.9390\u001b[0m  0.0204\n",
            "     44        \u001b[36m0.9352\u001b[0m  0.0180\n",
            "     45        \u001b[36m0.9315\u001b[0m  0.0184\n",
            "     46        \u001b[36m0.9279\u001b[0m  0.0155\n",
            "     47        \u001b[36m0.9243\u001b[0m  0.0175\n",
            "     48        \u001b[36m0.9208\u001b[0m  0.0160\n",
            "     49        \u001b[36m0.9174\u001b[0m  0.0168\n",
            "     50        \u001b[36m0.9140\u001b[0m  0.0172\n",
            "     51        \u001b[36m0.9107\u001b[0m  0.0180\n",
            "     52        \u001b[36m0.9075\u001b[0m  0.0175\n",
            "     53        \u001b[36m0.9043\u001b[0m  0.0276\n",
            "     54        \u001b[36m0.9012\u001b[0m  0.0192\n",
            "     55        \u001b[36m0.8981\u001b[0m  0.0158\n",
            "     56        \u001b[36m0.8951\u001b[0m  0.0173\n",
            "     57        \u001b[36m0.8922\u001b[0m  0.0160\n",
            "     58        \u001b[36m0.8893\u001b[0m  0.0168\n",
            "     59        \u001b[36m0.8865\u001b[0m  0.0165\n",
            "     60        \u001b[36m0.8838\u001b[0m  0.0168\n",
            "     61        \u001b[36m0.8810\u001b[0m  0.0178\n",
            "     62        \u001b[36m0.8784\u001b[0m  0.0167\n",
            "     63        \u001b[36m0.8758\u001b[0m  0.0172\n",
            "     64        \u001b[36m0.8733\u001b[0m  0.0177\n",
            "     65        \u001b[36m0.8708\u001b[0m  0.0169\n",
            "     66        \u001b[36m0.8683\u001b[0m  0.0211\n",
            "     67        \u001b[36m0.8659\u001b[0m  0.0167\n",
            "     68        \u001b[36m0.8636\u001b[0m  0.0150\n",
            "     69        \u001b[36m0.8613\u001b[0m  0.0185\n",
            "     70        \u001b[36m0.8591\u001b[0m  0.0162\n",
            "     71        \u001b[36m0.8568\u001b[0m  0.0178\n",
            "     72        \u001b[36m0.8547\u001b[0m  0.0167\n",
            "     73        \u001b[36m0.8526\u001b[0m  0.0163\n",
            "     74        \u001b[36m0.8505\u001b[0m  0.0180\n",
            "     75        \u001b[36m0.8485\u001b[0m  0.0172\n",
            "     76        \u001b[36m0.8465\u001b[0m  0.0175\n",
            "     77        \u001b[36m0.8446\u001b[0m  0.0173\n",
            "     78        \u001b[36m0.8427\u001b[0m  0.0177\n",
            "     79        \u001b[36m0.8408\u001b[0m  0.0177\n",
            "     80        \u001b[36m0.8390\u001b[0m  0.0205\n",
            "     81        \u001b[36m0.8372\u001b[0m  0.0184\n",
            "     82        \u001b[36m0.8354\u001b[0m  0.0186\n",
            "     83        \u001b[36m0.8337\u001b[0m  0.0162\n",
            "     84        \u001b[36m0.8320\u001b[0m  0.0209\n",
            "     85        \u001b[36m0.8304\u001b[0m  0.0178\n",
            "     86        \u001b[36m0.8288\u001b[0m  0.0164\n",
            "     87        \u001b[36m0.8272\u001b[0m  0.0170\n",
            "     88        \u001b[36m0.8257\u001b[0m  0.0156\n",
            "     89        \u001b[36m0.8241\u001b[0m  0.0152\n",
            "     90        \u001b[36m0.8227\u001b[0m  0.0166\n",
            "     91        \u001b[36m0.8212\u001b[0m  0.0187\n",
            "     92        \u001b[36m0.8198\u001b[0m  0.0172\n",
            "     93        \u001b[36m0.8184\u001b[0m  0.0177\n",
            "     94        \u001b[36m0.8170\u001b[0m  0.0180\n",
            "     95        \u001b[36m0.8157\u001b[0m  0.0234\n",
            "     96        \u001b[36m0.8144\u001b[0m  0.0169\n",
            "     97        \u001b[36m0.8131\u001b[0m  0.0187\n",
            "     98        \u001b[36m0.8118\u001b[0m  0.0160\n",
            "     99        \u001b[36m0.8106\u001b[0m  0.0165\n",
            "    100        \u001b[36m0.8094\u001b[0m  0.0164\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0091\u001b[0m  0.0160\n",
            "      2        \u001b[36m1.0064\u001b[0m  0.0161\n",
            "      3        \u001b[36m1.0038\u001b[0m  0.0157\n",
            "      4        \u001b[36m1.0012\u001b[0m  0.0155\n",
            "      5        \u001b[36m0.9986\u001b[0m  0.0160\n",
            "      6        \u001b[36m0.9961\u001b[0m  0.0170\n",
            "      7        \u001b[36m0.9936\u001b[0m  0.0175\n",
            "      8        \u001b[36m0.9911\u001b[0m  0.0156\n",
            "      9        \u001b[36m0.9887\u001b[0m  0.0156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m0.9863\u001b[0m  0.0174\n",
            "     11        \u001b[36m0.9840\u001b[0m  0.0168\n",
            "     12        \u001b[36m0.9817\u001b[0m  0.0219\n",
            "     13        \u001b[36m0.9794\u001b[0m  0.0172\n",
            "     14        \u001b[36m0.9772\u001b[0m  0.0162\n",
            "     15        \u001b[36m0.9750\u001b[0m  0.0179\n",
            "     16        \u001b[36m0.9728\u001b[0m  0.0205\n",
            "     17        \u001b[36m0.9707\u001b[0m  0.0248\n",
            "     18        \u001b[36m0.9686\u001b[0m  0.0276\n",
            "     19        \u001b[36m0.9665\u001b[0m  0.0203\n",
            "     20        \u001b[36m0.9645\u001b[0m  0.0185\n",
            "     21        \u001b[36m0.9624\u001b[0m  0.0172\n",
            "     22        \u001b[36m0.9605\u001b[0m  0.0181\n",
            "     23        \u001b[36m0.9585\u001b[0m  0.0166\n",
            "     24        \u001b[36m0.9566\u001b[0m  0.0176\n",
            "     25        \u001b[36m0.9547\u001b[0m  0.0177\n",
            "     26        \u001b[36m0.9528\u001b[0m  0.0179\n",
            "     27        \u001b[36m0.9510\u001b[0m  0.0178\n",
            "     28        \u001b[36m0.9492\u001b[0m  0.0182\n",
            "     29        \u001b[36m0.9474\u001b[0m  0.0156\n",
            "     30        \u001b[36m0.9456\u001b[0m  0.0159\n",
            "     31        \u001b[36m0.9438\u001b[0m  0.0184\n",
            "     32        \u001b[36m0.9421\u001b[0m  0.0152\n",
            "     33        \u001b[36m0.9404\u001b[0m  0.0152\n",
            "     34        \u001b[36m0.9387\u001b[0m  0.0154\n",
            "     35        \u001b[36m0.9370\u001b[0m  0.0154\n",
            "     36        \u001b[36m0.9354\u001b[0m  0.0155\n",
            "     37        \u001b[36m0.9337\u001b[0m  0.0147\n",
            "     38        \u001b[36m0.9321\u001b[0m  0.0145\n",
            "     39        \u001b[36m0.9305\u001b[0m  0.0169\n",
            "     40        \u001b[36m0.9289\u001b[0m  0.0163\n",
            "     41        \u001b[36m0.9274\u001b[0m  0.0154\n",
            "     42        \u001b[36m0.9258\u001b[0m  0.0158\n",
            "     43        \u001b[36m0.9243\u001b[0m  0.0160\n",
            "     44        \u001b[36m0.9228\u001b[0m  0.0157\n",
            "     45        \u001b[36m0.9213\u001b[0m  0.0147\n",
            "     46        \u001b[36m0.9198\u001b[0m  0.0155\n",
            "     47        \u001b[36m0.9183\u001b[0m  0.0149\n",
            "     48        \u001b[36m0.9168\u001b[0m  0.0216\n",
            "     49        \u001b[36m0.9154\u001b[0m  0.0154\n",
            "     50        \u001b[36m0.9139\u001b[0m  0.0152\n",
            "     51        \u001b[36m0.9125\u001b[0m  0.0152\n",
            "     52        \u001b[36m0.9111\u001b[0m  0.0155\n",
            "     53        \u001b[36m0.9096\u001b[0m  0.0146\n",
            "     54        \u001b[36m0.9082\u001b[0m  0.0159\n",
            "     55        \u001b[36m0.9068\u001b[0m  0.0145\n",
            "     56        \u001b[36m0.9055\u001b[0m  0.0157\n",
            "     57        \u001b[36m0.9041\u001b[0m  0.0158\n",
            "     58        \u001b[36m0.9027\u001b[0m  0.0172\n",
            "     59        \u001b[36m0.9014\u001b[0m  0.0158\n",
            "     60        \u001b[36m0.9000\u001b[0m  0.0164\n",
            "     61        \u001b[36m0.8987\u001b[0m  0.0170\n",
            "     62        \u001b[36m0.8973\u001b[0m  0.0159\n",
            "     63        \u001b[36m0.8960\u001b[0m  0.0168\n",
            "     64        \u001b[36m0.8947\u001b[0m  0.0172\n",
            "     65        \u001b[36m0.8934\u001b[0m  0.0173\n",
            "     66        \u001b[36m0.8920\u001b[0m  0.0150\n",
            "     67        \u001b[36m0.8907\u001b[0m  0.0208\n",
            "     68        \u001b[36m0.8894\u001b[0m  0.0178\n",
            "     69        \u001b[36m0.8881\u001b[0m  0.0232\n",
            "     70        \u001b[36m0.8869\u001b[0m  0.0244\n",
            "     71        \u001b[36m0.8856\u001b[0m  0.0180\n",
            "     72        \u001b[36m0.8843\u001b[0m  0.0211\n",
            "     73        \u001b[36m0.8830\u001b[0m  0.0211\n",
            "     74        \u001b[36m0.8818\u001b[0m  0.0193\n",
            "     75        \u001b[36m0.8805\u001b[0m  0.0234\n",
            "     76        \u001b[36m0.8792\u001b[0m  0.0186\n",
            "     77        \u001b[36m0.8780\u001b[0m  0.0205\n",
            "     78        \u001b[36m0.8767\u001b[0m  0.0169\n",
            "     79        \u001b[36m0.8755\u001b[0m  0.0186\n",
            "     80        \u001b[36m0.8742\u001b[0m  0.0177\n",
            "     81        \u001b[36m0.8730\u001b[0m  0.0180\n",
            "     82        \u001b[36m0.8717\u001b[0m  0.0155\n",
            "     83        \u001b[36m0.8705\u001b[0m  0.0157\n",
            "     84        \u001b[36m0.8693\u001b[0m  0.0152\n",
            "     85        \u001b[36m0.8681\u001b[0m  0.0168\n",
            "     86        \u001b[36m0.8668\u001b[0m  0.0156\n",
            "     87        \u001b[36m0.8656\u001b[0m  0.0153\n",
            "     88        \u001b[36m0.8644\u001b[0m  0.0162\n",
            "     89        \u001b[36m0.8632\u001b[0m  0.0232\n",
            "     90        \u001b[36m0.8619\u001b[0m  0.0156\n",
            "     91        \u001b[36m0.8607\u001b[0m  0.0161\n",
            "     92        \u001b[36m0.8595\u001b[0m  0.0154\n",
            "     93        \u001b[36m0.8583\u001b[0m  0.0156\n",
            "     94        \u001b[36m0.8571\u001b[0m  0.0156\n",
            "     95        \u001b[36m0.8559\u001b[0m  0.0153\n",
            "     96        \u001b[36m0.8547\u001b[0m  0.0162\n",
            "     97        \u001b[36m0.8535\u001b[0m  0.0153\n",
            "     98        \u001b[36m0.8523\u001b[0m  0.0155\n",
            "     99        \u001b[36m0.8511\u001b[0m  0.0150\n",
            "    100        \u001b[36m0.8499\u001b[0m  0.0155\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m2.4001\u001b[0m  0.0210\n",
            "      2        \u001b[36m2.2914\u001b[0m  0.0205\n",
            "      3        \u001b[36m2.1887\u001b[0m  0.0238\n",
            "      4        \u001b[36m2.0894\u001b[0m  0.0209\n",
            "      5        \u001b[36m1.9847\u001b[0m  0.0199\n",
            "      6        \u001b[36m1.8764\u001b[0m  0.0200\n",
            "      7        \u001b[36m1.7620\u001b[0m  0.0203\n",
            "      8        \u001b[36m1.6420\u001b[0m  0.0207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m1.5065\u001b[0m  0.0240\n",
            "     10        \u001b[36m1.3681\u001b[0m  0.0215\n",
            "     11        \u001b[36m1.2223\u001b[0m  0.0253\n",
            "     12        \u001b[36m1.0755\u001b[0m  0.0213\n",
            "     13        \u001b[36m0.9373\u001b[0m  0.0212\n",
            "     14        \u001b[36m0.8190\u001b[0m  0.0261\n",
            "     15        \u001b[36m0.7341\u001b[0m  0.0286\n",
            "     16        \u001b[36m0.6852\u001b[0m  0.0229\n",
            "     17        \u001b[36m0.6563\u001b[0m  0.0225\n",
            "     18        \u001b[36m0.6411\u001b[0m  0.0237\n",
            "     19        \u001b[36m0.6303\u001b[0m  0.0231\n",
            "     20        \u001b[36m0.6251\u001b[0m  0.0224\n",
            "     21        \u001b[36m0.6192\u001b[0m  0.0216\n",
            "     22        \u001b[36m0.6161\u001b[0m  0.0206\n",
            "     23        \u001b[36m0.6140\u001b[0m  0.0201\n",
            "     24        \u001b[36m0.6124\u001b[0m  0.0201\n",
            "     25        \u001b[36m0.6104\u001b[0m  0.0241\n",
            "     26        \u001b[36m0.6085\u001b[0m  0.0213\n",
            "     27        \u001b[36m0.6075\u001b[0m  0.0237\n",
            "     28        \u001b[36m0.6066\u001b[0m  0.0209\n",
            "     29        \u001b[36m0.6058\u001b[0m  0.0202\n",
            "     30        \u001b[36m0.6050\u001b[0m  0.0204\n",
            "     31        \u001b[36m0.6043\u001b[0m  0.0202\n",
            "     32        \u001b[36m0.6036\u001b[0m  0.0208\n",
            "     33        \u001b[36m0.6029\u001b[0m  0.0206\n",
            "     34        \u001b[36m0.6023\u001b[0m  0.0211\n",
            "     35        \u001b[36m0.6014\u001b[0m  0.0198\n",
            "     36        \u001b[36m0.5981\u001b[0m  0.0210\n",
            "     37        \u001b[36m0.5976\u001b[0m  0.0208\n",
            "     38        \u001b[36m0.5971\u001b[0m  0.0198\n",
            "     39        \u001b[36m0.5965\u001b[0m  0.0200\n",
            "     40        \u001b[36m0.5960\u001b[0m  0.0199\n",
            "     41        \u001b[36m0.5955\u001b[0m  0.0245\n",
            "     42        0.5995  0.0204\n",
            "     43        \u001b[36m0.5945\u001b[0m  0.0200\n",
            "     44        \u001b[36m0.5939\u001b[0m  0.0197\n",
            "     45        0.5971  0.0202\n",
            "     46        \u001b[36m0.5920\u001b[0m  0.0192\n",
            "     47        \u001b[36m0.5906\u001b[0m  0.0219\n",
            "     48        \u001b[36m0.5892\u001b[0m  0.0254\n",
            "     49        \u001b[36m0.5885\u001b[0m  0.0232\n",
            "     50        \u001b[36m0.5878\u001b[0m  0.0254\n",
            "     51        \u001b[36m0.5874\u001b[0m  0.0228\n",
            "     52        \u001b[36m0.5870\u001b[0m  0.0224\n",
            "     53        \u001b[36m0.5865\u001b[0m  0.0236\n",
            "     54        \u001b[36m0.5860\u001b[0m  0.0337\n",
            "     55        \u001b[36m0.5856\u001b[0m  0.0254\n",
            "     56        \u001b[36m0.5851\u001b[0m  0.0240\n",
            "     57        \u001b[36m0.5847\u001b[0m  0.0234\n",
            "     58        \u001b[36m0.5842\u001b[0m  0.0255\n",
            "     59        \u001b[36m0.5838\u001b[0m  0.0244\n",
            "     60        \u001b[36m0.5834\u001b[0m  0.0211\n",
            "     61        \u001b[36m0.5830\u001b[0m  0.0208\n",
            "     62        \u001b[36m0.5826\u001b[0m  0.0205\n",
            "     63        \u001b[36m0.5821\u001b[0m  0.0206\n",
            "     64        \u001b[36m0.5819\u001b[0m  0.0221\n",
            "     65        \u001b[36m0.5803\u001b[0m  0.0199\n",
            "     66        0.5818  0.0211\n",
            "     67        0.5906  0.0211\n",
            "     68        \u001b[36m0.5766\u001b[0m  0.0204\n",
            "     69        \u001b[36m0.5638\u001b[0m  0.0216\n",
            "     70        \u001b[36m0.5577\u001b[0m  0.0216\n",
            "     71        \u001b[36m0.5522\u001b[0m  0.0280\n",
            "     72        0.5572  0.0207\n",
            "     73        \u001b[36m0.5447\u001b[0m  0.0212\n",
            "     74        \u001b[36m0.5416\u001b[0m  0.0201\n",
            "     75        \u001b[36m0.5374\u001b[0m  0.0196\n",
            "     76        \u001b[36m0.5320\u001b[0m  0.0198\n",
            "     77        0.5321  0.0210\n",
            "     78        0.5331  0.0204\n",
            "     79        0.5362  0.0250\n",
            "     80        0.5655  0.0218\n",
            "     81        0.5541  0.0197\n",
            "     82        0.5454  0.0224\n",
            "     83        0.5445  0.0231\n",
            "     84        0.5439  0.0223\n",
            "     85        0.5434  0.0241\n",
            "     86        0.5430  0.0280\n",
            "     87        0.5425  0.0241\n",
            "     88        0.5421  0.0258\n",
            "     89        0.5418  0.0238\n",
            "     90        0.5411  0.0247\n",
            "     91        0.5407  0.0248\n",
            "     92        0.5402  0.0223\n",
            "     93        0.5397  0.0255\n",
            "     94        0.5393  0.0281\n",
            "     95        0.5390  0.0225\n",
            "     96        0.5387  0.0205\n",
            "     97        0.5384  0.0195\n",
            "     98        0.5381  0.0210\n",
            "     99        0.5378  0.0209\n",
            "    100        0.5375  0.0207\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.4671\u001b[0m  0.0225\n",
            "      2        \u001b[36m1.3032\u001b[0m  0.0224\n",
            "      3        \u001b[36m1.1779\u001b[0m  0.0222\n",
            "      4        \u001b[36m1.0651\u001b[0m  0.0310\n",
            "      5        \u001b[36m0.9687\u001b[0m  0.0218\n",
            "      6        \u001b[36m0.8891\u001b[0m  0.0202\n",
            "      7        \u001b[36m0.8223\u001b[0m  0.0239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.7788\u001b[0m  0.0239\n",
            "      9        \u001b[36m0.7427\u001b[0m  0.0230\n",
            "     10        \u001b[36m0.7182\u001b[0m  0.0221\n",
            "     11        \u001b[36m0.6982\u001b[0m  0.0213\n",
            "     12        \u001b[36m0.6818\u001b[0m  0.0232\n",
            "     13        \u001b[36m0.6678\u001b[0m  0.0230\n",
            "     14        \u001b[36m0.6555\u001b[0m  0.0231\n",
            "     15        \u001b[36m0.6450\u001b[0m  0.0242\n",
            "     16        \u001b[36m0.6418\u001b[0m  0.0237\n",
            "     17        \u001b[36m0.6353\u001b[0m  0.0219\n",
            "     18        \u001b[36m0.6243\u001b[0m  0.0226\n",
            "     19        \u001b[36m0.6216\u001b[0m  0.0237\n",
            "     20        0.6227  0.0247\n",
            "     21        \u001b[36m0.6140\u001b[0m  0.0280\n",
            "     22        \u001b[36m0.6109\u001b[0m  0.0252\n",
            "     23        \u001b[36m0.6073\u001b[0m  0.0269\n",
            "     24        \u001b[36m0.6058\u001b[0m  0.0246\n",
            "     25        \u001b[36m0.6036\u001b[0m  0.0252\n",
            "     26        \u001b[36m0.6009\u001b[0m  0.0289\n",
            "     27        \u001b[36m0.5986\u001b[0m  0.0245\n",
            "     28        \u001b[36m0.5956\u001b[0m  0.0238\n",
            "     29        \u001b[36m0.5922\u001b[0m  0.0234\n",
            "     30        \u001b[36m0.5892\u001b[0m  0.0287\n",
            "     31        \u001b[36m0.5854\u001b[0m  0.0302\n",
            "     32        \u001b[36m0.5846\u001b[0m  0.0301\n",
            "     33        \u001b[36m0.5813\u001b[0m  0.0218\n",
            "     34        \u001b[36m0.5774\u001b[0m  0.0214\n",
            "     35        \u001b[36m0.5754\u001b[0m  0.0266\n",
            "     36        \u001b[36m0.5723\u001b[0m  0.0224\n",
            "     37        \u001b[36m0.5712\u001b[0m  0.0256\n",
            "     38        0.5733  0.0213\n",
            "     39        \u001b[36m0.5703\u001b[0m  0.0206\n",
            "     40        \u001b[36m0.5679\u001b[0m  0.0209\n",
            "     41        0.5684  0.0237\n",
            "     42        0.5696  0.0213\n",
            "     43        0.5688  0.0205\n",
            "     44        \u001b[36m0.5662\u001b[0m  0.0211\n",
            "     45        \u001b[36m0.5611\u001b[0m  0.0209\n",
            "     46        \u001b[36m0.5608\u001b[0m  0.0244\n",
            "     47        0.5629  0.0229\n",
            "     48        0.5626  0.0209\n",
            "     49        0.5623  0.0215\n",
            "     50        0.5619  0.0253\n",
            "     51        0.5616  0.0211\n",
            "     52        0.5613  0.0229\n",
            "     53        0.5610  0.0255\n",
            "     54        0.5629  0.0254\n",
            "     55        0.5626  0.0223\n",
            "     56        0.5623  0.0213\n",
            "     57        0.5620  0.0221\n",
            "     58        0.5617  0.0235\n",
            "     59        0.5614  0.0272\n",
            "     60        0.5611  0.0281\n",
            "     61        0.5609  0.0276\n",
            "     62        \u001b[36m0.5607\u001b[0m  0.0234\n",
            "     63        \u001b[36m0.5604\u001b[0m  0.0266\n",
            "     64        \u001b[36m0.5602\u001b[0m  0.0265\n",
            "     65        \u001b[36m0.5600\u001b[0m  0.0249\n",
            "     66        \u001b[36m0.5598\u001b[0m  0.0229\n",
            "     67        \u001b[36m0.5597\u001b[0m  0.0214\n",
            "     68        \u001b[36m0.5595\u001b[0m  0.0228\n",
            "     69        \u001b[36m0.5593\u001b[0m  0.0305\n",
            "     70        \u001b[36m0.5592\u001b[0m  0.0248\n",
            "     71        \u001b[36m0.5590\u001b[0m  0.0256\n",
            "     72        \u001b[36m0.5589\u001b[0m  0.0227\n",
            "     73        \u001b[36m0.5587\u001b[0m  0.0233\n",
            "     74        \u001b[36m0.5586\u001b[0m  0.0173\n",
            "     75        \u001b[36m0.5585\u001b[0m  0.0206\n",
            "     76        \u001b[36m0.5584\u001b[0m  0.0179\n",
            "     77        \u001b[36m0.5582\u001b[0m  0.0221\n",
            "     78        0.5584  0.0218\n",
            "     79        \u001b[36m0.5582\u001b[0m  0.0177\n",
            "     80        \u001b[36m0.5579\u001b[0m  0.0236\n",
            "     81        \u001b[36m0.5566\u001b[0m  0.0175\n",
            "     82        \u001b[36m0.5540\u001b[0m  0.0203\n",
            "     83        0.5637  0.0179\n",
            "     84        0.5633  0.0199\n",
            "     85        0.5653  0.0186\n",
            "     86        0.5636  0.0227\n",
            "     87        0.5597  0.0230\n",
            "     88        0.5590  0.0157\n",
            "     89        0.5609  0.0162\n",
            "     90        0.5609  0.0172\n",
            "     91        0.5586  0.0245\n",
            "     92        \u001b[36m0.5528\u001b[0m  0.0294\n",
            "     93        0.5552  0.0207\n",
            "     94        0.5558  0.0222\n",
            "     95        0.5547  0.0251\n",
            "     96        0.5551  0.0225\n",
            "     97        0.5535  0.0196\n",
            "     98        0.5530  0.0260\n",
            "     99        \u001b[36m0.5506\u001b[0m  0.0229\n",
            "    100        \u001b[36m0.5492\u001b[0m  0.0267\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.1778\u001b[0m  0.0142\n",
            "      2        \u001b[36m1.1690\u001b[0m  0.0302\n",
            "      3        \u001b[36m1.1604\u001b[0m  0.0207\n",
            "      4        \u001b[36m1.1519\u001b[0m  0.0213\n",
            "      5        \u001b[36m1.1435\u001b[0m  0.0147\n",
            "      6        \u001b[36m1.1352\u001b[0m  0.0168\n",
            "      7        \u001b[36m1.1270\u001b[0m  0.0133\n",
            "      8        \u001b[36m1.1190\u001b[0m  0.0162\n",
            "      9        \u001b[36m1.1110\u001b[0m  0.0175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.1032\u001b[0m  0.0184\n",
            "     11        \u001b[36m1.0955\u001b[0m  0.0209\n",
            "     12        \u001b[36m1.0879\u001b[0m  0.0159\n",
            "     13        \u001b[36m1.0804\u001b[0m  0.0148\n",
            "     14        \u001b[36m1.0730\u001b[0m  0.0155\n",
            "     15        \u001b[36m1.0655\u001b[0m  0.0188\n",
            "     16        \u001b[36m1.0522\u001b[0m  0.0162\n",
            "     17        \u001b[36m1.0515\u001b[0m  0.0206\n",
            "     18        \u001b[36m1.0406\u001b[0m  0.0160\n",
            "     19        \u001b[36m1.0222\u001b[0m  0.0158\n",
            "     20        \u001b[36m0.9982\u001b[0m  0.0154\n",
            "     21        \u001b[36m0.9849\u001b[0m  0.0156\n",
            "     22        \u001b[36m0.9785\u001b[0m  0.0158\n",
            "     23        \u001b[36m0.9722\u001b[0m  0.0163\n",
            "     24        \u001b[36m0.9660\u001b[0m  0.0153\n",
            "     25        \u001b[36m0.9599\u001b[0m  0.0167\n",
            "     26        \u001b[36m0.9539\u001b[0m  0.0165\n",
            "     27        \u001b[36m0.9480\u001b[0m  0.0161\n",
            "     28        \u001b[36m0.9423\u001b[0m  0.0150\n",
            "     29        \u001b[36m0.9366\u001b[0m  0.0157\n",
            "     30        \u001b[36m0.9311\u001b[0m  0.0201\n",
            "     31        \u001b[36m0.9257\u001b[0m  0.0155\n",
            "     32        \u001b[36m0.9203\u001b[0m  0.0181\n",
            "     33        \u001b[36m0.9151\u001b[0m  0.0181\n",
            "     34        \u001b[36m0.9100\u001b[0m  0.0181\n",
            "     35        \u001b[36m0.9050\u001b[0m  0.0209\n",
            "     36        \u001b[36m0.9000\u001b[0m  0.0166\n",
            "     37        \u001b[36m0.8952\u001b[0m  0.0190\n",
            "     38        \u001b[36m0.8904\u001b[0m  0.0202\n",
            "     39        \u001b[36m0.8858\u001b[0m  0.0241\n",
            "     40        \u001b[36m0.8812\u001b[0m  0.0238\n",
            "     41        \u001b[36m0.8768\u001b[0m  0.0196\n",
            "     42        \u001b[36m0.8724\u001b[0m  0.0213\n",
            "     43        \u001b[36m0.8681\u001b[0m  0.0210\n",
            "     44        \u001b[36m0.8639\u001b[0m  0.0239\n",
            "     45        \u001b[36m0.8598\u001b[0m  0.0215\n",
            "     46        \u001b[36m0.8557\u001b[0m  0.0237\n",
            "     47        \u001b[36m0.8518\u001b[0m  0.0197\n",
            "     48        \u001b[36m0.8479\u001b[0m  0.0195\n",
            "     49        \u001b[36m0.8441\u001b[0m  0.0220\n",
            "     50        \u001b[36m0.8404\u001b[0m  0.0259\n",
            "     51        \u001b[36m0.8367\u001b[0m  0.0199\n",
            "     52        \u001b[36m0.8332\u001b[0m  0.0202\n",
            "     53        \u001b[36m0.8297\u001b[0m  0.0164\n",
            "     54        \u001b[36m0.8263\u001b[0m  0.0228\n",
            "     55        \u001b[36m0.8229\u001b[0m  0.0199\n",
            "     56        \u001b[36m0.8196\u001b[0m  0.0183\n",
            "     57        \u001b[36m0.8165\u001b[0m  0.0158\n",
            "     58        \u001b[36m0.8133\u001b[0m  0.0202\n",
            "     59        \u001b[36m0.8103\u001b[0m  0.0164\n",
            "     60        \u001b[36m0.8073\u001b[0m  0.0154\n",
            "     61        \u001b[36m0.8043\u001b[0m  0.0172\n",
            "     62        \u001b[36m0.8015\u001b[0m  0.0174\n",
            "     63        \u001b[36m0.7986\u001b[0m  0.0166\n",
            "     64        \u001b[36m0.7959\u001b[0m  0.0161\n",
            "     65        \u001b[36m0.7932\u001b[0m  0.0148\n",
            "     66        \u001b[36m0.7906\u001b[0m  0.0114\n",
            "     67        \u001b[36m0.7880\u001b[0m  0.0170\n",
            "     68        \u001b[36m0.7855\u001b[0m  0.0188\n",
            "     69        \u001b[36m0.7830\u001b[0m  0.0160\n",
            "     70        \u001b[36m0.7806\u001b[0m  0.0150\n",
            "     71        \u001b[36m0.7782\u001b[0m  0.0170\n",
            "     72        \u001b[36m0.7759\u001b[0m  0.0190\n",
            "     73        \u001b[36m0.7736\u001b[0m  0.0168\n",
            "     74        \u001b[36m0.7714\u001b[0m  0.0223\n",
            "     75        \u001b[36m0.7692\u001b[0m  0.0175\n",
            "     76        \u001b[36m0.7670\u001b[0m  0.0169\n",
            "     77        \u001b[36m0.7649\u001b[0m  0.0197\n",
            "     78        \u001b[36m0.7629\u001b[0m  0.0193\n",
            "     79        \u001b[36m0.7609\u001b[0m  0.0238\n",
            "     80        \u001b[36m0.7589\u001b[0m  0.0239\n",
            "     81        \u001b[36m0.7569\u001b[0m  0.0206\n",
            "     82        \u001b[36m0.7550\u001b[0m  0.0232\n",
            "     83        \u001b[36m0.7531\u001b[0m  0.0205\n",
            "     84        \u001b[36m0.7513\u001b[0m  0.0218\n",
            "     85        \u001b[36m0.7495\u001b[0m  0.0223\n",
            "     86        \u001b[36m0.7477\u001b[0m  0.0163\n",
            "     87        \u001b[36m0.7459\u001b[0m  0.0286\n",
            "     88        \u001b[36m0.7442\u001b[0m  0.0270\n",
            "     89        \u001b[36m0.7425\u001b[0m  0.0233\n",
            "     90        \u001b[36m0.7408\u001b[0m  0.0186\n",
            "     91        \u001b[36m0.7392\u001b[0m  0.0170\n",
            "     92        \u001b[36m0.7376\u001b[0m  0.0156\n",
            "     93        \u001b[36m0.7360\u001b[0m  0.0219\n",
            "     94        \u001b[36m0.7344\u001b[0m  0.0190\n",
            "     95        \u001b[36m0.7329\u001b[0m  0.0158\n",
            "     96        \u001b[36m0.7314\u001b[0m  0.0178\n",
            "     97        \u001b[36m0.7299\u001b[0m  0.0175\n",
            "     98        \u001b[36m0.7284\u001b[0m  0.0219\n",
            "     99        \u001b[36m0.7269\u001b[0m  0.0156\n",
            "    100        \u001b[36m0.7255\u001b[0m  0.0171\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.5030\u001b[0m  0.0171\n",
            "      2        \u001b[36m1.4917\u001b[0m  0.0157\n",
            "      3        \u001b[36m1.4813\u001b[0m  0.0167\n",
            "      4        \u001b[36m1.4717\u001b[0m  0.0182\n",
            "      5        \u001b[36m1.4628\u001b[0m  0.0157\n",
            "      6        \u001b[36m1.4546\u001b[0m  0.0153\n",
            "      7        \u001b[36m1.4470\u001b[0m  0.0125\n",
            "      8        \u001b[36m1.4400\u001b[0m  0.0183\n",
            "      9        \u001b[36m1.4334\u001b[0m  0.0198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m1.4273\u001b[0m  0.0249\n",
            "     11        \u001b[36m1.4216\u001b[0m  0.0172\n",
            "     12        \u001b[36m1.4162\u001b[0m  0.0189\n",
            "     13        \u001b[36m1.4111\u001b[0m  0.0179\n",
            "     14        \u001b[36m1.4064\u001b[0m  0.0167\n",
            "     15        \u001b[36m1.4019\u001b[0m  0.0212\n",
            "     16        \u001b[36m1.3976\u001b[0m  0.0191\n",
            "     17        \u001b[36m1.3935\u001b[0m  0.0169\n",
            "     18        \u001b[36m1.3896\u001b[0m  0.0183\n",
            "     19        \u001b[36m1.3859\u001b[0m  0.0214\n",
            "     20        \u001b[36m1.3823\u001b[0m  0.0208\n",
            "     21        \u001b[36m1.3789\u001b[0m  0.0258\n",
            "     22        \u001b[36m1.3755\u001b[0m  0.0196\n",
            "     23        \u001b[36m1.3723\u001b[0m  0.0248\n",
            "     24        \u001b[36m1.3692\u001b[0m  0.0170\n",
            "     25        \u001b[36m1.3662\u001b[0m  0.0191\n",
            "     26        \u001b[36m1.3632\u001b[0m  0.0197\n",
            "     27        \u001b[36m1.3603\u001b[0m  0.0165\n",
            "     28        \u001b[36m1.3575\u001b[0m  0.0186\n",
            "     29        \u001b[36m1.3548\u001b[0m  0.0174\n",
            "     30        \u001b[36m1.3521\u001b[0m  0.0177\n",
            "     31        \u001b[36m1.3494\u001b[0m  0.0179\n",
            "     32        \u001b[36m1.3468\u001b[0m  0.0167\n",
            "     33        \u001b[36m1.3443\u001b[0m  0.0190\n",
            "     34        \u001b[36m1.3418\u001b[0m  0.0174\n",
            "     35        \u001b[36m1.3393\u001b[0m  0.0179\n",
            "     36        \u001b[36m1.3368\u001b[0m  0.0170\n",
            "     37        \u001b[36m1.3344\u001b[0m  0.0159\n",
            "     38        \u001b[36m1.3321\u001b[0m  0.0158\n",
            "     39        \u001b[36m1.3297\u001b[0m  0.0152\n",
            "     40        \u001b[36m1.3274\u001b[0m  0.0166\n",
            "     41        \u001b[36m1.3251\u001b[0m  0.0270\n",
            "     42        \u001b[36m1.3228\u001b[0m  0.0164\n",
            "     43        \u001b[36m1.3205\u001b[0m  0.0165\n",
            "     44        \u001b[36m1.3183\u001b[0m  0.0171\n",
            "     45        \u001b[36m1.3160\u001b[0m  0.0177\n",
            "     46        \u001b[36m1.3138\u001b[0m  0.0232\n",
            "     47        \u001b[36m1.3116\u001b[0m  0.0164\n",
            "     48        \u001b[36m1.3095\u001b[0m  0.0172\n",
            "     49        \u001b[36m1.3073\u001b[0m  0.0164\n",
            "     50        \u001b[36m1.3052\u001b[0m  0.0156\n",
            "     51        \u001b[36m1.3030\u001b[0m  0.0178\n",
            "     52        \u001b[36m1.3009\u001b[0m  0.0177\n",
            "     53        \u001b[36m1.2988\u001b[0m  0.0217\n",
            "     54        \u001b[36m1.2967\u001b[0m  0.0198\n",
            "     55        \u001b[36m1.2946\u001b[0m  0.0178\n",
            "     56        \u001b[36m1.2925\u001b[0m  0.0227\n",
            "     57        \u001b[36m1.2905\u001b[0m  0.0201\n",
            "     58        \u001b[36m1.2884\u001b[0m  0.0223\n",
            "     59        \u001b[36m1.2864\u001b[0m  0.0289\n",
            "     60        \u001b[36m1.2843\u001b[0m  0.0197\n",
            "     61        \u001b[36m1.2823\u001b[0m  0.0199\n",
            "     62        \u001b[36m1.2803\u001b[0m  0.0297\n",
            "     63        \u001b[36m1.2783\u001b[0m  0.0216\n",
            "     64        \u001b[36m1.2763\u001b[0m  0.0228\n",
            "     65        \u001b[36m1.2743\u001b[0m  0.0193\n",
            "     66        \u001b[36m1.2723\u001b[0m  0.0225\n",
            "     67        \u001b[36m1.2703\u001b[0m  0.0214\n",
            "     68        \u001b[36m1.2684\u001b[0m  0.0192\n",
            "     69        \u001b[36m1.2664\u001b[0m  0.0201\n",
            "     70        \u001b[36m1.2645\u001b[0m  0.0226\n",
            "     71        \u001b[36m1.2625\u001b[0m  0.0173\n",
            "     72        \u001b[36m1.2606\u001b[0m  0.0192\n",
            "     73        \u001b[36m1.2587\u001b[0m  0.0171\n",
            "     74        \u001b[36m1.2567\u001b[0m  0.0204\n",
            "     75        \u001b[36m1.2548\u001b[0m  0.0211\n",
            "     76        \u001b[36m1.2529\u001b[0m  0.0184\n",
            "     77        \u001b[36m1.2510\u001b[0m  0.0229\n",
            "     78        \u001b[36m1.2491\u001b[0m  0.0162\n",
            "     79        \u001b[36m1.2472\u001b[0m  0.0163\n",
            "     80        \u001b[36m1.2454\u001b[0m  0.0202\n",
            "     81        \u001b[36m1.2435\u001b[0m  0.0166\n",
            "     82        \u001b[36m1.2416\u001b[0m  0.0213\n",
            "     83        \u001b[36m1.2398\u001b[0m  0.0187\n",
            "     84        \u001b[36m1.2379\u001b[0m  0.0199\n",
            "     85        \u001b[36m1.2361\u001b[0m  0.0177\n",
            "     86        \u001b[36m1.2342\u001b[0m  0.0152\n",
            "     87        \u001b[36m1.2324\u001b[0m  0.0189\n",
            "     88        \u001b[36m1.2306\u001b[0m  0.0192\n",
            "     89        \u001b[36m1.2287\u001b[0m  0.0182\n",
            "     90        \u001b[36m1.2269\u001b[0m  0.0230\n",
            "     91        \u001b[36m1.2251\u001b[0m  0.0182\n",
            "     92        \u001b[36m1.2233\u001b[0m  0.0184\n",
            "     93        \u001b[36m1.2215\u001b[0m  0.0190\n",
            "     94        \u001b[36m1.2197\u001b[0m  0.0223\n",
            "     95        \u001b[36m1.2179\u001b[0m  0.0195\n",
            "     96        \u001b[36m1.2161\u001b[0m  0.0220\n",
            "     97        \u001b[36m1.2144\u001b[0m  0.0212\n",
            "     98        \u001b[36m1.2126\u001b[0m  0.0204\n",
            "     99        \u001b[36m1.2108\u001b[0m  0.0201\n",
            "    100        \u001b[36m1.2091\u001b[0m  0.0205\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0271\n",
            "      2        1.0000  0.0246\n",
            "      3        1.0000  0.0268\n",
            "      4        1.0000  0.0251\n",
            "      5        1.0000  0.0252\n",
            "      6        1.0000  0.0202\n",
            "      7        1.0000  0.0272\n",
            "      8        1.0000  0.0286\n",
            "      9        1.0000  0.0257\n",
            "     10        1.0000  0.0290\n",
            "     11        1.0000  0.0206\n",
            "     12        1.0000  0.0183\n",
            "     13        1.0000  0.0232\n",
            "     14        1.0000  0.0217\n",
            "     15        1.0000  0.0229\n",
            "     16        1.0000  0.0184\n",
            "     17        1.0000  0.0309\n",
            "     18        1.0000  0.0231\n",
            "     19        1.0000  0.0227\n",
            "     20        1.0000  0.0337\n",
            "     21        1.0000  0.0259\n",
            "     22        1.0000  0.0218\n",
            "     23        1.0000  0.0274\n",
            "     24        1.0000  0.0278\n",
            "     25        1.0000  0.0273\n",
            "     26        1.0000  0.0164\n",
            "     27        1.0000  0.0231\n",
            "     28        1.0000  0.0198\n",
            "     29        1.0000  0.0231\n",
            "     30        1.0000  0.0273\n",
            "     31        1.0000  0.0249\n",
            "     32        1.0000  0.0224\n",
            "     33        1.0000  0.0271\n",
            "     34        1.0000  0.0258\n",
            "     35        1.0000  0.0258\n",
            "     36        1.0000  0.0184\n",
            "     37        1.0000  0.0257\n",
            "     38        1.0000  0.0268\n",
            "     39        1.0000  0.0262\n",
            "     40        1.0000  0.0276\n",
            "     41        1.0000  0.0223\n",
            "     42        1.0000  0.0210\n",
            "     43        1.0000  0.0271\n",
            "     44        1.0000  0.0208\n",
            "     45        1.0000  0.0282\n",
            "     46        1.0000  0.0199\n",
            "     47        1.0000  0.0237\n",
            "     48        1.0000  0.0206\n",
            "     49        1.0000  0.0222\n",
            "     50        1.0000  0.0226\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0203\n",
            "      2        1.0000  0.0272\n",
            "      3        1.0000  0.0201\n",
            "      4        1.0000  0.0274\n",
            "      5        1.0000  0.0228\n",
            "      6        1.0000  0.0272\n",
            "      7        1.0000  0.0281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        1.0000  0.0236\n",
            "      9        1.0000  0.0222\n",
            "     10        1.0000  0.0245\n",
            "     11        1.0000  0.0206\n",
            "     12        1.0000  0.0228\n",
            "     13        1.0000  0.0299\n",
            "     14        1.0000  0.0193\n",
            "     15        1.0000  0.0253\n",
            "     16        1.0000  0.0259\n",
            "     17        1.0000  0.0251\n",
            "     18        1.0000  0.0192\n",
            "     19        1.0000  0.0280\n",
            "     20        1.0000  0.0256\n",
            "     21        1.0000  0.0256\n",
            "     22        1.0000  0.0241\n",
            "     23        1.0000  0.0194\n",
            "     24        1.0000  0.0168\n",
            "     25        1.0000  0.0274\n",
            "     26        1.0000  0.0306\n",
            "     27        1.0000  0.0304\n",
            "     28        1.0000  0.0209\n",
            "     29        1.0000  0.0204\n",
            "     30        1.0000  0.0211\n",
            "     31        1.0000  0.0217\n",
            "     32        1.0000  0.0261\n",
            "     33        1.0000  0.0200\n",
            "     34        1.0000  0.0245\n",
            "     35        1.0000  0.0245\n",
            "     36        1.0000  0.0272\n",
            "     37        1.0000  0.0270\n",
            "     38        1.0000  0.0196\n",
            "     39        1.0000  0.0173\n",
            "     40        1.0000  0.0238\n",
            "     41        1.0000  0.0250\n",
            "     42        1.0000  0.0290\n",
            "     43        1.0000  0.0293\n",
            "     44        1.0000  0.0255\n",
            "     45        1.0000  0.0272\n",
            "     46        1.0000  0.0203\n",
            "     47        1.0000  0.0275\n",
            "     48        1.0000  0.0249\n",
            "     49        1.0000  0.0195\n",
            "     50        1.0000  0.0268\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0164\n",
            "      2        1.0000  0.0178\n",
            "      3        1.0000  0.0183\n",
            "      4        1.0000  0.0193\n",
            "      5        1.0000  0.0198\n",
            "      6        1.0000  0.0204\n",
            "      7        1.0000  0.0196\n",
            "      8        1.0000  0.0165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        1.0000  0.0174\n",
            "     10        1.0000  0.0181\n",
            "     11        1.0000  0.0193\n",
            "     12        1.0000  0.0199\n",
            "     13        1.0000  0.0171\n",
            "     14        1.0000  0.0137\n",
            "     15        1.0000  0.0188\n",
            "     16        1.0000  0.0168\n",
            "     17        1.0000  0.0214\n",
            "     18        1.0000  0.0224\n",
            "     19        1.0000  0.0190\n",
            "     20        1.0000  0.0189\n",
            "     21        1.0000  0.0156\n",
            "     22        1.0000  0.0198\n",
            "     23        1.0000  0.0147\n",
            "     24        1.0000  0.0171\n",
            "     25        1.0000  0.0170\n",
            "     26        1.0000  0.0169\n",
            "     27        1.0000  0.0175\n",
            "     28        1.0000  0.0172\n",
            "     29        1.0000  0.0212\n",
            "     30        1.0000  0.0167\n",
            "     31        1.0000  0.0165\n",
            "     32        1.0000  0.0165\n",
            "     33        1.0000  0.0159\n",
            "     34        1.0000  0.0164\n",
            "     35        1.0000  0.0200\n",
            "     36        1.0000  0.0279\n",
            "     37        1.0000  0.0252\n",
            "     38        1.0000  0.0184\n",
            "     39        1.0000  0.0176\n",
            "     40        1.0000  0.0188\n",
            "     41        1.0000  0.0193\n",
            "     42        1.0000  0.0179\n",
            "     43        1.0000  0.0176\n",
            "     44        1.0000  0.0172\n",
            "     45        1.0000  0.0177\n",
            "     46        1.0000  0.0185\n",
            "     47        1.0000  0.0209\n",
            "     48        1.0000  0.0208\n",
            "     49        1.0000  0.0211\n",
            "     50        1.0000  0.0191\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0188\n",
            "      2        1.0000  0.0183\n",
            "      3        1.0000  0.0197\n",
            "      4        1.0000  0.0199\n",
            "      5        1.0000  0.0208\n",
            "      6        1.0000  0.0251\n",
            "      7        1.0000  0.0185\n",
            "      8        1.0000  0.0185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        1.0000  0.0182\n",
            "     10        1.0000  0.0164\n",
            "     11        1.0000  0.0169\n",
            "     12        1.0000  0.0170\n",
            "     13        1.0000  0.0165\n",
            "     14        1.0000  0.0229\n",
            "     15        1.0000  0.0165\n",
            "     16        1.0000  0.0172\n",
            "     17        1.0000  0.0229\n",
            "     18        1.0000  0.0186\n",
            "     19        1.0000  0.0228\n",
            "     20        1.0000  0.0176\n",
            "     21        1.0000  0.0198\n",
            "     22        1.0000  0.0172\n",
            "     23        1.0000  0.0168\n",
            "     24        1.0000  0.0166\n",
            "     25        1.0000  0.0167\n",
            "     26        1.0000  0.0170\n",
            "     27        1.0000  0.0178\n",
            "     28        1.0000  0.0167\n",
            "     29        1.0000  0.0218\n",
            "     30        1.0000  0.0176\n",
            "     31        1.0000  0.0178\n",
            "     32        1.0000  0.0202\n",
            "     33        1.0000  0.0190\n",
            "     34        1.0000  0.0209\n",
            "     35        1.0000  0.0192\n",
            "     36        1.0000  0.0181\n",
            "     37        1.0000  0.0203\n",
            "     38        1.0000  0.0194\n",
            "     39        1.0000  0.0188\n",
            "     40        1.0000  0.0206\n",
            "     41        1.0000  0.0214\n",
            "     42        1.0000  0.0227\n",
            "     43        1.0000  0.0221\n",
            "     44        1.0000  0.0211\n",
            "     45        1.0000  0.0183\n",
            "     46        1.0000  0.0183\n",
            "     47        1.0000  0.0182\n",
            "     48        1.0000  0.0189\n",
            "     49        1.0000  0.0211\n",
            "     50        1.0000  0.0186\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0259\n",
            "      2        1.0000  0.0227\n",
            "      3        1.0000  0.0241\n",
            "      4        1.0000  0.0231\n",
            "      5        1.0000  0.0228\n",
            "      6        1.0000  0.0283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        1.0000  0.0224\n",
            "      8        1.0000  0.0237\n",
            "      9        1.0000  0.0239\n",
            "     10        1.0000  0.0227\n",
            "     11        1.0000  0.0237\n",
            "     12        1.0000  0.0269\n",
            "     13        1.0000  0.0220\n",
            "     14        1.0000  0.0227\n",
            "     15        1.0000  0.0206\n",
            "     16        1.0000  0.0217\n",
            "     17        1.0000  0.0266\n",
            "     18        1.0000  0.0220\n",
            "     19        1.0000  0.0225\n",
            "     20        1.0000  0.0227\n",
            "     21        1.0000  0.0210\n",
            "     22        1.0000  0.0265\n",
            "     23        1.0000  0.0249\n",
            "     24        1.0000  0.0240\n",
            "     25        1.0000  0.0223\n",
            "     26        1.0000  0.0241\n",
            "     27        1.0000  0.0246\n",
            "     28        1.0000  0.0247\n",
            "     29        1.0000  0.0269\n",
            "     30        1.0000  0.0251\n",
            "     31        1.0000  0.0302\n",
            "     32        1.0000  0.0253\n",
            "     33        1.0000  0.0237\n",
            "     34        1.0000  0.0228\n",
            "     35        1.0000  0.0228\n",
            "     36        1.0000  0.0209\n",
            "     37        1.0000  0.0206\n",
            "     38        1.0000  0.0275\n",
            "     39        1.0000  0.0205\n",
            "     40        1.0000  0.0204\n",
            "     41        1.0000  0.0202\n",
            "     42        1.0000  0.0210\n",
            "     43        1.0000  0.0207\n",
            "     44        1.0000  0.0203\n",
            "     45        1.0000  0.0203\n",
            "     46        1.0000  0.0202\n",
            "     47        1.0000  0.0201\n",
            "     48        1.0000  0.0208\n",
            "     49        1.0000  0.0204\n",
            "     50        1.0000  0.0216\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0200\n",
            "      2        1.0000  0.0211\n",
            "      3        1.0000  0.0228\n",
            "      4        1.0000  0.0248\n",
            "      5        1.0000  0.0219\n",
            "      6        1.0000  0.0234\n",
            "      7        1.0000  0.0208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        1.0000  0.0242\n",
            "      9        1.0000  0.0218\n",
            "     10        1.0000  0.0228\n",
            "     11        1.0000  0.0261\n",
            "     12        1.0000  0.0307\n",
            "     13        1.0000  0.0331\n",
            "     14        1.0000  0.0274\n",
            "     15        1.0000  0.0249\n",
            "     16        1.0000  0.0291\n",
            "     17        1.0000  0.0264\n",
            "     18        1.0000  0.0233\n",
            "     19        1.0000  0.0217\n",
            "     20        1.0000  0.0242\n",
            "     21        1.0000  0.0250\n",
            "     22        1.0000  0.0218\n",
            "     23        1.0000  0.0207\n",
            "     24        1.0000  0.0205\n",
            "     25        1.0000  0.0204\n",
            "     26        1.0000  0.0212\n",
            "     27        1.0000  0.0226\n",
            "     28        1.0000  0.0207\n",
            "     29        1.0000  0.0207\n",
            "     30        1.0000  0.0207\n",
            "     31        1.0000  0.0212\n",
            "     32        1.0000  0.0223\n",
            "     33        1.0000  0.0256\n",
            "     34        1.0000  0.0251\n",
            "     35        1.0000  0.0251\n",
            "     36        1.0000  0.0240\n",
            "     37        1.0000  0.0231\n",
            "     38        1.0000  0.0214\n",
            "     39        1.0000  0.0215\n",
            "     40        1.0000  0.0242\n",
            "     41        1.0000  0.0208\n",
            "     42        1.0000  0.0216\n",
            "     43        1.0000  0.0225\n",
            "     44        1.0000  0.0211\n",
            "     45        1.0000  0.0226\n",
            "     46        1.0000  0.0222\n",
            "     47        1.0000  0.0220\n",
            "     48        1.0000  0.0214\n",
            "     49        1.0000  0.0209\n",
            "     50        1.0000  0.0215\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0195\n",
            "      2        1.0000  0.0231\n",
            "      3        1.0000  0.0212\n",
            "      4        1.0000  0.0200\n",
            "      5        1.0000  0.0223\n",
            "      6        1.0000  0.0196\n",
            "      7        1.0000  0.0216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        1.0000  0.0189\n",
            "      9        1.0000  0.0201\n",
            "     10        1.0000  0.0211\n",
            "     11        1.0000  0.0170\n",
            "     12        1.0000  0.0173\n",
            "     13        1.0000  0.0174\n",
            "     14        1.0000  0.0171\n",
            "     15        1.0000  0.0188\n",
            "     16        1.0000  0.0174\n",
            "     17        1.0000  0.0170\n",
            "     18        1.0000  0.0244\n",
            "     19        1.0000  0.0175\n",
            "     20        1.0000  0.0171\n",
            "     21        1.0000  0.0171\n",
            "     22        1.0000  0.0179\n",
            "     23        1.0000  0.0189\n",
            "     24        1.0000  0.0166\n",
            "     25        1.0000  0.0173\n",
            "     26        1.0000  0.0194\n",
            "     27        1.0000  0.0183\n",
            "     28        1.0000  0.0177\n",
            "     29        1.0000  0.0178\n",
            "     30        1.0000  0.0177\n",
            "     31        1.0000  0.0170\n",
            "     32        1.0000  0.0170\n",
            "     33        1.0000  0.0191\n",
            "     34        1.0000  0.0176\n",
            "     35        1.0000  0.0186\n",
            "     36        1.0000  0.0200\n",
            "     37        1.0000  0.0191\n",
            "     38        1.0000  0.0203\n",
            "     39        1.0000  0.0173\n",
            "     40        1.0000  0.0186\n",
            "     41        1.0000  0.0178\n",
            "     42        1.0000  0.0179\n",
            "     43        1.0000  0.0188\n",
            "     44        1.0000  0.0201\n",
            "     45        1.0000  0.0198\n",
            "     46        1.0000  0.0187\n",
            "     47        1.0000  0.0220\n",
            "     48        1.0000  0.0192\n",
            "     49        1.0000  0.0204\n",
            "     50        1.0000  0.0208\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0215\n",
            "      2        1.0000  0.0201\n",
            "      3        1.0000  0.0221\n",
            "      4        1.0000  0.0193\n",
            "      5        1.0000  0.0188\n",
            "      6        1.0000  0.0270\n",
            "      7        1.0000  0.0177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        1.0000  0.0169\n",
            "      9        1.0000  0.0170\n",
            "     10        1.0000  0.0176\n",
            "     11        1.0000  0.0198\n",
            "     12        1.0000  0.0201\n",
            "     13        1.0000  0.0178\n",
            "     14        1.0000  0.0174\n",
            "     15        1.0000  0.0187\n",
            "     16        1.0000  0.0172\n",
            "     17        1.0000  0.0174\n",
            "     18        1.0000  0.0167\n",
            "     19        1.0000  0.0173\n",
            "     20        1.0000  0.0168\n",
            "     21        1.0000  0.0180\n",
            "     22        1.0000  0.0183\n",
            "     23        1.0000  0.0186\n",
            "     24        1.0000  0.0222\n",
            "     25        1.0000  0.0232\n",
            "     26        1.0000  0.0186\n",
            "     27        1.0000  0.0255\n",
            "     28        1.0000  0.0170\n",
            "     29        1.0000  0.0183\n",
            "     30        1.0000  0.0197\n",
            "     31        1.0000  0.0170\n",
            "     32        1.0000  0.0189\n",
            "     33        1.0000  0.0188\n",
            "     34        1.0000  0.0213\n",
            "     35        1.0000  0.0199\n",
            "     36        1.0000  0.0205\n",
            "     37        1.0000  0.0198\n",
            "     38        1.0000  0.0215\n",
            "     39        1.0000  0.0200\n",
            "     40        1.0000  0.0201\n",
            "     41        1.0000  0.0217\n",
            "     42        1.0000  0.0184\n",
            "     43        1.0000  0.0178\n",
            "     44        1.0000  0.0186\n",
            "     45        1.0000  0.0252\n",
            "     46        1.0000  0.0160\n",
            "     47        1.0000  0.0164\n",
            "     48        1.0000  0.0176\n",
            "     49        1.0000  0.0168\n",
            "     50        1.0000  0.0188\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8470\u001b[0m  0.0212\n",
            "      2        \u001b[36m0.8406\u001b[0m  0.0213\n",
            "      3        \u001b[36m0.8344\u001b[0m  0.0238\n",
            "      4        \u001b[36m0.8279\u001b[0m  0.0237\n",
            "      5        \u001b[36m0.8256\u001b[0m  0.0261\n",
            "      6        \u001b[36m0.8231\u001b[0m  0.0235\n",
            "      7        \u001b[36m0.8206\u001b[0m  0.0264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        0.8219  0.0224\n",
            "      9        \u001b[36m0.8186\u001b[0m  0.0224\n",
            "     10        \u001b[36m0.8102\u001b[0m  0.0228\n",
            "     11        \u001b[36m0.8020\u001b[0m  0.0210\n",
            "     12        \u001b[36m0.7925\u001b[0m  0.0213\n",
            "     13        \u001b[36m0.7792\u001b[0m  0.0278\n",
            "     14        \u001b[36m0.6511\u001b[0m  0.0229\n",
            "     15        \u001b[36m0.6444\u001b[0m  0.0212\n",
            "     16        \u001b[36m0.6393\u001b[0m  0.0243\n",
            "     17        \u001b[36m0.6357\u001b[0m  0.0253\n",
            "     18        \u001b[36m0.6329\u001b[0m  0.0246\n",
            "     19        \u001b[36m0.6318\u001b[0m  0.0265\n",
            "     20        \u001b[36m0.6298\u001b[0m  0.0248\n",
            "     21        \u001b[36m0.6242\u001b[0m  0.0257\n",
            "     22        \u001b[36m0.6199\u001b[0m  0.0269\n",
            "     23        0.6219  0.0243\n",
            "     24        \u001b[36m0.6183\u001b[0m  0.0258\n",
            "     25        \u001b[36m0.6129\u001b[0m  0.0274\n",
            "     26        0.6135  0.0257\n",
            "     27        \u001b[36m0.6111\u001b[0m  0.0297\n",
            "     28        \u001b[36m0.6101\u001b[0m  0.0259\n",
            "     29        \u001b[36m0.6095\u001b[0m  0.0246\n",
            "     30        \u001b[36m0.6087\u001b[0m  0.0300\n",
            "     31        \u001b[36m0.6061\u001b[0m  0.0303\n",
            "     32        \u001b[36m0.6042\u001b[0m  0.0229\n",
            "     33        \u001b[36m0.6029\u001b[0m  0.0212\n",
            "     34        \u001b[36m0.5998\u001b[0m  0.0234\n",
            "     35        \u001b[36m0.5942\u001b[0m  0.0227\n",
            "     36        \u001b[36m0.5925\u001b[0m  0.0239\n",
            "     37        \u001b[36m0.5882\u001b[0m  0.0274\n",
            "     38        \u001b[36m0.5840\u001b[0m  0.0233\n",
            "     39        0.5847  0.0284\n",
            "     40        \u001b[36m0.5702\u001b[0m  0.0255\n",
            "     41        \u001b[36m0.5594\u001b[0m  0.0266\n",
            "     42        \u001b[36m0.5546\u001b[0m  0.0290\n",
            "     43        \u001b[36m0.5537\u001b[0m  0.0275\n",
            "     44        \u001b[36m0.5510\u001b[0m  0.0247\n",
            "     45        \u001b[36m0.5440\u001b[0m  0.0280\n",
            "     46        \u001b[36m0.5371\u001b[0m  0.0301\n",
            "     47        \u001b[36m0.5337\u001b[0m  0.0284\n",
            "     48        \u001b[36m0.5286\u001b[0m  0.0241\n",
            "     49        \u001b[36m0.5247\u001b[0m  0.0258\n",
            "     50        \u001b[36m0.5237\u001b[0m  0.0249\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9333\u001b[0m  0.0272\n",
            "      2        0.9333  0.0223\n",
            "      3        0.9368  0.0264\n",
            "      4        0.9368  0.0244\n",
            "      5        0.9404  0.0249\n",
            "      6        0.9439  0.0278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        0.9439  0.0295\n",
            "      8        0.9439  0.0368\n",
            "      9        0.9474  0.0316\n",
            "     10        0.9439  0.0261\n",
            "     11        0.9368  0.0332\n",
            "     12        0.9368  0.0287\n",
            "     13        \u001b[36m0.9298\u001b[0m  0.0341\n",
            "     14        \u001b[36m0.9263\u001b[0m  0.0356\n",
            "     15        0.9263  0.0274\n",
            "     16        0.9263  0.0283\n",
            "     17        0.9263  0.0278\n",
            "     18        0.9263  0.0255\n",
            "     19        0.9263  0.0250\n",
            "     20        0.9263  0.0264\n",
            "     21        0.9263  0.0229\n",
            "     22        0.9263  0.0234\n",
            "     23        0.9263  0.0242\n",
            "     24        0.9263  0.0233\n",
            "     25        0.9263  0.0244\n",
            "     26        0.9263  0.0219\n",
            "     27        0.9263  0.0251\n",
            "     28        0.9263  0.0225\n",
            "     29        0.9263  0.0225\n",
            "     30        0.9263  0.0235\n",
            "     31        0.9263  0.0217\n",
            "     32        0.9263  0.0241\n",
            "     33        0.9263  0.0278\n",
            "     34        0.9263  0.0267\n",
            "     35        0.9263  0.0260\n",
            "     36        0.9263  0.0246\n",
            "     37        0.9263  0.0284\n",
            "     38        \u001b[36m0.9228\u001b[0m  0.0269\n",
            "     39        0.9228  0.0233\n",
            "     40        0.9228  0.0277\n",
            "     41        0.9228  0.0258\n",
            "     42        0.9228  0.0175\n",
            "     43        0.9228  0.0210\n",
            "     44        0.9228  0.0280\n",
            "     45        0.9228  0.0325\n",
            "     46        \u001b[36m0.9193\u001b[0m  0.0262\n",
            "     47        0.9193  0.0181\n",
            "     48        0.9193  0.0191\n",
            "     49        0.9193  0.0341\n",
            "     50        0.9193  0.0256\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4049\u001b[0m  0.0183\n",
            "      2        0.4049  0.0216\n",
            "      3        0.4049  0.0201\n",
            "      4        0.4049  0.0265\n",
            "      5        0.4049  0.0159\n",
            "      6        0.4049  0.0239\n",
            "      7        0.4049  0.0157\n",
            "      8        0.4049  0.0250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        0.4049  0.0194\n",
            "     10        0.4049  0.0210\n",
            "     11        0.4049  0.0200\n",
            "     12        0.4049  0.0178\n",
            "     13        0.4049  0.0205\n",
            "     14        0.4049  0.0193\n",
            "     15        0.4049  0.0217\n",
            "     16        0.4049  0.0222\n",
            "     17        0.4049  0.0211\n",
            "     18        0.4049  0.0230\n",
            "     19        0.4049  0.0200\n",
            "     20        0.4049  0.0214\n",
            "     21        0.4049  0.0189\n",
            "     22        0.4049  0.0200\n",
            "     23        0.4049  0.0209\n",
            "     24        0.4049  0.0204\n",
            "     25        0.4049  0.0230\n",
            "     26        0.4049  0.0219\n",
            "     27        0.4049  0.0199\n",
            "     28        0.4049  0.0196\n",
            "     29        0.4049  0.0245\n",
            "     30        0.4049  0.0193\n",
            "     31        0.4049  0.0213\n",
            "     32        0.4049  0.0243\n",
            "     33        0.4049  0.0191\n",
            "     34        0.4049  0.0176\n",
            "     35        0.4049  0.0159\n",
            "     36        0.4049  0.0162\n",
            "     37        0.4049  0.0168\n",
            "     38        0.4049  0.0189\n",
            "     39        0.4049  0.0202\n",
            "     40        0.4049  0.0197\n",
            "     41        0.4049  0.0197\n",
            "     42        0.4049  0.0226\n",
            "     43        0.4049  0.0282\n",
            "     44        0.4049  0.0220\n",
            "     45        0.4049  0.0224\n",
            "     46        0.4049  0.0205\n",
            "     47        0.4049  0.0177\n",
            "     48        0.4049  0.0180\n",
            "     49        0.4049  0.0180\n",
            "     50        0.4049  0.0166\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8386\u001b[0m  0.0166\n",
            "      2        0.8386  0.0221\n",
            "      3        0.8386  0.0192\n",
            "      4        0.8386  0.0169\n",
            "      5        0.8386  0.0173\n",
            "      6        0.8386  0.0176\n",
            "      7        0.8386  0.0204\n",
            "      8        0.8386  0.0193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        0.8386  0.0217\n",
            "     10        0.8386  0.0194\n",
            "     11        0.8386  0.0243\n",
            "     12        0.8386  0.0248\n",
            "     13        0.8386  0.0234\n",
            "     14        0.8386  0.0217\n",
            "     15        0.8386  0.0239\n",
            "     16        0.8386  0.0198\n",
            "     17        0.8386  0.0221\n",
            "     18        0.8386  0.0207\n",
            "     19        0.8386  0.0220\n",
            "     20        0.8386  0.0245\n",
            "     21        0.8386  0.0207\n",
            "     22        0.8386  0.0174\n",
            "     23        0.8386  0.0214\n",
            "     24        0.8386  0.0184\n",
            "     25        0.8386  0.0200\n",
            "     26        0.8386  0.0311\n",
            "     27        0.8386  0.0181\n",
            "     28        0.8386  0.0208\n",
            "     29        0.8386  0.0180\n",
            "     30        0.8386  0.0211\n",
            "     31        0.8386  0.0196\n",
            "     32        0.8386  0.0191\n",
            "     33        0.8386  0.0201\n",
            "     34        0.8386  0.0207\n",
            "     35        0.8386  0.0164\n",
            "     36        0.8386  0.0173\n",
            "     37        0.8386  0.0181\n",
            "     38        0.8386  0.0169\n",
            "     39        0.8386  0.0196\n",
            "     40        0.8386  0.0214\n",
            "     41        0.8386  0.0193\n",
            "     42        0.8386  0.0202\n",
            "     43        0.8386  0.0208\n",
            "     44        0.8386  0.0199\n",
            "     45        0.8386  0.0200\n",
            "     46        0.8386  0.0203\n",
            "     47        0.8386  0.0223\n",
            "     48        0.8386  0.0264\n",
            "     49        0.8386  0.0201\n",
            "     50        0.8386  0.0214\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9577\u001b[0m  0.0270\n",
            "      2        0.9577  0.0277\n",
            "      3        0.9577  0.0253\n",
            "      4        0.9577  0.0266\n",
            "      5        0.9577  0.0208\n",
            "      6        0.9577  0.0253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        0.9577  0.0299\n",
            "      8        0.9577  0.0227\n",
            "      9        0.9577  0.0238\n",
            "     10        0.9577  0.0285\n",
            "     11        0.9577  0.0205\n",
            "     12        0.9577  0.0189\n",
            "     13        0.9577  0.0227\n",
            "     14        0.9577  0.0244\n",
            "     15        0.9577  0.0256\n",
            "     16        0.9577  0.0248\n",
            "     17        0.9577  0.0192\n",
            "     18        0.9577  0.0327\n",
            "     19        0.9577  0.0241\n",
            "     20        0.9577  0.0224\n",
            "     21        0.9577  0.0221\n",
            "     22        0.9577  0.0246\n",
            "     23        0.9577  0.0205\n",
            "     24        0.9613  0.0204\n",
            "     25        0.9613  0.0210\n",
            "     26        0.9613  0.0272\n",
            "     27        0.9613  0.0241\n",
            "     28        0.9613  0.0220\n",
            "     29        0.9613  0.0214\n",
            "     30        0.9613  0.0284\n",
            "     31        0.9613  0.0231\n",
            "     32        0.9613  0.0226\n",
            "     33        0.9613  0.0200\n",
            "     34        0.9613  0.0218\n",
            "     35        0.9613  0.0253\n",
            "     36        0.9613  0.0245\n",
            "     37        0.9613  0.0257\n",
            "     38        0.9613  0.0267\n",
            "     39        0.9613  0.0189\n",
            "     40        \u001b[36m0.9542\u001b[0m  0.0245\n",
            "     41        \u001b[36m0.9296\u001b[0m  0.0212\n",
            "     42        \u001b[36m0.9225\u001b[0m  0.0305\n",
            "     43        0.9225  0.0259\n",
            "     44        0.9225  0.0265\n",
            "     45        0.9225  0.0245\n",
            "     46        0.9225  0.0244\n",
            "     47        0.9225  0.0236\n",
            "     48        0.9225  0.0245\n",
            "     49        0.9225  0.0250\n",
            "     50        0.9225  0.0182\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4386\u001b[0m  0.0225\n",
            "      2        0.4386  0.0334\n",
            "      3        0.4386  0.0233\n",
            "      4        \u001b[36m0.4386\u001b[0m  0.0243\n",
            "      5        \u001b[36m0.4175\u001b[0m  0.0224\n",
            "      6        \u001b[36m0.4140\u001b[0m  0.0250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        0.4140  0.0263\n",
            "      8        0.4140  0.0247\n",
            "      9        0.4140  0.0227\n",
            "     10        0.4140  0.0289\n",
            "     11        0.4140  0.0244\n",
            "     12        0.4140  0.0213\n",
            "     13        0.4140  0.0175\n",
            "     14        0.4140  0.0225\n",
            "     15        0.4140  0.0259\n",
            "     16        \u001b[36m0.4140\u001b[0m  0.0261\n",
            "     17        \u001b[36m0.4140\u001b[0m  0.0267\n",
            "     18        \u001b[36m0.4105\u001b[0m  0.0307\n",
            "     19        0.4105  0.0258\n",
            "     20        0.4105  0.0345\n",
            "     21        0.4105  0.0284\n",
            "     22        0.4105  0.0280\n",
            "     23        0.4105  0.0173\n",
            "     24        0.4105  0.0264\n",
            "     25        0.4105  0.0246\n",
            "     26        0.4105  0.0334\n",
            "     27        0.4105  0.0226\n",
            "     28        0.4105  0.0234\n",
            "     29        0.4105  0.0200\n",
            "     30        0.4105  0.0236\n",
            "     31        0.4105  0.0226\n",
            "     32        0.4105  0.0253\n",
            "     33        0.4105  0.0254\n",
            "     34        0.4105  0.0243\n",
            "     35        0.4105  0.0232\n",
            "     36        0.4105  0.0223\n",
            "     37        0.4105  0.0250\n",
            "     38        0.4105  0.0199\n",
            "     39        0.4105  0.0261\n",
            "     40        0.4105  0.0233\n",
            "     41        0.4105  0.0171\n",
            "     42        0.4105  0.0217\n",
            "     43        0.4105  0.0215\n",
            "     44        0.4105  0.0224\n",
            "     45        0.4105  0.0225\n",
            "     46        0.4105  0.0234\n",
            "     47        0.4105  0.0226\n",
            "     48        0.4105  0.0222\n",
            "     49        0.4105  0.0217\n",
            "     50        0.4105  0.0240\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9401\u001b[0m  0.0175\n",
            "      2        0.9401  0.0209\n",
            "      3        0.9401  0.0213\n",
            "      4        0.9401  0.0212\n",
            "      5        0.9401  0.0206\n",
            "      6        0.9401  0.0221\n",
            "      7        0.9401  0.0187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        0.9401  0.0251\n",
            "      9        0.9401  0.0250\n",
            "     10        0.9401  0.0208\n",
            "     11        0.9401  0.0201\n",
            "     12        0.9401  0.0223\n",
            "     13        0.9401  0.0203\n",
            "     14        0.9401  0.0181\n",
            "     15        0.9401  0.0189\n",
            "     16        0.9401  0.0186\n",
            "     17        0.9401  0.0162\n",
            "     18        0.9401  0.0211\n",
            "     19        0.9401  0.0202\n",
            "     20        0.9401  0.0189\n",
            "     21        0.9401  0.0187\n",
            "     22        0.9401  0.0208\n",
            "     23        0.9401  0.0187\n",
            "     24        0.9401  0.0204\n",
            "     25        0.9401  0.0185\n",
            "     26        0.9401  0.0243\n",
            "     27        0.9401  0.0217\n",
            "     28        0.9401  0.0207\n",
            "     29        0.9401  0.0201\n",
            "     30        0.9401  0.0239\n",
            "     31        0.9401  0.0197\n",
            "     32        0.9401  0.0185\n",
            "     33        0.9401  0.0215\n",
            "     34        0.9401  0.0202\n",
            "     35        0.9401  0.0267\n",
            "     36        0.9401  0.0185\n",
            "     37        0.9401  0.0176\n",
            "     38        0.9401  0.0165\n",
            "     39        0.9401  0.0221\n",
            "     40        0.9401  0.0211\n",
            "     41        0.9401  0.0207\n",
            "     42        0.9401  0.0197\n",
            "     43        0.9401  0.0207\n",
            "     44        0.9401  0.0180\n",
            "     45        0.9401  0.0176\n",
            "     46        0.9401  0.0216\n",
            "     47        0.9401  0.0227\n",
            "     48        0.9401  0.0226\n",
            "     49        0.9401  0.0209\n",
            "     50        0.9401  0.0226\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9368\u001b[0m  0.0216\n",
            "      2        0.9368  0.0219\n",
            "      3        0.9368  0.0215\n",
            "      4        0.9368  0.0224\n",
            "      5        0.9368  0.0177\n",
            "      6        0.9368  0.0176\n",
            "      7        0.9368  0.0175\n",
            "      8        0.9368  0.0178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        0.9368  0.0189\n",
            "     10        0.9368  0.0208\n",
            "     11        0.9368  0.0176\n",
            "     12        0.9368  0.0180\n",
            "     13        0.9368  0.0176\n",
            "     14        0.9368  0.0174\n",
            "     15        0.9368  0.0175\n",
            "     16        0.9368  0.0242\n",
            "     17        0.9368  0.0177\n",
            "     18        0.9368  0.0182\n",
            "     19        0.9368  0.0181\n",
            "     20        0.9368  0.0184\n",
            "     21        0.9368  0.0176\n",
            "     22        0.9368  0.0192\n",
            "     23        0.9368  0.0207\n",
            "     24        0.9368  0.0212\n",
            "     25        0.9368  0.0298\n",
            "     26        0.9368  0.0218\n",
            "     27        0.9368  0.0202\n",
            "     28        0.9368  0.0212\n",
            "     29        0.9368  0.0254\n",
            "     30        0.9368  0.0198\n",
            "     31        0.9368  0.0181\n",
            "     32        0.9368  0.0215\n",
            "     33        0.9368  0.0197\n",
            "     34        0.9368  0.0184\n",
            "     35        0.9368  0.0200\n",
            "     36        0.9368  0.0201\n",
            "     37        0.9368  0.0224\n",
            "     38        0.9368  0.0238\n",
            "     39        0.9368  0.0212\n",
            "     40        0.9368  0.0197\n",
            "     41        0.9368  0.0203\n",
            "     42        0.9368  0.0193\n",
            "     43        0.9368  0.0176\n",
            "     44        0.9368  0.0197\n",
            "     45        0.9368  0.0190\n",
            "     46        0.9368  0.0181\n",
            "     47        0.9368  0.0174\n",
            "     48        0.9368  0.0172\n",
            "     49        0.9368  0.0182\n",
            "     50        0.9368  0.0184\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9924\u001b[0m  0.0310\n",
            "      2        \u001b[36m0.9918\u001b[0m  0.0222\n",
            "      3        \u001b[36m0.9911\u001b[0m  0.0220\n",
            "      4        \u001b[36m0.9904\u001b[0m  0.0215\n",
            "      5        \u001b[36m0.9896\u001b[0m  0.0225\n",
            "      6        \u001b[36m0.9887\u001b[0m  0.0174\n",
            "      7        \u001b[36m0.9876\u001b[0m  0.0212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.9865\u001b[0m  0.0229\n",
            "      9        \u001b[36m0.9852\u001b[0m  0.0243\n",
            "     10        \u001b[36m0.9837\u001b[0m  0.0228\n",
            "     11        \u001b[36m0.9820\u001b[0m  0.0230\n",
            "     12        \u001b[36m0.9801\u001b[0m  0.0222\n",
            "     13        \u001b[36m0.9779\u001b[0m  0.0276\n",
            "     14        \u001b[36m0.9753\u001b[0m  0.0258\n",
            "     15        \u001b[36m0.9723\u001b[0m  0.0243\n",
            "     16        \u001b[36m0.9687\u001b[0m  0.0325\n",
            "     17        \u001b[36m0.9644\u001b[0m  0.0306\n",
            "     18        \u001b[36m0.9593\u001b[0m  0.0225\n",
            "     19        \u001b[36m0.9530\u001b[0m  0.0226\n",
            "     20        \u001b[36m0.9453\u001b[0m  0.0282\n",
            "     21        \u001b[36m0.9358\u001b[0m  0.0267\n",
            "     22        \u001b[36m0.9238\u001b[0m  0.0277\n",
            "     23        \u001b[36m0.9087\u001b[0m  0.0252\n",
            "     24        \u001b[36m0.8896\u001b[0m  0.0248\n",
            "     25        \u001b[36m0.8652\u001b[0m  0.0248\n",
            "     26        \u001b[36m0.8340\u001b[0m  0.0229\n",
            "     27        \u001b[36m0.7953\u001b[0m  0.0267\n",
            "     28        \u001b[36m0.7503\u001b[0m  0.0269\n",
            "     29        \u001b[36m0.7023\u001b[0m  0.0291\n",
            "     30        \u001b[36m0.6553\u001b[0m  0.0188\n",
            "     31        \u001b[36m0.6119\u001b[0m  0.0233\n",
            "     32        \u001b[36m0.5731\u001b[0m  0.0186\n",
            "     33        \u001b[36m0.5392\u001b[0m  0.0242\n",
            "     34        \u001b[36m0.5103\u001b[0m  0.0239\n",
            "     35        \u001b[36m0.4867\u001b[0m  0.0270\n",
            "     36        \u001b[36m0.4683\u001b[0m  0.0210\n",
            "     37        \u001b[36m0.4545\u001b[0m  0.0224\n",
            "     38        \u001b[36m0.4443\u001b[0m  0.0199\n",
            "     39        \u001b[36m0.4367\u001b[0m  0.0226\n",
            "     40        \u001b[36m0.4309\u001b[0m  0.0250\n",
            "     41        \u001b[36m0.4262\u001b[0m  0.0251\n",
            "     42        \u001b[36m0.4224\u001b[0m  0.0187\n",
            "     43        \u001b[36m0.4193\u001b[0m  0.0215\n",
            "     44        \u001b[36m0.4166\u001b[0m  0.0211\n",
            "     45        \u001b[36m0.4142\u001b[0m  0.0221\n",
            "     46        \u001b[36m0.4121\u001b[0m  0.0222\n",
            "     47        \u001b[36m0.4102\u001b[0m  0.0210\n",
            "     48        \u001b[36m0.4085\u001b[0m  0.0282\n",
            "     49        \u001b[36m0.4070\u001b[0m  0.0251\n",
            "     50        \u001b[36m0.4056\u001b[0m  0.0283\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9922\u001b[0m  0.0240\n",
            "      2        \u001b[36m0.9915\u001b[0m  0.0214\n",
            "      3        \u001b[36m0.9907\u001b[0m  0.0179\n",
            "      4        \u001b[36m0.9899\u001b[0m  0.0219\n",
            "      5        \u001b[36m0.9889\u001b[0m  0.0259\n",
            "      6        \u001b[36m0.9878\u001b[0m  0.0246\n",
            "      7        \u001b[36m0.9864\u001b[0m  0.0282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.9848\u001b[0m  0.0340\n",
            "      9        \u001b[36m0.9828\u001b[0m  0.0285\n",
            "     10        \u001b[36m0.9802\u001b[0m  0.0269\n",
            "     11        \u001b[36m0.9768\u001b[0m  0.0216\n",
            "     12        \u001b[36m0.9722\u001b[0m  0.0197\n",
            "     13        \u001b[36m0.9656\u001b[0m  0.0281\n",
            "     14        \u001b[36m0.9559\u001b[0m  0.0276\n",
            "     15        \u001b[36m0.9408\u001b[0m  0.0257\n",
            "     16        \u001b[36m0.9171\u001b[0m  0.0239\n",
            "     17        \u001b[36m0.8799\u001b[0m  0.0215\n",
            "     18        \u001b[36m0.8247\u001b[0m  0.0231\n",
            "     19        \u001b[36m0.7515\u001b[0m  0.0190\n",
            "     20        \u001b[36m0.6697\u001b[0m  0.0237\n",
            "     21        \u001b[36m0.5940\u001b[0m  0.0237\n",
            "     22        \u001b[36m0.5352\u001b[0m  0.0237\n",
            "     23        \u001b[36m0.4943\u001b[0m  0.0261\n",
            "     24        \u001b[36m0.4674\u001b[0m  0.0261\n",
            "     25        \u001b[36m0.4494\u001b[0m  0.0268\n",
            "     26        \u001b[36m0.4370\u001b[0m  0.0222\n",
            "     27        \u001b[36m0.4282\u001b[0m  0.0227\n",
            "     28        \u001b[36m0.4215\u001b[0m  0.0238\n",
            "     29        \u001b[36m0.4163\u001b[0m  0.0233\n",
            "     30        \u001b[36m0.4121\u001b[0m  0.0242\n",
            "     31        \u001b[36m0.4086\u001b[0m  0.0229\n",
            "     32        \u001b[36m0.4058\u001b[0m  0.0221\n",
            "     33        \u001b[36m0.4033\u001b[0m  0.0267\n",
            "     34        \u001b[36m0.4012\u001b[0m  0.0276\n",
            "     35        \u001b[36m0.3994\u001b[0m  0.0324\n",
            "     36        \u001b[36m0.3978\u001b[0m  0.0316\n",
            "     37        \u001b[36m0.3964\u001b[0m  0.0275\n",
            "     38        \u001b[36m0.3951\u001b[0m  0.0201\n",
            "     39        \u001b[36m0.3940\u001b[0m  0.0175\n",
            "     40        \u001b[36m0.3930\u001b[0m  0.0302\n",
            "     41        \u001b[36m0.3920\u001b[0m  0.0282\n",
            "     42        \u001b[36m0.3912\u001b[0m  0.0273\n",
            "     43        \u001b[36m0.3904\u001b[0m  0.0248\n",
            "     44        \u001b[36m0.3896\u001b[0m  0.0284\n",
            "     45        \u001b[36m0.3890\u001b[0m  0.0199\n",
            "     46        \u001b[36m0.3883\u001b[0m  0.0302\n",
            "     47        \u001b[36m0.3878\u001b[0m  0.0387\n",
            "     48        \u001b[36m0.3872\u001b[0m  0.0258\n",
            "     49        \u001b[36m0.3867\u001b[0m  0.0255\n",
            "     50        \u001b[36m0.3862\u001b[0m  0.0183\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8925\u001b[0m  0.0139\n",
            "      2        \u001b[36m0.8919\u001b[0m  0.0220\n",
            "      3        \u001b[36m0.8912\u001b[0m  0.0176\n",
            "      4        \u001b[36m0.8905\u001b[0m  0.0262\n",
            "      5        \u001b[36m0.8899\u001b[0m  0.0189\n",
            "      6        \u001b[36m0.8892\u001b[0m  0.0185\n",
            "      7        \u001b[36m0.8885\u001b[0m  0.0195\n",
            "      8        \u001b[36m0.8878\u001b[0m  0.0191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.8872\u001b[0m  0.0210\n",
            "     10        \u001b[36m0.8865\u001b[0m  0.0199\n",
            "     11        \u001b[36m0.8857\u001b[0m  0.0181\n",
            "     12        \u001b[36m0.8850\u001b[0m  0.0184\n",
            "     13        \u001b[36m0.8843\u001b[0m  0.0192\n",
            "     14        \u001b[36m0.8836\u001b[0m  0.0192\n",
            "     15        \u001b[36m0.8829\u001b[0m  0.0212\n",
            "     16        \u001b[36m0.8821\u001b[0m  0.0200\n",
            "     17        \u001b[36m0.8814\u001b[0m  0.0206\n",
            "     18        \u001b[36m0.8806\u001b[0m  0.0189\n",
            "     19        \u001b[36m0.8799\u001b[0m  0.0181\n",
            "     20        \u001b[36m0.8791\u001b[0m  0.0193\n",
            "     21        \u001b[36m0.8783\u001b[0m  0.0189\n",
            "     22        \u001b[36m0.8775\u001b[0m  0.0211\n",
            "     23        \u001b[36m0.8767\u001b[0m  0.0284\n",
            "     24        \u001b[36m0.8759\u001b[0m  0.0223\n",
            "     25        \u001b[36m0.8751\u001b[0m  0.0225\n",
            "     26        \u001b[36m0.8743\u001b[0m  0.0215\n",
            "     27        \u001b[36m0.8735\u001b[0m  0.0240\n",
            "     28        \u001b[36m0.8727\u001b[0m  0.0221\n",
            "     29        \u001b[36m0.8718\u001b[0m  0.0214\n",
            "     30        \u001b[36m0.8710\u001b[0m  0.0219\n",
            "     31        \u001b[36m0.8701\u001b[0m  0.0225\n",
            "     32        \u001b[36m0.8693\u001b[0m  0.0216\n",
            "     33        \u001b[36m0.8684\u001b[0m  0.0210\n",
            "     34        \u001b[36m0.8675\u001b[0m  0.0213\n",
            "     35        \u001b[36m0.8666\u001b[0m  0.0277\n",
            "     36        \u001b[36m0.8657\u001b[0m  0.0216\n",
            "     37        \u001b[36m0.8648\u001b[0m  0.0232\n",
            "     38        \u001b[36m0.8639\u001b[0m  0.0220\n",
            "     39        \u001b[36m0.8630\u001b[0m  0.0219\n",
            "     40        \u001b[36m0.8620\u001b[0m  0.0230\n",
            "     41        \u001b[36m0.8611\u001b[0m  0.0188\n",
            "     42        \u001b[36m0.8601\u001b[0m  0.0216\n",
            "     43        \u001b[36m0.8592\u001b[0m  0.0172\n",
            "     44        \u001b[36m0.8582\u001b[0m  0.0161\n",
            "     45        \u001b[36m0.8572\u001b[0m  0.0197\n",
            "     46        \u001b[36m0.8562\u001b[0m  0.0227\n",
            "     47        \u001b[36m0.8552\u001b[0m  0.0243\n",
            "     48        \u001b[36m0.8542\u001b[0m  0.0214\n",
            "     49        \u001b[36m0.8532\u001b[0m  0.0156\n",
            "     50        \u001b[36m0.8522\u001b[0m  0.0186\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9952\u001b[0m  0.0182\n",
            "      2        \u001b[36m0.9952\u001b[0m  0.0181\n",
            "      3        \u001b[36m0.9952\u001b[0m  0.0194\n",
            "      4        \u001b[36m0.9952\u001b[0m  0.0243\n",
            "      5        \u001b[36m0.9952\u001b[0m  0.0163\n",
            "      6        \u001b[36m0.9952\u001b[0m  0.0201\n",
            "      7        \u001b[36m0.9952\u001b[0m  0.0167\n",
            "      8        \u001b[36m0.9952\u001b[0m  0.0182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.9952\u001b[0m  0.0214\n",
            "     10        \u001b[36m0.9952\u001b[0m  0.0180\n",
            "     11        \u001b[36m0.9951\u001b[0m  0.0177\n",
            "     12        \u001b[36m0.9951\u001b[0m  0.0234\n",
            "     13        \u001b[36m0.9951\u001b[0m  0.0228\n",
            "     14        \u001b[36m0.9951\u001b[0m  0.0226\n",
            "     15        \u001b[36m0.9951\u001b[0m  0.0228\n",
            "     16        \u001b[36m0.9951\u001b[0m  0.0230\n",
            "     17        \u001b[36m0.9951\u001b[0m  0.0231\n",
            "     18        \u001b[36m0.9951\u001b[0m  0.0255\n",
            "     19        \u001b[36m0.9951\u001b[0m  0.0213\n",
            "     20        \u001b[36m0.9951\u001b[0m  0.0237\n",
            "     21        \u001b[36m0.9951\u001b[0m  0.0237\n",
            "     22        \u001b[36m0.9951\u001b[0m  0.0250\n",
            "     23        \u001b[36m0.9951\u001b[0m  0.0231\n",
            "     24        \u001b[36m0.9951\u001b[0m  0.0246\n",
            "     25        \u001b[36m0.9951\u001b[0m  0.0302\n",
            "     26        \u001b[36m0.9951\u001b[0m  0.0183\n",
            "     27        \u001b[36m0.9951\u001b[0m  0.0179\n",
            "     28        \u001b[36m0.9951\u001b[0m  0.0179\n",
            "     29        \u001b[36m0.9951\u001b[0m  0.0187\n",
            "     30        \u001b[36m0.9951\u001b[0m  0.0180\n",
            "     31        \u001b[36m0.9951\u001b[0m  0.0220\n",
            "     32        \u001b[36m0.9951\u001b[0m  0.0192\n",
            "     33        \u001b[36m0.9951\u001b[0m  0.0211\n",
            "     34        \u001b[36m0.9951\u001b[0m  0.0200\n",
            "     35        \u001b[36m0.9951\u001b[0m  0.0200\n",
            "     36        \u001b[36m0.9951\u001b[0m  0.0218\n",
            "     37        \u001b[36m0.9951\u001b[0m  0.0190\n",
            "     38        \u001b[36m0.9951\u001b[0m  0.0174\n",
            "     39        \u001b[36m0.9951\u001b[0m  0.0173\n",
            "     40        \u001b[36m0.9951\u001b[0m  0.0175\n",
            "     41        \u001b[36m0.9951\u001b[0m  0.0165\n",
            "     42        \u001b[36m0.9951\u001b[0m  0.0170\n",
            "     43        \u001b[36m0.9951\u001b[0m  0.0180\n",
            "     44        \u001b[36m0.9951\u001b[0m  0.0201\n",
            "     45        \u001b[36m0.9951\u001b[0m  0.0162\n",
            "     46        \u001b[36m0.9951\u001b[0m  0.0176\n",
            "     47        \u001b[36m0.9951\u001b[0m  0.0178\n",
            "     48        \u001b[36m0.9951\u001b[0m  0.0180\n",
            "     49        \u001b[36m0.9951\u001b[0m  0.0183\n",
            "     50        \u001b[36m0.9951\u001b[0m  0.0202\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9973\u001b[0m  0.0275\n",
            "      2        \u001b[36m0.9969\u001b[0m  0.0249\n",
            "      3        \u001b[36m0.9964\u001b[0m  0.0293\n",
            "      4        \u001b[36m0.9958\u001b[0m  0.0297\n",
            "      5        \u001b[36m0.9951\u001b[0m  0.0277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        \u001b[36m0.9942\u001b[0m  0.0274\n",
            "      7        \u001b[36m0.9930\u001b[0m  0.0260\n",
            "      8        \u001b[36m0.9916\u001b[0m  0.0270\n",
            "      9        \u001b[36m0.9897\u001b[0m  0.0278\n",
            "     10        \u001b[36m0.9872\u001b[0m  0.0284\n",
            "     11        \u001b[36m0.9840\u001b[0m  0.0267\n",
            "     12        \u001b[36m0.9797\u001b[0m  0.0232\n",
            "     13        \u001b[36m0.9740\u001b[0m  0.0263\n",
            "     14        \u001b[36m0.9664\u001b[0m  0.0223\n",
            "     15        \u001b[36m0.9561\u001b[0m  0.0244\n",
            "     16        \u001b[36m0.9424\u001b[0m  0.0265\n",
            "     17        \u001b[36m0.9244\u001b[0m  0.0209\n",
            "     18        \u001b[36m0.9009\u001b[0m  0.0209\n",
            "     19        \u001b[36m0.8710\u001b[0m  0.0268\n",
            "     20        \u001b[36m0.8346\u001b[0m  0.0289\n",
            "     21        \u001b[36m0.7921\u001b[0m  0.0225\n",
            "     22        \u001b[36m0.7453\u001b[0m  0.0217\n",
            "     23        \u001b[36m0.6970\u001b[0m  0.0240\n",
            "     24        \u001b[36m0.6502\u001b[0m  0.0241\n",
            "     25        \u001b[36m0.6079\u001b[0m  0.0223\n",
            "     26        \u001b[36m0.5716\u001b[0m  0.0243\n",
            "     27        \u001b[36m0.5417\u001b[0m  0.0255\n",
            "     28        \u001b[36m0.5178\u001b[0m  0.0183\n",
            "     29        \u001b[36m0.4984\u001b[0m  0.0187\n",
            "     30        \u001b[36m0.4819\u001b[0m  0.0350\n",
            "     31        \u001b[36m0.4668\u001b[0m  0.0284\n",
            "     32        \u001b[36m0.4516\u001b[0m  0.0307\n",
            "     33        \u001b[36m0.4352\u001b[0m  0.0242\n",
            "     34        \u001b[36m0.4202\u001b[0m  0.0241\n",
            "     35        \u001b[36m0.4101\u001b[0m  0.0191\n",
            "     36        \u001b[36m0.4045\u001b[0m  0.0273\n",
            "     37        \u001b[36m0.4014\u001b[0m  0.0311\n",
            "     38        \u001b[36m0.3993\u001b[0m  0.0309\n",
            "     39        \u001b[36m0.3978\u001b[0m  0.0401\n",
            "     40        \u001b[36m0.3966\u001b[0m  0.0309\n",
            "     41        \u001b[36m0.3955\u001b[0m  0.0308\n",
            "     42        \u001b[36m0.3946\u001b[0m  0.0295\n",
            "     43        \u001b[36m0.3937\u001b[0m  0.0253\n",
            "     44        \u001b[36m0.3929\u001b[0m  0.0180\n",
            "     45        \u001b[36m0.3922\u001b[0m  0.0338\n",
            "     46        \u001b[36m0.3915\u001b[0m  0.0274\n",
            "     47        \u001b[36m0.3908\u001b[0m  0.0226\n",
            "     48        \u001b[36m0.3902\u001b[0m  0.0223\n",
            "     49        \u001b[36m0.3896\u001b[0m  0.0215\n",
            "     50        \u001b[36m0.3891\u001b[0m  0.0221\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9999\u001b[0m  0.0257\n",
            "      2        \u001b[36m0.9998\u001b[0m  0.0234\n",
            "      3        \u001b[36m0.9998\u001b[0m  0.0274\n",
            "      4        \u001b[36m0.9998\u001b[0m  0.0231\n",
            "      5        \u001b[36m0.9997\u001b[0m  0.0182\n",
            "      6        \u001b[36m0.9997\u001b[0m  0.0274\n",
            "      7        \u001b[36m0.9996\u001b[0m  0.0240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.9996\u001b[0m  0.0377\n",
            "      9        \u001b[36m0.9995\u001b[0m  0.0293\n",
            "     10        \u001b[36m0.9993\u001b[0m  0.0267\n",
            "     11        \u001b[36m0.9992\u001b[0m  0.0213\n",
            "     12        \u001b[36m0.9990\u001b[0m  0.0201\n",
            "     13        \u001b[36m0.9986\u001b[0m  0.0283\n",
            "     14        \u001b[36m0.9982\u001b[0m  0.0322\n",
            "     15        \u001b[36m0.9977\u001b[0m  0.0297\n",
            "     16        \u001b[36m0.9968\u001b[0m  0.0306\n",
            "     17        \u001b[36m0.9957\u001b[0m  0.0285\n",
            "     18        \u001b[36m0.9941\u001b[0m  0.0256\n",
            "     19        \u001b[36m0.9917\u001b[0m  0.0281\n",
            "     20        \u001b[36m0.9882\u001b[0m  0.0340\n",
            "     21        \u001b[36m0.9832\u001b[0m  0.0350\n",
            "     22        \u001b[36m0.9758\u001b[0m  0.0343\n",
            "     23        \u001b[36m0.9651\u001b[0m  0.0305\n",
            "     24        \u001b[36m0.9496\u001b[0m  0.0252\n",
            "     25        \u001b[36m0.9277\u001b[0m  0.0212\n",
            "     26        \u001b[36m0.8973\u001b[0m  0.0272\n",
            "     27        \u001b[36m0.8571\u001b[0m  0.0220\n",
            "     28        \u001b[36m0.8068\u001b[0m  0.0239\n",
            "     29        \u001b[36m0.7481\u001b[0m  0.0234\n",
            "     30        \u001b[36m0.6853\u001b[0m  0.0190\n",
            "     31        \u001b[36m0.6249\u001b[0m  0.0241\n",
            "     32        \u001b[36m0.5737\u001b[0m  0.0256\n",
            "     33        \u001b[36m0.5334\u001b[0m  0.0235\n",
            "     34        \u001b[36m0.5009\u001b[0m  0.0253\n",
            "     35        \u001b[36m0.4731\u001b[0m  0.0178\n",
            "     36        \u001b[36m0.4491\u001b[0m  0.0257\n",
            "     37        \u001b[36m0.4310\u001b[0m  0.0202\n",
            "     38        \u001b[36m0.4199\u001b[0m  0.0239\n",
            "     39        \u001b[36m0.4131\u001b[0m  0.0244\n",
            "     40        \u001b[36m0.4078\u001b[0m  0.0282\n",
            "     41        \u001b[36m0.4026\u001b[0m  0.0186\n",
            "     42        \u001b[36m0.3972\u001b[0m  0.0304\n",
            "     43        \u001b[36m0.3932\u001b[0m  0.0210\n",
            "     44        \u001b[36m0.3908\u001b[0m  0.0214\n",
            "     45        \u001b[36m0.3895\u001b[0m  0.0264\n",
            "     46        \u001b[36m0.3886\u001b[0m  0.0253\n",
            "     47        \u001b[36m0.3879\u001b[0m  0.0244\n",
            "     48        \u001b[36m0.3872\u001b[0m  0.0188\n",
            "     49        \u001b[36m0.3867\u001b[0m  0.0236\n",
            "     50        \u001b[36m0.3861\u001b[0m  0.0246\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9991\u001b[0m  0.0239\n",
            "      2        \u001b[36m0.9991\u001b[0m  0.0233\n",
            "      3        \u001b[36m0.9991\u001b[0m  0.0156\n",
            "      4        \u001b[36m0.9991\u001b[0m  0.0248\n",
            "      5        \u001b[36m0.9991\u001b[0m  0.0169\n",
            "      6        \u001b[36m0.9991\u001b[0m  0.0275\n",
            "      7        \u001b[36m0.9991\u001b[0m  0.0248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.9991\u001b[0m  0.0288\n",
            "      9        \u001b[36m0.9991\u001b[0m  0.0192\n",
            "     10        \u001b[36m0.9991\u001b[0m  0.0194\n",
            "     11        \u001b[36m0.9991\u001b[0m  0.0192\n",
            "     12        \u001b[36m0.9991\u001b[0m  0.0217\n",
            "     13        \u001b[36m0.9991\u001b[0m  0.0190\n",
            "     14        \u001b[36m0.9991\u001b[0m  0.0191\n",
            "     15        \u001b[36m0.9991\u001b[0m  0.0189\n",
            "     16        \u001b[36m0.9991\u001b[0m  0.0178\n",
            "     17        \u001b[36m0.9991\u001b[0m  0.0187\n",
            "     18        \u001b[36m0.9991\u001b[0m  0.0186\n",
            "     19        \u001b[36m0.9991\u001b[0m  0.0177\n",
            "     20        \u001b[36m0.9991\u001b[0m  0.0174\n",
            "     21        \u001b[36m0.9991\u001b[0m  0.0208\n",
            "     22        \u001b[36m0.9991\u001b[0m  0.0185\n",
            "     23        \u001b[36m0.9991\u001b[0m  0.0194\n",
            "     24        \u001b[36m0.9991\u001b[0m  0.0194\n",
            "     25        \u001b[36m0.9991\u001b[0m  0.0187\n",
            "     26        \u001b[36m0.9991\u001b[0m  0.0180\n",
            "     27        \u001b[36m0.9991\u001b[0m  0.0181\n",
            "     28        \u001b[36m0.9991\u001b[0m  0.0192\n",
            "     29        \u001b[36m0.9991\u001b[0m  0.0182\n",
            "     30        \u001b[36m0.9991\u001b[0m  0.0184\n",
            "     31        \u001b[36m0.9991\u001b[0m  0.0255\n",
            "     32        \u001b[36m0.9991\u001b[0m  0.0210\n",
            "     33        \u001b[36m0.9991\u001b[0m  0.0207\n",
            "     34        \u001b[36m0.9991\u001b[0m  0.0218\n",
            "     35        \u001b[36m0.9991\u001b[0m  0.0235\n",
            "     36        \u001b[36m0.9991\u001b[0m  0.0207\n",
            "     37        \u001b[36m0.9991\u001b[0m  0.0269\n",
            "     38        \u001b[36m0.9991\u001b[0m  0.0210\n",
            "     39        \u001b[36m0.9991\u001b[0m  0.0277\n",
            "     40        \u001b[36m0.9991\u001b[0m  0.0276\n",
            "     41        \u001b[36m0.9991\u001b[0m  0.0269\n",
            "     42        \u001b[36m0.9991\u001b[0m  0.0209\n",
            "     43        \u001b[36m0.9991\u001b[0m  0.0209\n",
            "     44        \u001b[36m0.9991\u001b[0m  0.0217\n",
            "     45        \u001b[36m0.9991\u001b[0m  0.0230\n",
            "     46        \u001b[36m0.9991\u001b[0m  0.0149\n",
            "     47        \u001b[36m0.9991\u001b[0m  0.0170\n",
            "     48        \u001b[36m0.9991\u001b[0m  0.0176\n",
            "     49        \u001b[36m0.9991\u001b[0m  0.0207\n",
            "     50        \u001b[36m0.9991\u001b[0m  0.0182\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9999\u001b[0m  0.0217\n",
            "      2        0.9999  0.0169\n",
            "      3        0.9999  0.0177\n",
            "      4        0.9999  0.0213\n",
            "      5        0.9999  0.0238\n",
            "      6        0.9999  0.0176\n",
            "      7        0.9999  0.0187\n",
            "      8        0.9999  0.0183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        0.9999  0.0261\n",
            "     10        0.9999  0.0234\n",
            "     11        0.9999  0.0201\n",
            "     12        0.9999  0.0194\n",
            "     13        0.9999  0.0191\n",
            "     14        0.9999  0.0160\n",
            "     15        0.9999  0.0199\n",
            "     16        0.9999  0.0166\n",
            "     17        0.9999  0.0273\n",
            "     18        0.9999  0.0270\n",
            "     19        0.9999  0.0184\n",
            "     20        0.9999  0.0185\n",
            "     21        0.9999  0.0235\n",
            "     22        0.9999  0.0223\n",
            "     23        0.9999  0.0267\n",
            "     24        0.9999  0.0240\n",
            "     25        0.9999  0.0198\n",
            "     26        0.9999  0.0217\n",
            "     27        0.9999  0.0202\n",
            "     28        0.9999  0.0258\n",
            "     29        0.9999  0.0277\n",
            "     30        0.9999  0.0241\n",
            "     31        0.9999  0.0214\n",
            "     32        0.9999  0.0222\n",
            "     33        \u001b[36m0.9999\u001b[0m  0.0214\n",
            "     34        \u001b[36m0.9999\u001b[0m  0.0205\n",
            "     35        0.9999  0.0247\n",
            "     36        0.9999  0.0241\n",
            "     37        0.9999  0.0222\n",
            "     38        0.9999  0.0318\n",
            "     39        0.9999  0.0267\n",
            "     40        0.9999  0.0194\n",
            "     41        0.9999  0.0177\n",
            "     42        0.9999  0.0241\n",
            "     43        0.9999  0.0187\n",
            "     44        0.9999  0.0192\n",
            "     45        0.9999  0.0171\n",
            "     46        0.9999  0.0126\n",
            "     47        0.9999  0.0266\n",
            "     48        0.9999  0.0212\n",
            "     49        0.9999  0.0226\n",
            "     50        0.9999  0.0188\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4767\u001b[0m  0.0242\n",
            "      2        \u001b[36m0.4702\u001b[0m  0.0283\n",
            "      3        \u001b[36m0.4648\u001b[0m  0.0182\n",
            "      4        \u001b[36m0.4571\u001b[0m  0.0346\n",
            "      5        \u001b[36m0.4520\u001b[0m  0.0269\n",
            "      6        \u001b[36m0.4476\u001b[0m  0.0236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.4438\u001b[0m  0.0158\n",
            "      8        \u001b[36m0.4403\u001b[0m  0.0244\n",
            "      9        \u001b[36m0.4372\u001b[0m  0.0250\n",
            "     10        \u001b[36m0.4344\u001b[0m  0.0233\n",
            "     11        \u001b[36m0.4319\u001b[0m  0.0284\n",
            "     12        \u001b[36m0.4296\u001b[0m  0.0265\n",
            "     13        \u001b[36m0.4276\u001b[0m  0.0289\n",
            "     14        \u001b[36m0.4258\u001b[0m  0.0374\n",
            "     15        \u001b[36m0.4242\u001b[0m  0.0287\n",
            "     16        \u001b[36m0.4227\u001b[0m  0.0251\n",
            "     17        \u001b[36m0.4213\u001b[0m  0.0186\n",
            "     18        \u001b[36m0.4201\u001b[0m  0.0288\n",
            "     19        \u001b[36m0.4189\u001b[0m  0.0279\n",
            "     20        \u001b[36m0.4178\u001b[0m  0.0233\n",
            "     21        \u001b[36m0.4167\u001b[0m  0.0262\n",
            "     22        \u001b[36m0.4158\u001b[0m  0.0218\n",
            "     23        \u001b[36m0.4149\u001b[0m  0.0210\n",
            "     24        \u001b[36m0.4140\u001b[0m  0.0351\n",
            "     25        \u001b[36m0.4130\u001b[0m  0.0278\n",
            "     26        \u001b[36m0.4122\u001b[0m  0.0242\n",
            "     27        \u001b[36m0.4114\u001b[0m  0.0227\n",
            "     28        \u001b[36m0.4106\u001b[0m  0.0275\n",
            "     29        \u001b[36m0.4098\u001b[0m  0.0214\n",
            "     30        \u001b[36m0.4090\u001b[0m  0.0188\n",
            "     31        \u001b[36m0.4081\u001b[0m  0.0225\n",
            "     32        \u001b[36m0.4073\u001b[0m  0.0268\n",
            "     33        \u001b[36m0.4064\u001b[0m  0.0309\n",
            "     34        \u001b[36m0.4055\u001b[0m  0.0237\n",
            "     35        \u001b[36m0.4046\u001b[0m  0.0249\n",
            "     36        \u001b[36m0.4036\u001b[0m  0.0199\n",
            "     37        \u001b[36m0.4026\u001b[0m  0.0215\n",
            "     38        \u001b[36m0.4015\u001b[0m  0.0246\n",
            "     39        \u001b[36m0.4004\u001b[0m  0.0186\n",
            "     40        \u001b[36m0.3993\u001b[0m  0.0229\n",
            "     41        \u001b[36m0.3981\u001b[0m  0.0302\n",
            "     42        \u001b[36m0.3970\u001b[0m  0.0257\n",
            "     43        \u001b[36m0.3960\u001b[0m  0.0200\n",
            "     44        \u001b[36m0.3951\u001b[0m  0.0231\n",
            "     45        \u001b[36m0.3936\u001b[0m  0.0286\n",
            "     46        \u001b[36m0.3930\u001b[0m  0.0262\n",
            "     47        \u001b[36m0.3924\u001b[0m  0.0293\n",
            "     48        \u001b[36m0.3918\u001b[0m  0.0295\n",
            "     49        \u001b[36m0.3913\u001b[0m  0.0273\n",
            "     50        \u001b[36m0.3909\u001b[0m  0.0254\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7707\u001b[0m  0.0187\n",
            "      2        \u001b[36m0.7560\u001b[0m  0.0182\n",
            "      3        \u001b[36m0.7409\u001b[0m  0.0242\n",
            "      4        \u001b[36m0.7262\u001b[0m  0.0223\n",
            "      5        \u001b[36m0.7111\u001b[0m  0.0223\n",
            "      6        \u001b[36m0.6997\u001b[0m  0.0230\n",
            "      7        \u001b[36m0.6898\u001b[0m  0.0240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.6809\u001b[0m  0.0320\n",
            "      9        \u001b[36m0.6737\u001b[0m  0.0273\n",
            "     10        \u001b[36m0.6676\u001b[0m  0.0271\n",
            "     11        \u001b[36m0.6622\u001b[0m  0.0225\n",
            "     12        \u001b[36m0.6571\u001b[0m  0.0221\n",
            "     13        \u001b[36m0.6523\u001b[0m  0.0232\n",
            "     14        \u001b[36m0.6472\u001b[0m  0.0204\n",
            "     15        \u001b[36m0.6420\u001b[0m  0.0231\n",
            "     16        \u001b[36m0.6359\u001b[0m  0.0251\n",
            "     17        \u001b[36m0.6282\u001b[0m  0.0224\n",
            "     18        \u001b[36m0.6193\u001b[0m  0.0233\n",
            "     19        \u001b[36m0.6089\u001b[0m  0.0222\n",
            "     20        \u001b[36m0.5976\u001b[0m  0.0225\n",
            "     21        \u001b[36m0.5865\u001b[0m  0.0269\n",
            "     22        \u001b[36m0.5763\u001b[0m  0.0234\n",
            "     23        \u001b[36m0.5673\u001b[0m  0.0227\n",
            "     24        \u001b[36m0.5594\u001b[0m  0.0248\n",
            "     25        \u001b[36m0.5520\u001b[0m  0.0275\n",
            "     26        \u001b[36m0.5450\u001b[0m  0.0261\n",
            "     27        \u001b[36m0.5371\u001b[0m  0.0254\n",
            "     28        \u001b[36m0.5295\u001b[0m  0.0248\n",
            "     29        \u001b[36m0.5215\u001b[0m  0.0203\n",
            "     30        \u001b[36m0.5130\u001b[0m  0.0308\n",
            "     31        \u001b[36m0.5044\u001b[0m  0.0273\n",
            "     32        \u001b[36m0.4958\u001b[0m  0.0310\n",
            "     33        \u001b[36m0.4875\u001b[0m  0.0238\n",
            "     34        \u001b[36m0.4803\u001b[0m  0.0195\n",
            "     35        \u001b[36m0.4739\u001b[0m  0.0194\n",
            "     36        \u001b[36m0.4647\u001b[0m  0.0190\n",
            "     37        \u001b[36m0.4597\u001b[0m  0.0214\n",
            "     38        \u001b[36m0.4552\u001b[0m  0.0214\n",
            "     39        \u001b[36m0.4512\u001b[0m  0.0230\n",
            "     40        \u001b[36m0.4483\u001b[0m  0.0206\n",
            "     41        \u001b[36m0.4459\u001b[0m  0.0264\n",
            "     42        \u001b[36m0.4437\u001b[0m  0.0221\n",
            "     43        \u001b[36m0.4416\u001b[0m  0.0211\n",
            "     44        \u001b[36m0.4397\u001b[0m  0.0261\n",
            "     45        \u001b[36m0.4379\u001b[0m  0.0240\n",
            "     46        \u001b[36m0.4362\u001b[0m  0.0231\n",
            "     47        \u001b[36m0.4347\u001b[0m  0.0236\n",
            "     48        \u001b[36m0.4332\u001b[0m  0.0215\n",
            "     49        \u001b[36m0.4318\u001b[0m  0.0257\n",
            "     50        \u001b[36m0.4305\u001b[0m  0.0250\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.5903\u001b[0m  0.0122\n",
            "      2        \u001b[36m0.5897\u001b[0m  0.0224\n",
            "      3        \u001b[36m0.5891\u001b[0m  0.0147\n",
            "      4        \u001b[36m0.5885\u001b[0m  0.0204\n",
            "      5        \u001b[36m0.5880\u001b[0m  0.0194\n",
            "      6        \u001b[36m0.5874\u001b[0m  0.0217\n",
            "      7        \u001b[36m0.5868\u001b[0m  0.0275\n",
            "      8        \u001b[36m0.5863\u001b[0m  0.0181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.5857\u001b[0m  0.0221\n",
            "     10        \u001b[36m0.5851\u001b[0m  0.0195\n",
            "     11        \u001b[36m0.5846\u001b[0m  0.0205\n",
            "     12        \u001b[36m0.5840\u001b[0m  0.0194\n",
            "     13        \u001b[36m0.5834\u001b[0m  0.0213\n",
            "     14        \u001b[36m0.5828\u001b[0m  0.0191\n",
            "     15        \u001b[36m0.5801\u001b[0m  0.0180\n",
            "     16        \u001b[36m0.5796\u001b[0m  0.0212\n",
            "     17        \u001b[36m0.5790\u001b[0m  0.0180\n",
            "     18        \u001b[36m0.5784\u001b[0m  0.0204\n",
            "     19        \u001b[36m0.5779\u001b[0m  0.0240\n",
            "     20        \u001b[36m0.5773\u001b[0m  0.0215\n",
            "     21        \u001b[36m0.5768\u001b[0m  0.0217\n",
            "     22        \u001b[36m0.5762\u001b[0m  0.0195\n",
            "     23        \u001b[36m0.5756\u001b[0m  0.0194\n",
            "     24        \u001b[36m0.5751\u001b[0m  0.0187\n",
            "     25        \u001b[36m0.5745\u001b[0m  0.0183\n",
            "     26        \u001b[36m0.5739\u001b[0m  0.0187\n",
            "     27        \u001b[36m0.5734\u001b[0m  0.0206\n",
            "     28        \u001b[36m0.5728\u001b[0m  0.0179\n",
            "     29        \u001b[36m0.5722\u001b[0m  0.0218\n",
            "     30        \u001b[36m0.5716\u001b[0m  0.0195\n",
            "     31        \u001b[36m0.5711\u001b[0m  0.0194\n",
            "     32        \u001b[36m0.5705\u001b[0m  0.0198\n",
            "     33        \u001b[36m0.5699\u001b[0m  0.0195\n",
            "     34        \u001b[36m0.5694\u001b[0m  0.0199\n",
            "     35        \u001b[36m0.5688\u001b[0m  0.0181\n",
            "     36        \u001b[36m0.5682\u001b[0m  0.0227\n",
            "     37        \u001b[36m0.5676\u001b[0m  0.0207\n",
            "     38        \u001b[36m0.5671\u001b[0m  0.0206\n",
            "     39        \u001b[36m0.5665\u001b[0m  0.0253\n",
            "     40        \u001b[36m0.5659\u001b[0m  0.0193\n",
            "     41        \u001b[36m0.5653\u001b[0m  0.0199\n",
            "     42        \u001b[36m0.5648\u001b[0m  0.0183\n",
            "     43        \u001b[36m0.5642\u001b[0m  0.0205\n",
            "     44        \u001b[36m0.5636\u001b[0m  0.0170\n",
            "     45        \u001b[36m0.5630\u001b[0m  0.0172\n",
            "     46        \u001b[36m0.5624\u001b[0m  0.0176\n",
            "     47        \u001b[36m0.5619\u001b[0m  0.0175\n",
            "     48        \u001b[36m0.5613\u001b[0m  0.0212\n",
            "     49        \u001b[36m0.5607\u001b[0m  0.0245\n",
            "     50        \u001b[36m0.5601\u001b[0m  0.0179\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.5618\u001b[0m  0.0182\n",
            "      2        \u001b[36m0.5613\u001b[0m  0.0174\n",
            "      3        \u001b[36m0.5608\u001b[0m  0.0161\n",
            "      4        \u001b[36m0.5603\u001b[0m  0.0169\n",
            "      5        \u001b[36m0.5597\u001b[0m  0.0179\n",
            "      6        \u001b[36m0.5592\u001b[0m  0.0186\n",
            "      7        \u001b[36m0.5587\u001b[0m  0.0162\n",
            "      8        \u001b[36m0.5582\u001b[0m  0.0193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.5576\u001b[0m  0.0220\n",
            "     10        \u001b[36m0.5570\u001b[0m  0.0209\n",
            "     11        \u001b[36m0.5565\u001b[0m  0.0219\n",
            "     12        \u001b[36m0.5560\u001b[0m  0.0223\n",
            "     13        \u001b[36m0.5555\u001b[0m  0.0232\n",
            "     14        \u001b[36m0.5550\u001b[0m  0.0222\n",
            "     15        \u001b[36m0.5545\u001b[0m  0.0209\n",
            "     16        \u001b[36m0.5540\u001b[0m  0.0224\n",
            "     17        \u001b[36m0.5535\u001b[0m  0.0244\n",
            "     18        \u001b[36m0.5530\u001b[0m  0.0185\n",
            "     19        \u001b[36m0.5525\u001b[0m  0.0179\n",
            "     20        \u001b[36m0.5521\u001b[0m  0.0187\n",
            "     21        \u001b[36m0.5516\u001b[0m  0.0175\n",
            "     22        \u001b[36m0.5511\u001b[0m  0.0173\n",
            "     23        \u001b[36m0.5506\u001b[0m  0.0182\n",
            "     24        \u001b[36m0.5501\u001b[0m  0.0220\n",
            "     25        \u001b[36m0.5496\u001b[0m  0.0245\n",
            "     26        \u001b[36m0.5492\u001b[0m  0.0187\n",
            "     27        \u001b[36m0.5487\u001b[0m  0.0194\n",
            "     28        \u001b[36m0.5482\u001b[0m  0.0235\n",
            "     29        \u001b[36m0.5477\u001b[0m  0.0224\n",
            "     30        \u001b[36m0.5473\u001b[0m  0.0169\n",
            "     31        \u001b[36m0.5468\u001b[0m  0.0173\n",
            "     32        \u001b[36m0.5463\u001b[0m  0.0177\n",
            "     33        \u001b[36m0.5459\u001b[0m  0.0172\n",
            "     34        \u001b[36m0.5454\u001b[0m  0.0178\n",
            "     35        \u001b[36m0.5450\u001b[0m  0.0168\n",
            "     36        \u001b[36m0.5445\u001b[0m  0.0187\n",
            "     37        \u001b[36m0.5436\u001b[0m  0.0176\n",
            "     38        \u001b[36m0.5430\u001b[0m  0.0231\n",
            "     39        \u001b[36m0.5425\u001b[0m  0.0192\n",
            "     40        \u001b[36m0.5421\u001b[0m  0.0206\n",
            "     41        \u001b[36m0.5416\u001b[0m  0.0203\n",
            "     42        \u001b[36m0.5412\u001b[0m  0.0300\n",
            "     43        \u001b[36m0.5408\u001b[0m  0.0221\n",
            "     44        \u001b[36m0.5403\u001b[0m  0.0216\n",
            "     45        \u001b[36m0.5399\u001b[0m  0.0211\n",
            "     46        \u001b[36m0.5395\u001b[0m  0.0178\n",
            "     47        \u001b[36m0.5390\u001b[0m  0.0253\n",
            "     48        \u001b[36m0.5386\u001b[0m  0.0214\n",
            "     49        \u001b[36m0.5382\u001b[0m  0.0262\n",
            "     50        \u001b[36m0.5378\u001b[0m  0.0265\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6440\u001b[0m  0.0290\n",
            "      2        \u001b[36m0.6156\u001b[0m  0.0255\n",
            "      3        \u001b[36m0.5970\u001b[0m  0.0276\n",
            "      4        \u001b[36m0.5732\u001b[0m  0.0276\n",
            "      5        \u001b[36m0.5542\u001b[0m  0.0246\n",
            "      6        \u001b[36m0.5381\u001b[0m  0.0264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.5241\u001b[0m  0.0304\n",
            "      8        \u001b[36m0.5117\u001b[0m  0.0249\n",
            "      9        \u001b[36m0.5000\u001b[0m  0.0232\n",
            "     10        \u001b[36m0.4893\u001b[0m  0.0233\n",
            "     11        \u001b[36m0.4784\u001b[0m  0.0208\n",
            "     12        \u001b[36m0.4683\u001b[0m  0.0258\n",
            "     13        \u001b[36m0.4607\u001b[0m  0.0183\n",
            "     14        \u001b[36m0.4517\u001b[0m  0.0247\n",
            "     15        \u001b[36m0.4436\u001b[0m  0.0266\n",
            "     16        \u001b[36m0.4390\u001b[0m  0.0332\n",
            "     17        \u001b[36m0.4343\u001b[0m  0.0229\n",
            "     18        \u001b[36m0.4304\u001b[0m  0.0228\n",
            "     19        \u001b[36m0.4272\u001b[0m  0.0250\n",
            "     20        \u001b[36m0.4242\u001b[0m  0.0213\n",
            "     21        \u001b[36m0.4213\u001b[0m  0.0356\n",
            "     22        \u001b[36m0.4186\u001b[0m  0.0278\n",
            "     23        0.4218  0.0255\n",
            "     24        \u001b[36m0.4127\u001b[0m  0.0196\n",
            "     25        \u001b[36m0.4099\u001b[0m  0.0304\n",
            "     26        \u001b[36m0.4073\u001b[0m  0.0318\n",
            "     27        \u001b[36m0.4048\u001b[0m  0.0284\n",
            "     28        \u001b[36m0.4024\u001b[0m  0.0262\n",
            "     29        \u001b[36m0.4002\u001b[0m  0.0232\n",
            "     30        \u001b[36m0.3983\u001b[0m  0.0292\n",
            "     31        \u001b[36m0.3966\u001b[0m  0.0351\n",
            "     32        \u001b[36m0.3953\u001b[0m  0.0289\n",
            "     33        \u001b[36m0.3941\u001b[0m  0.0308\n",
            "     34        \u001b[36m0.3932\u001b[0m  0.0233\n",
            "     35        \u001b[36m0.3923\u001b[0m  0.0298\n",
            "     36        \u001b[36m0.3916\u001b[0m  0.0292\n",
            "     37        \u001b[36m0.3909\u001b[0m  0.0238\n",
            "     38        \u001b[36m0.3903\u001b[0m  0.0202\n",
            "     39        \u001b[36m0.3879\u001b[0m  0.0204\n",
            "     40        \u001b[36m0.3873\u001b[0m  0.0231\n",
            "     41        \u001b[36m0.3868\u001b[0m  0.0236\n",
            "     42        \u001b[36m0.3863\u001b[0m  0.0232\n",
            "     43        \u001b[36m0.3857\u001b[0m  0.0211\n",
            "     44        \u001b[36m0.3852\u001b[0m  0.0231\n",
            "     45        \u001b[36m0.3848\u001b[0m  0.0261\n",
            "     46        \u001b[36m0.3843\u001b[0m  0.0253\n",
            "     47        \u001b[36m0.3838\u001b[0m  0.0270\n",
            "     48        \u001b[36m0.3833\u001b[0m  0.0258\n",
            "     49        \u001b[36m0.3829\u001b[0m  0.0235\n",
            "     50        \u001b[36m0.3825\u001b[0m  0.0236\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7014\u001b[0m  0.0167\n",
            "      2        \u001b[36m0.6742\u001b[0m  0.0206\n",
            "      3        \u001b[36m0.6373\u001b[0m  0.0280\n",
            "      4        \u001b[36m0.5847\u001b[0m  0.0270\n",
            "      5        \u001b[36m0.5268\u001b[0m  0.0263\n",
            "      6        \u001b[36m0.4788\u001b[0m  0.0271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.4500\u001b[0m  0.0228\n",
            "      8        \u001b[36m0.4348\u001b[0m  0.0291\n",
            "      9        \u001b[36m0.4263\u001b[0m  0.0267\n",
            "     10        \u001b[36m0.4212\u001b[0m  0.0262\n",
            "     11        \u001b[36m0.4177\u001b[0m  0.0237\n",
            "     12        \u001b[36m0.4152\u001b[0m  0.0230\n",
            "     13        \u001b[36m0.4132\u001b[0m  0.0272\n",
            "     14        \u001b[36m0.4115\u001b[0m  0.0287\n",
            "     15        \u001b[36m0.4106\u001b[0m  0.0260\n",
            "     16        \u001b[36m0.4089\u001b[0m  0.0251\n",
            "     17        \u001b[36m0.4078\u001b[0m  0.0218\n",
            "     18        \u001b[36m0.4069\u001b[0m  0.0288\n",
            "     19        \u001b[36m0.4060\u001b[0m  0.0255\n",
            "     20        \u001b[36m0.4052\u001b[0m  0.0298\n",
            "     21        \u001b[36m0.4044\u001b[0m  0.0282\n",
            "     22        \u001b[36m0.4038\u001b[0m  0.0235\n",
            "     23        \u001b[36m0.4031\u001b[0m  0.0213\n",
            "     24        \u001b[36m0.4025\u001b[0m  0.0212\n",
            "     25        \u001b[36m0.4019\u001b[0m  0.0232\n",
            "     26        \u001b[36m0.4014\u001b[0m  0.0346\n",
            "     27        \u001b[36m0.4009\u001b[0m  0.0251\n",
            "     28        \u001b[36m0.4004\u001b[0m  0.0199\n",
            "     29        \u001b[36m0.3999\u001b[0m  0.0207\n",
            "     30        \u001b[36m0.3994\u001b[0m  0.0229\n",
            "     31        \u001b[36m0.3989\u001b[0m  0.0233\n",
            "     32        \u001b[36m0.3985\u001b[0m  0.0265\n",
            "     33        \u001b[36m0.3980\u001b[0m  0.0180\n",
            "     34        \u001b[36m0.3976\u001b[0m  0.0299\n",
            "     35        \u001b[36m0.3971\u001b[0m  0.0254\n",
            "     36        \u001b[36m0.3967\u001b[0m  0.0226\n",
            "     37        \u001b[36m0.3963\u001b[0m  0.0222\n",
            "     38        \u001b[36m0.3959\u001b[0m  0.0231\n",
            "     39        \u001b[36m0.3954\u001b[0m  0.0228\n",
            "     40        \u001b[36m0.3949\u001b[0m  0.0239\n",
            "     41        \u001b[36m0.3946\u001b[0m  0.0233\n",
            "     42        \u001b[36m0.3942\u001b[0m  0.0278\n",
            "     43        \u001b[36m0.3939\u001b[0m  0.0237\n",
            "     44        \u001b[36m0.3936\u001b[0m  0.0252\n",
            "     45        \u001b[36m0.3932\u001b[0m  0.0241\n",
            "     46        \u001b[36m0.3929\u001b[0m  0.0183\n",
            "     47        \u001b[36m0.3926\u001b[0m  0.0290\n",
            "     48        \u001b[36m0.3924\u001b[0m  0.0322\n",
            "     49        \u001b[36m0.3921\u001b[0m  0.0254\n",
            "     50        \u001b[36m0.3918\u001b[0m  0.0252\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.5846\u001b[0m  0.0205\n",
            "      2        \u001b[36m0.5824\u001b[0m  0.0144\n",
            "      3        \u001b[36m0.5802\u001b[0m  0.0238\n",
            "      4        \u001b[36m0.5747\u001b[0m  0.0179\n",
            "      5        \u001b[36m0.5720\u001b[0m  0.0168\n",
            "      6        \u001b[36m0.5701\u001b[0m  0.0180\n",
            "      7        \u001b[36m0.5682\u001b[0m  0.0190\n",
            "      8        \u001b[36m0.5663\u001b[0m  0.0200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.5645\u001b[0m  0.0174\n",
            "     10        \u001b[36m0.5627\u001b[0m  0.0177\n",
            "     11        \u001b[36m0.5610\u001b[0m  0.0179\n",
            "     12        \u001b[36m0.5593\u001b[0m  0.0186\n",
            "     13        \u001b[36m0.5576\u001b[0m  0.0179\n",
            "     14        \u001b[36m0.5559\u001b[0m  0.0168\n",
            "     15        \u001b[36m0.5543\u001b[0m  0.0173\n",
            "     16        \u001b[36m0.5528\u001b[0m  0.0168\n",
            "     17        \u001b[36m0.5512\u001b[0m  0.0232\n",
            "     18        \u001b[36m0.5497\u001b[0m  0.0182\n",
            "     19        \u001b[36m0.5483\u001b[0m  0.0179\n",
            "     20        \u001b[36m0.5468\u001b[0m  0.0217\n",
            "     21        \u001b[36m0.5454\u001b[0m  0.0246\n",
            "     22        \u001b[36m0.5441\u001b[0m  0.0199\n",
            "     23        \u001b[36m0.5427\u001b[0m  0.0261\n",
            "     24        \u001b[36m0.5414\u001b[0m  0.0237\n",
            "     25        \u001b[36m0.5402\u001b[0m  0.0193\n",
            "     26        \u001b[36m0.5389\u001b[0m  0.0209\n",
            "     27        \u001b[36m0.5377\u001b[0m  0.0199\n",
            "     28        \u001b[36m0.5365\u001b[0m  0.0197\n",
            "     29        \u001b[36m0.5354\u001b[0m  0.0208\n",
            "     30        \u001b[36m0.5343\u001b[0m  0.0217\n",
            "     31        \u001b[36m0.5332\u001b[0m  0.0211\n",
            "     32        \u001b[36m0.5321\u001b[0m  0.0195\n",
            "     33        \u001b[36m0.5311\u001b[0m  0.0231\n",
            "     34        \u001b[36m0.5300\u001b[0m  0.0231\n",
            "     35        \u001b[36m0.5290\u001b[0m  0.0223\n",
            "     36        \u001b[36m0.5281\u001b[0m  0.0237\n",
            "     37        \u001b[36m0.5271\u001b[0m  0.0241\n",
            "     38        \u001b[36m0.5262\u001b[0m  0.0231\n",
            "     39        \u001b[36m0.5253\u001b[0m  0.0246\n",
            "     40        \u001b[36m0.5240\u001b[0m  0.0237\n",
            "     41        \u001b[36m0.5235\u001b[0m  0.0264\n",
            "     42        \u001b[36m0.5227\u001b[0m  0.0321\n",
            "     43        \u001b[36m0.5219\u001b[0m  0.0198\n",
            "     44        \u001b[36m0.5211\u001b[0m  0.0204\n",
            "     45        \u001b[36m0.5203\u001b[0m  0.0188\n",
            "     46        \u001b[36m0.5195\u001b[0m  0.0223\n",
            "     47        \u001b[36m0.5187\u001b[0m  0.0192\n",
            "     48        \u001b[36m0.5180\u001b[0m  0.0191\n",
            "     49        \u001b[36m0.5173\u001b[0m  0.0188\n",
            "     50        \u001b[36m0.5165\u001b[0m  0.0190\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4315\u001b[0m  0.0190\n",
            "      2        \u001b[36m0.4313\u001b[0m  0.0212\n",
            "      3        \u001b[36m0.4311\u001b[0m  0.0211\n",
            "      4        \u001b[36m0.4310\u001b[0m  0.0188\n",
            "      5        \u001b[36m0.4308\u001b[0m  0.0183\n",
            "      6        \u001b[36m0.4306\u001b[0m  0.0214\n",
            "      7        \u001b[36m0.4304\u001b[0m  0.0124\n",
            "      8        \u001b[36m0.4303\u001b[0m  0.0227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.4301\u001b[0m  0.0219\n",
            "     10        \u001b[36m0.4299\u001b[0m  0.0351\n",
            "     11        \u001b[36m0.4298\u001b[0m  0.0289\n",
            "     12        \u001b[36m0.4296\u001b[0m  0.0210\n",
            "     13        \u001b[36m0.4294\u001b[0m  0.0214\n",
            "     14        \u001b[36m0.4293\u001b[0m  0.0234\n",
            "     15        \u001b[36m0.4291\u001b[0m  0.0196\n",
            "     16        \u001b[36m0.4289\u001b[0m  0.0253\n",
            "     17        \u001b[36m0.4288\u001b[0m  0.0211\n",
            "     18        \u001b[36m0.4286\u001b[0m  0.0209\n",
            "     19        \u001b[36m0.4285\u001b[0m  0.0200\n",
            "     20        \u001b[36m0.4283\u001b[0m  0.0214\n",
            "     21        \u001b[36m0.4281\u001b[0m  0.0220\n",
            "     22        \u001b[36m0.4280\u001b[0m  0.0173\n",
            "     23        \u001b[36m0.4278\u001b[0m  0.0196\n",
            "     24        \u001b[36m0.4277\u001b[0m  0.0222\n",
            "     25        \u001b[36m0.4275\u001b[0m  0.0185\n",
            "     26        \u001b[36m0.4274\u001b[0m  0.0167\n",
            "     27        \u001b[36m0.4272\u001b[0m  0.0177\n",
            "     28        \u001b[36m0.4271\u001b[0m  0.0189\n",
            "     29        \u001b[36m0.4269\u001b[0m  0.0186\n",
            "     30        \u001b[36m0.4268\u001b[0m  0.0193\n",
            "     31        \u001b[36m0.4266\u001b[0m  0.0184\n",
            "     32        \u001b[36m0.4265\u001b[0m  0.0188\n",
            "     33        \u001b[36m0.4263\u001b[0m  0.0201\n",
            "     34        \u001b[36m0.4262\u001b[0m  0.0168\n",
            "     35        \u001b[36m0.4261\u001b[0m  0.0202\n",
            "     36        \u001b[36m0.4259\u001b[0m  0.0217\n",
            "     37        \u001b[36m0.4258\u001b[0m  0.0210\n",
            "     38        \u001b[36m0.4256\u001b[0m  0.0174\n",
            "     39        \u001b[36m0.4255\u001b[0m  0.0183\n",
            "     40        \u001b[36m0.4254\u001b[0m  0.0196\n",
            "     41        \u001b[36m0.4252\u001b[0m  0.0171\n",
            "     42        \u001b[36m0.4251\u001b[0m  0.0166\n",
            "     43        \u001b[36m0.4249\u001b[0m  0.0175\n",
            "     44        \u001b[36m0.4248\u001b[0m  0.0183\n",
            "     45        \u001b[36m0.4247\u001b[0m  0.0192\n",
            "     46        \u001b[36m0.4245\u001b[0m  0.0174\n",
            "     47        \u001b[36m0.4244\u001b[0m  0.0180\n",
            "     48        \u001b[36m0.4243\u001b[0m  0.0200\n",
            "     49        \u001b[36m0.4241\u001b[0m  0.0171\n",
            "     50        \u001b[36m0.4240\u001b[0m  0.0170\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0346\n",
            "      2        1.0000  0.0270\n",
            "      3        1.0000  0.0260\n",
            "      4        1.0000  0.0277\n",
            "      5        1.0000  0.0284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      6        1.0000  0.0304\n",
            "      7        1.0000  0.0322\n",
            "      8        1.0000  0.0271\n",
            "      9        1.0000  0.0224\n",
            "     10        1.0000  0.0236\n",
            "     11        1.0000  0.0196\n",
            "     12        1.0000  0.0285\n",
            "     13        1.0000  0.0237\n",
            "     14        1.0000  0.0268\n",
            "     15        1.0000  0.0162\n",
            "     16        1.0000  0.0217\n",
            "     17        1.0000  0.0226\n",
            "     18        1.0000  0.0286\n",
            "     19        1.0000  0.0250\n",
            "     20        1.0000  0.0239\n",
            "     21        1.0000  0.0227\n",
            "     22        1.0000  0.0230\n",
            "     23        1.0000  0.0172\n",
            "     24        1.0000  0.0231\n",
            "     25        1.0000  0.0225\n",
            "     26        1.0000  0.0239\n",
            "     27        1.0000  0.0217\n",
            "     28        1.0000  0.0225\n",
            "     29        1.0000  0.0208\n",
            "     30        1.0000  0.0239\n",
            "     31        1.0000  0.0241\n",
            "     32        1.0000  0.0226\n",
            "     33        1.0000  0.0245\n",
            "     34        1.0000  0.0192\n",
            "     35        1.0000  0.0240\n",
            "     36        1.0000  0.0280\n",
            "     37        1.0000  0.0330\n",
            "     38        1.0000  0.0259\n",
            "     39        1.0000  0.0280\n",
            "     40        1.0000  0.0280\n",
            "     41        1.0000  0.0211\n",
            "     42        1.0000  0.0303\n",
            "     43        1.0000  0.0308\n",
            "     44        1.0000  0.0280\n",
            "     45        1.0000  0.0235\n",
            "     46        1.0000  0.0203\n",
            "     47        1.0000  0.0185\n",
            "     48        1.0000  0.0219\n",
            "     49        1.0000  0.0216\n",
            "     50        1.0000  0.0308\n",
            "     51        1.0000  0.0238\n",
            "     52        1.0000  0.0164\n",
            "     53        1.0000  0.0228\n",
            "     54        1.0000  0.0252\n",
            "     55        1.0000  0.0225\n",
            "     56        1.0000  0.0208\n",
            "     57        1.0000  0.0212\n",
            "     58        1.0000  0.0163\n",
            "     59        1.0000  0.0229\n",
            "     60        1.0000  0.0242\n",
            "     61        1.0000  0.0229\n",
            "     62        1.0000  0.0206\n",
            "     63        1.0000  0.0244\n",
            "     64        1.0000  0.0181\n",
            "     65        1.0000  0.0215\n",
            "     66        1.0000  0.0262\n",
            "     67        1.0000  0.0271\n",
            "     68        1.0000  0.0236\n",
            "     69        1.0000  0.0232\n",
            "     70        1.0000  0.0222\n",
            "     71        1.0000  0.0269\n",
            "     72        1.0000  0.0223\n",
            "     73        1.0000  0.0220\n",
            "     74        1.0000  0.0260\n",
            "     75        1.0000  0.0278\n",
            "     76        1.0000  0.0296\n",
            "     77        1.0000  0.0277\n",
            "     78        1.0000  0.0242\n",
            "     79        1.0000  0.0198\n",
            "     80        1.0000  0.0316\n",
            "     81        1.0000  0.0301\n",
            "     82        1.0000  0.0263\n",
            "     83        1.0000  0.0265\n",
            "     84        1.0000  0.0208\n",
            "     85        1.0000  0.0191\n",
            "     86        1.0000  0.0238\n",
            "     87        1.0000  0.0206\n",
            "     88        1.0000  0.0262\n",
            "     89        1.0000  0.0314\n",
            "     90        1.0000  0.0256\n",
            "     91        1.0000  0.0192\n",
            "     92        1.0000  0.0268\n",
            "     93        1.0000  0.0243\n",
            "     94        1.0000  0.0231\n",
            "     95        1.0000  0.0213\n",
            "     96        1.0000  0.0159\n",
            "     97        1.0000  0.0220\n",
            "     98        1.0000  0.0236\n",
            "     99        1.0000  0.0202\n",
            "    100        1.0000  0.0226\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0207\n",
            "      2        1.0000  0.0251\n",
            "      3        1.0000  0.0264\n",
            "      4        1.0000  0.0214\n",
            "      5        1.0000  0.0240\n",
            "      6        1.0000  0.0233\n",
            "      7        1.0000  0.0239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        1.0000  0.0263\n",
            "      9        1.0000  0.0363\n",
            "     10        1.0000  0.0258\n",
            "     11        1.0000  0.0274\n",
            "     12        1.0000  0.0214\n",
            "     13        1.0000  0.0235\n",
            "     14        1.0000  0.0212\n",
            "     15        1.0000  0.0217\n",
            "     16        1.0000  0.0214\n",
            "     17        1.0000  0.0228\n",
            "     18        1.0000  0.0214\n",
            "     19        1.0000  0.0217\n",
            "     20        1.0000  0.0227\n",
            "     21        1.0000  0.0240\n",
            "     22        1.0000  0.0223\n",
            "     23        1.0000  0.0249\n",
            "     24        1.0000  0.0252\n",
            "     25        1.0000  0.0225\n",
            "     26        1.0000  0.0217\n",
            "     27        1.0000  0.0219\n",
            "     28        1.0000  0.0224\n",
            "     29        1.0000  0.0231\n",
            "     30        1.0000  0.0267\n",
            "     31        1.0000  0.0219\n",
            "     32        1.0000  0.0202\n",
            "     33        1.0000  0.0171\n",
            "     34        1.0000  0.0228\n",
            "     35        1.0000  0.0211\n",
            "     36        1.0000  0.0221\n",
            "     37        1.0000  0.0243\n",
            "     38        1.0000  0.0227\n",
            "     39        1.0000  0.0243\n",
            "     40        1.0000  0.0224\n",
            "     41        1.0000  0.0248\n",
            "     42        1.0000  0.0223\n",
            "     43        1.0000  0.0228\n",
            "     44        1.0000  0.0230\n",
            "     45        1.0000  0.0213\n",
            "     46        1.0000  0.0271\n",
            "     47        1.0000  0.0284\n",
            "     48        1.0000  0.0409\n",
            "     49        1.0000  0.0245\n",
            "     50        1.0000  0.0264\n",
            "     51        1.0000  0.0178\n",
            "     52        1.0000  0.0302\n",
            "     53        1.0000  0.0249\n",
            "     54        1.0000  0.0243\n",
            "     55        1.0000  0.0182\n",
            "     56        1.0000  0.0219\n",
            "     57        1.0000  0.0235\n",
            "     58        1.0000  0.0244\n",
            "     59        1.0000  0.0216\n",
            "     60        1.0000  0.0223\n",
            "     61        1.0000  0.0240\n",
            "     62        1.0000  0.0210\n",
            "     63        1.0000  0.0220\n",
            "     64        1.0000  0.0229\n",
            "     65        1.0000  0.0221\n",
            "     66        1.0000  0.0217\n",
            "     67        1.0000  0.0226\n",
            "     68        1.0000  0.0220\n",
            "     69        1.0000  0.0211\n",
            "     70        1.0000  0.0212\n",
            "     71        1.0000  0.0220\n",
            "     72        1.0000  0.0285\n",
            "     73        1.0000  0.0243\n",
            "     74        1.0000  0.0178\n",
            "     75        1.0000  0.0241\n",
            "     76        1.0000  0.0240\n",
            "     77        1.0000  0.0241\n",
            "     78        1.0000  0.0250\n",
            "     79        1.0000  0.0248\n",
            "     80        1.0000  0.0230\n",
            "     81        1.0000  0.0251\n",
            "     82        1.0000  0.0251\n",
            "     83        1.0000  0.0258\n",
            "     84        1.0000  0.0333\n",
            "     85        1.0000  0.0304\n",
            "     86        1.0000  0.0278\n",
            "     87        1.0000  0.0230\n",
            "     88        1.0000  0.0247\n",
            "     89        1.0000  0.0238\n",
            "     90        1.0000  0.0220\n",
            "     91        1.0000  0.0168\n",
            "     92        1.0000  0.0267\n",
            "     93        1.0000  0.0253\n",
            "     94        1.0000  0.0234\n",
            "     95        1.0000  0.0226\n",
            "     96        1.0000  0.0196\n",
            "     97        1.0000  0.0235\n",
            "     98        1.0000  0.0218\n",
            "     99        1.0000  0.0217\n",
            "    100        1.0000  0.0251\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0152\n",
            "      2        1.0000  0.0171\n",
            "      3        1.0000  0.0172\n",
            "      4        1.0000  0.0144\n",
            "      5        1.0000  0.0221\n",
            "      6        1.0000  0.0214\n",
            "      7        1.0000  0.0201\n",
            "      8        1.0000  0.0200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        1.0000  0.0167\n",
            "     10        1.0000  0.0205\n",
            "     11        1.0000  0.0192\n",
            "     12        1.0000  0.0177\n",
            "     13        1.0000  0.0173\n",
            "     14        1.0000  0.0190\n",
            "     15        1.0000  0.0186\n",
            "     16        1.0000  0.0201\n",
            "     17        1.0000  0.0196\n",
            "     18        1.0000  0.0181\n",
            "     19        1.0000  0.0184\n",
            "     20        1.0000  0.0206\n",
            "     21        1.0000  0.0196\n",
            "     22        1.0000  0.0194\n",
            "     23        1.0000  0.0194\n",
            "     24        1.0000  0.0189\n",
            "     25        1.0000  0.0195\n",
            "     26        1.0000  0.0223\n",
            "     27        1.0000  0.0218\n",
            "     28        1.0000  0.0188\n",
            "     29        1.0000  0.0194\n",
            "     30        1.0000  0.0186\n",
            "     31        1.0000  0.0167\n",
            "     32        1.0000  0.0165\n",
            "     33        1.0000  0.0172\n",
            "     34        1.0000  0.0179\n",
            "     35        1.0000  0.0195\n",
            "     36        1.0000  0.0171\n",
            "     37        1.0000  0.0177\n",
            "     38        1.0000  0.0174\n",
            "     39        1.0000  0.0172\n",
            "     40        1.0000  0.0185\n",
            "     41        1.0000  0.0174\n",
            "     42        1.0000  0.0180\n",
            "     43        1.0000  0.0174\n",
            "     44        1.0000  0.0181\n",
            "     45        1.0000  0.0175\n",
            "     46        1.0000  0.0167\n",
            "     47        1.0000  0.0168\n",
            "     48        1.0000  0.0171\n",
            "     49        1.0000  0.0166\n",
            "     50        1.0000  0.0208\n",
            "     51        1.0000  0.0251\n",
            "     52        1.0000  0.0221\n",
            "     53        1.0000  0.0159\n",
            "     54        1.0000  0.0181\n",
            "     55        1.0000  0.0167\n",
            "     56        1.0000  0.0166\n",
            "     57        1.0000  0.0172\n",
            "     58        1.0000  0.0195\n",
            "     59        1.0000  0.0195\n",
            "     60        1.0000  0.0205\n",
            "     61        1.0000  0.0270\n",
            "     62        1.0000  0.0205\n",
            "     63        1.0000  0.0271\n",
            "     64        1.0000  0.0195\n",
            "     65        1.0000  0.0224\n",
            "     66        1.0000  0.0216\n",
            "     67        1.0000  0.0217\n",
            "     68        1.0000  0.0194\n",
            "     69        1.0000  0.0194\n",
            "     70        1.0000  0.0232\n",
            "     71        1.0000  0.0254\n",
            "     72        1.0000  0.0231\n",
            "     73        1.0000  0.0200\n",
            "     74        1.0000  0.0184\n",
            "     75        1.0000  0.0227\n",
            "     76        1.0000  0.0176\n",
            "     77        1.0000  0.0222\n",
            "     78        1.0000  0.0199\n",
            "     79        1.0000  0.0208\n",
            "     80        1.0000  0.0207\n",
            "     81        1.0000  0.0211\n",
            "     82        1.0000  0.0194\n",
            "     83        1.0000  0.0189\n",
            "     84        1.0000  0.0218\n",
            "     85        1.0000  0.0215\n",
            "     86        1.0000  0.0187\n",
            "     87        1.0000  0.0196\n",
            "     88        1.0000  0.0195\n",
            "     89        1.0000  0.0251\n",
            "     90        1.0000  0.0207\n",
            "     91        1.0000  0.0232\n",
            "     92        1.0000  0.0286\n",
            "     93        1.0000  0.0257\n",
            "     94        1.0000  0.0231\n",
            "     95        1.0000  0.0188\n",
            "     96        1.0000  0.0181\n",
            "     97        1.0000  0.0192\n",
            "     98        1.0000  0.0205\n",
            "     99        1.0000  0.0211\n",
            "    100        1.0000  0.0227\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0225\n",
            "      2        1.0000  0.0215\n",
            "      3        1.0000  0.0185\n",
            "      4        1.0000  0.0214\n",
            "      5        1.0000  0.0202\n",
            "      6        1.0000  0.0200\n",
            "      7        1.0000  0.0193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        1.0000  0.0243\n",
            "      9        1.0000  0.0184\n",
            "     10        1.0000  0.0196\n",
            "     11        1.0000  0.0249\n",
            "     12        1.0000  0.0197\n",
            "     13        1.0000  0.0205\n",
            "     14        1.0000  0.0222\n",
            "     15        1.0000  0.0186\n",
            "     16        1.0000  0.0178\n",
            "     17        1.0000  0.0207\n",
            "     18        1.0000  0.0191\n",
            "     19        1.0000  0.0198\n",
            "     20        1.0000  0.0251\n",
            "     21        1.0000  0.0191\n",
            "     22        1.0000  0.0176\n",
            "     23        1.0000  0.0181\n",
            "     24        1.0000  0.0177\n",
            "     25        1.0000  0.0170\n",
            "     26        1.0000  0.0173\n",
            "     27        1.0000  0.0164\n",
            "     28        1.0000  0.0205\n",
            "     29        1.0000  0.0177\n",
            "     30        1.0000  0.0191\n",
            "     31        1.0000  0.0172\n",
            "     32        1.0000  0.0199\n",
            "     33        1.0000  0.0221\n",
            "     34        1.0000  0.0215\n",
            "     35        1.0000  0.0196\n",
            "     36        1.0000  0.0197\n",
            "     37        1.0000  0.0186\n",
            "     38        1.0000  0.0192\n",
            "     39        1.0000  0.0197\n",
            "     40        1.0000  0.0220\n",
            "     41        1.0000  0.0190\n",
            "     42        1.0000  0.0205\n",
            "     43        1.0000  0.0244\n",
            "     44        1.0000  0.0247\n",
            "     45        1.0000  0.0240\n",
            "     46        1.0000  0.0203\n",
            "     47        1.0000  0.0190\n",
            "     48        1.0000  0.0199\n",
            "     49        1.0000  0.0220\n",
            "     50        1.0000  0.0223\n",
            "     51        1.0000  0.0282\n",
            "     52        1.0000  0.0250\n",
            "     53        1.0000  0.0226\n",
            "     54        1.0000  0.0234\n",
            "     55        1.0000  0.0226\n",
            "     56        1.0000  0.0198\n",
            "     57        1.0000  0.0202\n",
            "     58        1.0000  0.0185\n",
            "     59        1.0000  0.0210\n",
            "     60        1.0000  0.0217\n",
            "     61        1.0000  0.0204\n",
            "     62        1.0000  0.0193\n",
            "     63        1.0000  0.0182\n",
            "     64        1.0000  0.0201\n",
            "     65        1.0000  0.0205\n",
            "     66        1.0000  0.0194\n",
            "     67        1.0000  0.0197\n",
            "     68        1.0000  0.0188\n",
            "     69        1.0000  0.0198\n",
            "     70        1.0000  0.0193\n",
            "     71        1.0000  0.0193\n",
            "     72        1.0000  0.0280\n",
            "     73        1.0000  0.0204\n",
            "     74        1.0000  0.0216\n",
            "     75        1.0000  0.0202\n",
            "     76        1.0000  0.0208\n",
            "     77        1.0000  0.0213\n",
            "     78        1.0000  0.0232\n",
            "     79        1.0000  0.0217\n",
            "     80        1.0000  0.0230\n",
            "     81        1.0000  0.0211\n",
            "     82        1.0000  0.0213\n",
            "     83        1.0000  0.0211\n",
            "     84        1.0000  0.0193\n",
            "     85        1.0000  0.0205\n",
            "     86        1.0000  0.0210\n",
            "     87        1.0000  0.0220\n",
            "     88        1.0000  0.0171\n",
            "     89        1.0000  0.0180\n",
            "     90        1.0000  0.0251\n",
            "     91        1.0000  0.0183\n",
            "     92        1.0000  0.0238\n",
            "     93        1.0000  0.0226\n",
            "     94        1.0000  0.0234\n",
            "     95        1.0000  0.0235\n",
            "     96        1.0000  0.0176\n",
            "     97        1.0000  0.0177\n",
            "     98        1.0000  0.0187\n",
            "     99        1.0000  0.0182\n",
            "    100        1.0000  0.0197\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0229\n",
            "      2        1.0000  0.0243\n",
            "      3        1.0000  0.0224\n",
            "      4        1.0000  0.0227\n",
            "      5        1.0000  0.0232\n",
            "      6        1.0000  0.0282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        1.0000  0.0245\n",
            "      8        1.0000  0.0271\n",
            "      9        1.0000  0.0288\n",
            "     10        1.0000  0.0278\n",
            "     11        1.0000  0.0260\n",
            "     12        1.0000  0.0264\n",
            "     13        1.0000  0.0265\n",
            "     14        1.0000  0.0256\n",
            "     15        1.0000  0.0294\n",
            "     16        1.0000  0.0259\n",
            "     17        1.0000  0.0255\n",
            "     18        1.0000  0.0295\n",
            "     19        1.0000  0.0273\n",
            "     20        1.0000  0.0259\n",
            "     21        1.0000  0.0290\n",
            "     22        1.0000  0.0265\n",
            "     23        1.0000  0.0299\n",
            "     24        1.0000  0.0390\n",
            "     25        1.0000  0.0269\n",
            "     26        1.0000  0.0236\n",
            "     27        1.0000  0.0372\n",
            "     28        1.0000  0.0325\n",
            "     29        1.0000  0.0229\n",
            "     30        1.0000  0.0309\n",
            "     31        1.0000  0.0255\n",
            "     32        1.0000  0.0211\n",
            "     33        1.0000  0.0225\n",
            "     34        1.0000  0.0253\n",
            "     35        1.0000  0.0258\n",
            "     36        1.0000  0.0254\n",
            "     37        1.0000  0.0319\n",
            "     38        1.0000  0.0246\n",
            "     39        1.0000  0.0248\n",
            "     40        1.0000  0.0259\n",
            "     41        1.0000  0.0259\n",
            "     42        1.0000  0.0305\n",
            "     43        1.0000  0.0237\n",
            "     44        1.0000  0.0262\n",
            "     45        1.0000  0.0261\n",
            "     46        1.0000  0.0262\n",
            "     47        1.0000  0.0265\n",
            "     48        1.0000  0.0304\n",
            "     49        1.0000  0.0292\n",
            "     50        1.0000  0.0299\n",
            "     51        1.0000  0.0287\n",
            "     52        1.0000  0.0304\n",
            "     53        1.0000  0.0289\n",
            "     54        1.0000  0.0237\n",
            "     55        1.0000  0.0245\n",
            "     56        1.0000  0.0290\n",
            "     57        1.0000  0.0228\n",
            "     58        1.0000  0.0267\n",
            "     59        1.0000  0.0361\n",
            "     60        1.0000  0.0224\n",
            "     61        1.0000  0.0241\n",
            "     62        1.0000  0.0241\n",
            "     63        1.0000  0.0183\n",
            "     64        1.0000  0.0247\n",
            "     65        1.0000  0.0243\n",
            "     66        1.0000  0.0223\n",
            "     67        1.0000  0.0215\n",
            "     68        1.0000  0.0258\n",
            "     69        1.0000  0.0221\n",
            "     70        1.0000  0.0228\n",
            "     71        1.0000  0.0242\n",
            "     72        1.0000  0.0203\n",
            "     73        1.0000  0.0219\n",
            "     74        1.0000  0.0248\n",
            "     75        1.0000  0.0266\n",
            "     76        1.0000  0.0234\n",
            "     77        1.0000  0.0226\n",
            "     78        1.0000  0.0296\n",
            "     79        1.0000  0.0319\n",
            "     80        1.0000  0.0248\n",
            "     81        1.0000  0.0244\n",
            "     82        1.0000  0.0303\n",
            "     83        1.0000  0.0161\n",
            "     84        1.0000  0.0231\n",
            "     85        1.0000  0.0232\n",
            "     86        1.0000  0.0219\n",
            "     87        1.0000  0.0254\n",
            "     88        1.0000  0.0234\n",
            "     89        1.0000  0.0283\n",
            "     90        1.0000  0.0239\n",
            "     91        1.0000  0.0246\n",
            "     92        1.0000  0.0240\n",
            "     93        1.0000  0.0252\n",
            "     94        1.0000  0.0244\n",
            "     95        1.0000  0.0279\n",
            "     96        1.0000  0.0347\n",
            "     97        1.0000  0.0259\n",
            "     98        1.0000  0.0240\n",
            "     99        1.0000  0.0244\n",
            "    100        1.0000  0.0235\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0290\n",
            "      2        1.0000  0.0235\n",
            "      3        1.0000  0.0256\n",
            "      4        1.0000  0.0245\n",
            "      5        1.0000  0.0231\n",
            "      6        1.0000  0.0234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        1.0000  0.0241\n",
            "      8        1.0000  0.0255\n",
            "      9        1.0000  0.0301\n",
            "     10        1.0000  0.0271\n",
            "     11        1.0000  0.0241\n",
            "     12        1.0000  0.0225\n",
            "     13        1.0000  0.0283\n",
            "     14        1.0000  0.0292\n",
            "     15        1.0000  0.0225\n",
            "     16        1.0000  0.0214\n",
            "     17        1.0000  0.0260\n",
            "     18        1.0000  0.0305\n",
            "     19        1.0000  0.0187\n",
            "     20        1.0000  0.0298\n",
            "     21        1.0000  0.0224\n",
            "     22        1.0000  0.0226\n",
            "     23        1.0000  0.0247\n",
            "     24        1.0000  0.0231\n",
            "     25        1.0000  0.0249\n",
            "     26        1.0000  0.0228\n",
            "     27        1.0000  0.0232\n",
            "     28        1.0000  0.0242\n",
            "     29        1.0000  0.0240\n",
            "     30        1.0000  0.0323\n",
            "     31        1.0000  0.0247\n",
            "     32        1.0000  0.0294\n",
            "     33        1.0000  0.0234\n",
            "     34        1.0000  0.0237\n",
            "     35        1.0000  0.0250\n",
            "     36        1.0000  0.0161\n",
            "     37        1.0000  0.0256\n",
            "     38        1.0000  0.0260\n",
            "     39        1.0000  0.0301\n",
            "     40        1.0000  0.0249\n",
            "     41        1.0000  0.0253\n",
            "     42        1.0000  0.0295\n",
            "     43        1.0000  0.0330\n",
            "     44        1.0000  0.0225\n",
            "     45        1.0000  0.0255\n",
            "     46        1.0000  0.0296\n",
            "     47        1.0000  0.0318\n",
            "     48        1.0000  0.0411\n",
            "     49        1.0000  0.0302\n",
            "     50        1.0000  0.0256\n",
            "     51        1.0000  0.0213\n",
            "     52        1.0000  0.0157\n",
            "     53        1.0000  0.0251\n",
            "     54        1.0000  0.0270\n",
            "     55        1.0000  0.0266\n",
            "     56        1.0000  0.0233\n",
            "     57        1.0000  0.0176\n",
            "     58        1.0000  0.0281\n",
            "     59        1.0000  0.0262\n",
            "     60        1.0000  0.0283\n",
            "     61        1.0000  0.0194\n",
            "     62        1.0000  0.0210\n",
            "     63        1.0000  0.0226\n",
            "     64        1.0000  0.0266\n",
            "     65        1.0000  0.0362\n",
            "     66        1.0000  0.0245\n",
            "     67        1.0000  0.0218\n",
            "     68        1.0000  0.0272\n",
            "     69        1.0000  0.0273\n",
            "     70        1.0000  0.0217\n",
            "     71        1.0000  0.0271\n",
            "     72        1.0000  0.0284\n",
            "     73        1.0000  0.0276\n",
            "     74        1.0000  0.0218\n",
            "     75        1.0000  0.0268\n",
            "     76        1.0000  0.0413\n",
            "     77        1.0000  0.0324\n",
            "     78        1.0000  0.0307\n",
            "     79        1.0000  0.0253\n",
            "     80        1.0000  0.0195\n",
            "     81        1.0000  0.0258\n",
            "     82        1.0000  0.0281\n",
            "     83        1.0000  0.0257\n",
            "     84        1.0000  0.0236\n",
            "     85        1.0000  0.0166\n",
            "     86        1.0000  0.0249\n",
            "     87        1.0000  0.0205\n",
            "     88        1.0000  0.0284\n",
            "     89        1.0000  0.0252\n",
            "     90        1.0000  0.0271\n",
            "     91        1.0000  0.0169\n",
            "     92        1.0000  0.0225\n",
            "     93        1.0000  0.0262\n",
            "     94        1.0000  0.0258\n",
            "     95        1.0000  0.0244\n",
            "     96        1.0000  0.0217\n",
            "     97        1.0000  0.0174\n",
            "     98        1.0000  0.0238\n",
            "     99        1.0000  0.0312\n",
            "    100        1.0000  0.0279\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0184\n",
            "      2        1.0000  0.0147\n",
            "      3        1.0000  0.0204\n",
            "      4        1.0000  0.0223\n",
            "      5        1.0000  0.0208\n",
            "      6        1.0000  0.0190\n",
            "      7        1.0000  0.0217\n",
            "      8        1.0000  0.0210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        1.0000  0.0192\n",
            "     10        1.0000  0.0180\n",
            "     11        1.0000  0.0203\n",
            "     12        1.0000  0.0203\n",
            "     13        1.0000  0.0191\n",
            "     14        1.0000  0.0178\n",
            "     15        1.0000  0.0199\n",
            "     16        1.0000  0.0219\n",
            "     17        1.0000  0.0187\n",
            "     18        1.0000  0.0187\n",
            "     19        1.0000  0.0195\n",
            "     20        1.0000  0.0169\n",
            "     21        1.0000  0.0173\n",
            "     22        1.0000  0.0230\n",
            "     23        1.0000  0.0169\n",
            "     24        1.0000  0.0165\n",
            "     25        1.0000  0.0164\n",
            "     26        1.0000  0.0164\n",
            "     27        1.0000  0.0166\n",
            "     28        1.0000  0.0193\n",
            "     29        1.0000  0.0252\n",
            "     30        1.0000  0.0185\n",
            "     31        1.0000  0.0189\n",
            "     32        1.0000  0.0187\n",
            "     33        1.0000  0.0171\n",
            "     34        1.0000  0.0165\n",
            "     35        1.0000  0.0165\n",
            "     36        1.0000  0.0170\n",
            "     37        1.0000  0.0177\n",
            "     38        1.0000  0.0163\n",
            "     39        1.0000  0.0235\n",
            "     40        1.0000  0.0232\n",
            "     41        1.0000  0.0207\n",
            "     42        1.0000  0.0284\n",
            "     43        1.0000  0.0278\n",
            "     44        1.0000  0.0196\n",
            "     45        1.0000  0.0179\n",
            "     46        1.0000  0.0181\n",
            "     47        1.0000  0.0183\n",
            "     48        1.0000  0.0216\n",
            "     49        1.0000  0.0196\n",
            "     50        1.0000  0.0205\n",
            "     51        1.0000  0.0199\n",
            "     52        1.0000  0.0234\n",
            "     53        1.0000  0.0203\n",
            "     54        1.0000  0.0188\n",
            "     55        1.0000  0.0208\n",
            "     56        1.0000  0.0199\n",
            "     57        1.0000  0.0211\n",
            "     58        1.0000  0.0204\n",
            "     59        1.0000  0.0198\n",
            "     60        1.0000  0.0205\n",
            "     61        1.0000  0.0209\n",
            "     62        1.0000  0.0195\n",
            "     63        1.0000  0.0179\n",
            "     64        1.0000  0.0178\n",
            "     65        1.0000  0.0184\n",
            "     66        1.0000  0.0186\n",
            "     67        1.0000  0.0187\n",
            "     68        1.0000  0.0187\n",
            "     69        1.0000  0.0201\n",
            "     70        1.0000  0.0172\n",
            "     71        1.0000  0.0173\n",
            "     72        1.0000  0.0169\n",
            "     73        1.0000  0.0169\n",
            "     74        1.0000  0.0166\n",
            "     75        1.0000  0.0160\n",
            "     76        1.0000  0.0173\n",
            "     77        1.0000  0.0163\n",
            "     78        1.0000  0.0160\n",
            "     79        1.0000  0.0170\n",
            "     80        1.0000  0.0170\n",
            "     81        1.0000  0.0159\n",
            "     82        1.0000  0.0169\n",
            "     83        1.0000  0.0207\n",
            "     84        1.0000  0.0189\n",
            "     85        1.0000  0.0206\n",
            "     86        1.0000  0.0183\n",
            "     87        1.0000  0.0179\n",
            "     88        1.0000  0.0213\n",
            "     89        1.0000  0.0168\n",
            "     90        1.0000  0.0166\n",
            "     91        1.0000  0.0192\n",
            "     92        1.0000  0.0183\n",
            "     93        1.0000  0.0201\n",
            "     94        1.0000  0.0192\n",
            "     95        1.0000  0.0185\n",
            "     96        1.0000  0.0206\n",
            "     97        1.0000  0.0191\n",
            "     98        1.0000  0.0204\n",
            "     99        1.0000  0.0182\n",
            "    100        1.0000  0.0178\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0163\n",
            "      2        1.0000  0.0169\n",
            "      3        1.0000  0.0185\n",
            "      4        1.0000  0.0171\n",
            "      5        1.0000  0.0177\n",
            "      6        1.0000  0.0175\n",
            "      7        1.0000  0.0169\n",
            "      8        1.0000  0.0168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        1.0000  0.0186\n",
            "     10        1.0000  0.0199\n",
            "     11        1.0000  0.0157\n",
            "     12        1.0000  0.0200\n",
            "     13        1.0000  0.0167\n",
            "     14        1.0000  0.0167\n",
            "     15        1.0000  0.0176\n",
            "     16        1.0000  0.0166\n",
            "     17        1.0000  0.0165\n",
            "     18        1.0000  0.0165\n",
            "     19        1.0000  0.0158\n",
            "     20        1.0000  0.0168\n",
            "     21        1.0000  0.0175\n",
            "     22        1.0000  0.0163\n",
            "     23        1.0000  0.0186\n",
            "     24        1.0000  0.0173\n",
            "     25        1.0000  0.0192\n",
            "     26        1.0000  0.0192\n",
            "     27        1.0000  0.0267\n",
            "     28        1.0000  0.0196\n",
            "     29        1.0000  0.0191\n",
            "     30        1.0000  0.0191\n",
            "     31        1.0000  0.0186\n",
            "     32        1.0000  0.0197\n",
            "     33        1.0000  0.0212\n",
            "     34        1.0000  0.0197\n",
            "     35        1.0000  0.0242\n",
            "     36        1.0000  0.0205\n",
            "     37        1.0000  0.0204\n",
            "     38        1.0000  0.0200\n",
            "     39        1.0000  0.0188\n",
            "     40        1.0000  0.0183\n",
            "     41        1.0000  0.0228\n",
            "     42        1.0000  0.0217\n",
            "     43        1.0000  0.0195\n",
            "     44        1.0000  0.0260\n",
            "     45        1.0000  0.0173\n",
            "     46        1.0000  0.0169\n",
            "     47        1.0000  0.0171\n",
            "     48        1.0000  0.0170\n",
            "     49        1.0000  0.0178\n",
            "     50        1.0000  0.0171\n",
            "     51        1.0000  0.0177\n",
            "     52        1.0000  0.0175\n",
            "     53        1.0000  0.0173\n",
            "     54        1.0000  0.0165\n",
            "     55        1.0000  0.0178\n",
            "     56        1.0000  0.0185\n",
            "     57        1.0000  0.0168\n",
            "     58        1.0000  0.0174\n",
            "     59        1.0000  0.0183\n",
            "     60        1.0000  0.0167\n",
            "     61        1.0000  0.0171\n",
            "     62        1.0000  0.0176\n",
            "     63        1.0000  0.0176\n",
            "     64        1.0000  0.0185\n",
            "     65        1.0000  0.0213\n",
            "     66        1.0000  0.0171\n",
            "     67        1.0000  0.0184\n",
            "     68        1.0000  0.0178\n",
            "     69        1.0000  0.0175\n",
            "     70        1.0000  0.0204\n",
            "     71        1.0000  0.0232\n",
            "     72        1.0000  0.0310\n",
            "     73        1.0000  0.0208\n",
            "     74        1.0000  0.0216\n",
            "     75        1.0000  0.0188\n",
            "     76        1.0000  0.0226\n",
            "     77        1.0000  0.0212\n",
            "     78        1.0000  0.0209\n",
            "     79        1.0000  0.0199\n",
            "     80        1.0000  0.0189\n",
            "     81        1.0000  0.0188\n",
            "     82        1.0000  0.0177\n",
            "     83        1.0000  0.0182\n",
            "     84        1.0000  0.0197\n",
            "     85        1.0000  0.0173\n",
            "     86        1.0000  0.0180\n",
            "     87        1.0000  0.0216\n",
            "     88        1.0000  0.0185\n",
            "     89        1.0000  0.0221\n",
            "     90        1.0000  0.0205\n",
            "     91        1.0000  0.0201\n",
            "     92        1.0000  0.0185\n",
            "     93        1.0000  0.0174\n",
            "     94        1.0000  0.0184\n",
            "     95        1.0000  0.0211\n",
            "     96        1.0000  0.0235\n",
            "     97        1.0000  0.0274\n",
            "     98        1.0000  0.0173\n",
            "     99        1.0000  0.0193\n",
            "    100        1.0000  0.0190\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m1.0000\u001b[0m  0.0234\n",
            "      2        1.0000  0.0248\n",
            "      3        1.0000  0.0237\n",
            "      4        1.0000  0.0264\n",
            "      5        1.0000  0.0231\n",
            "      6        1.0000  0.0213\n",
            "      7        1.0000  0.0218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        1.0000  0.0240\n",
            "      9        1.0000  0.0215\n",
            "     10        1.0000  0.0229\n",
            "     11        1.0000  0.0277\n",
            "     12        1.0000  0.0355\n",
            "     13        1.0000  0.0264\n",
            "     14        1.0000  0.0254\n",
            "     15        \u001b[36m1.0000\u001b[0m  0.0250\n",
            "     16        \u001b[36m0.9965\u001b[0m  0.0258\n",
            "     17        0.9965  0.0250\n",
            "     18        0.9965  0.0253\n",
            "     19        0.9965  0.0243\n",
            "     20        0.9965  0.0240\n",
            "     21        0.9965  0.0232\n",
            "     22        0.9965  0.0224\n",
            "     23        0.9965  0.0217\n",
            "     24        0.9965  0.0226\n",
            "     25        0.9965  0.0230\n",
            "     26        0.9965  0.0220\n",
            "     27        0.9965  0.0284\n",
            "     28        0.9965  0.0221\n",
            "     29        0.9965  0.0219\n",
            "     30        0.9965  0.0240\n",
            "     31        0.9965  0.0215\n",
            "     32        0.9965  0.0216\n",
            "     33        0.9965  0.0214\n",
            "     34        0.9965  0.0207\n",
            "     35        0.9965  0.0204\n",
            "     36        0.9965  0.0200\n",
            "     37        0.9965  0.0213\n",
            "     38        0.9965  0.0249\n",
            "     39        0.9965  0.0231\n",
            "     40        0.9965  0.0234\n",
            "     41        0.9965  0.0244\n",
            "     42        0.9965  0.0224\n",
            "     43        0.9965  0.0245\n",
            "     44        0.9965  0.0254\n",
            "     45        0.9965  0.0226\n",
            "     46        0.9965  0.0242\n",
            "     47        0.9965  0.0250\n",
            "     48        0.9965  0.0267\n",
            "     49        0.9965  0.0388\n",
            "     50        0.9965  0.0312\n",
            "     51        \u001b[36m0.9965\u001b[0m  0.0294\n",
            "     52        \u001b[36m0.9858\u001b[0m  0.0313\n",
            "     53        \u001b[36m0.9716\u001b[0m  0.0289\n",
            "     54        \u001b[36m0.9555\u001b[0m  0.0291\n",
            "     55        \u001b[36m0.9535\u001b[0m  0.0247\n",
            "     56        \u001b[36m0.9533\u001b[0m  0.0250\n",
            "     57        \u001b[36m0.9531\u001b[0m  0.0321\n",
            "     58        \u001b[36m0.9530\u001b[0m  0.0250\n",
            "     59        \u001b[36m0.9529\u001b[0m  0.0293\n",
            "     60        \u001b[36m0.9527\u001b[0m  0.0234\n",
            "     61        \u001b[36m0.9526\u001b[0m  0.0220\n",
            "     62        \u001b[36m0.9525\u001b[0m  0.0228\n",
            "     63        \u001b[36m0.9524\u001b[0m  0.0240\n",
            "     64        \u001b[36m0.9523\u001b[0m  0.0233\n",
            "     65        \u001b[36m0.9522\u001b[0m  0.0218\n",
            "     66        \u001b[36m0.9521\u001b[0m  0.0219\n",
            "     67        \u001b[36m0.9521\u001b[0m  0.0208\n",
            "     68        \u001b[36m0.9520\u001b[0m  0.0220\n",
            "     69        \u001b[36m0.9519\u001b[0m  0.0233\n",
            "     70        \u001b[36m0.9518\u001b[0m  0.0232\n",
            "     71        \u001b[36m0.9517\u001b[0m  0.0220\n",
            "     72        \u001b[36m0.9517\u001b[0m  0.0213\n",
            "     73        \u001b[36m0.9516\u001b[0m  0.0238\n",
            "     74        \u001b[36m0.9515\u001b[0m  0.0222\n",
            "     75        \u001b[36m0.9514\u001b[0m  0.0209\n",
            "     76        \u001b[36m0.9514\u001b[0m  0.0222\n",
            "     77        \u001b[36m0.9513\u001b[0m  0.0231\n",
            "     78        \u001b[36m0.9512\u001b[0m  0.0210\n",
            "     79        \u001b[36m0.9512\u001b[0m  0.0233\n",
            "     80        \u001b[36m0.9511\u001b[0m  0.0252\n",
            "     81        \u001b[36m0.9510\u001b[0m  0.0271\n",
            "     82        \u001b[36m0.9510\u001b[0m  0.0253\n",
            "     83        \u001b[36m0.9509\u001b[0m  0.0258\n",
            "     84        \u001b[36m0.9455\u001b[0m  0.0264\n",
            "     85        \u001b[36m0.8946\u001b[0m  0.0293\n",
            "     86        \u001b[36m0.7954\u001b[0m  0.0398\n",
            "     87        \u001b[36m0.6152\u001b[0m  0.0267\n",
            "     88        \u001b[36m0.4995\u001b[0m  0.0263\n",
            "     89        \u001b[36m0.4516\u001b[0m  0.0269\n",
            "     90        \u001b[36m0.4161\u001b[0m  0.0244\n",
            "     91        \u001b[36m0.4101\u001b[0m  0.0257\n",
            "     92        \u001b[36m0.4064\u001b[0m  0.0218\n",
            "     93        \u001b[36m0.4063\u001b[0m  0.0218\n",
            "     94        \u001b[36m0.4063\u001b[0m  0.0212\n",
            "     95        \u001b[36m0.4063\u001b[0m  0.0224\n",
            "     96        \u001b[36m0.4063\u001b[0m  0.0240\n",
            "     97        \u001b[36m0.4062\u001b[0m  0.0226\n",
            "     98        \u001b[36m0.4062\u001b[0m  0.0223\n",
            "     99        \u001b[36m0.4062\u001b[0m  0.0297\n",
            "    100        \u001b[36m0.4062\u001b[0m  0.0224\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.3965\u001b[0m  0.0216\n",
            "      2        0.3965  0.0227\n",
            "      3        \u001b[36m0.3930\u001b[0m  0.0223\n",
            "      4        0.3930  0.0214\n",
            "      5        0.3930  0.0234\n",
            "      6        0.3930  0.0240\n",
            "      7        0.3930  0.0275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        0.3930  0.0269\n",
            "      9        0.3930  0.0246\n",
            "     10        0.3930  0.0258\n",
            "     11        0.3930  0.0234\n",
            "     12        0.3930  0.0260\n",
            "     13        0.3930  0.0238\n",
            "     14        0.3930  0.0255\n",
            "     15        0.3930  0.0269\n",
            "     16        0.3930  0.0308\n",
            "     17        0.3930  0.0247\n",
            "     18        \u001b[36m0.3895\u001b[0m  0.0241\n",
            "     19        0.3895  0.0242\n",
            "     20        0.3895  0.0237\n",
            "     21        0.3897  0.0352\n",
            "     22        0.3895  0.0261\n",
            "     23        0.3895  0.0214\n",
            "     24        0.3895  0.0205\n",
            "     25        0.3895  0.0205\n",
            "     26        0.3895  0.0203\n",
            "     27        0.3895  0.0211\n",
            "     28        0.3895  0.0213\n",
            "     29        0.3895  0.0207\n",
            "     30        0.3895  0.0318\n",
            "     31        0.3895  0.0219\n",
            "     32        0.3895  0.0219\n",
            "     33        0.3895  0.0207\n",
            "     34        0.3895  0.0214\n",
            "     35        0.3895  0.0221\n",
            "     36        0.3895  0.0209\n",
            "     37        0.3895  0.0208\n",
            "     38        0.3895  0.0209\n",
            "     39        0.3895  0.0220\n",
            "     40        0.3895  0.0244\n",
            "     41        0.3895  0.0246\n",
            "     42        0.3895  0.0269\n",
            "     43        0.3895  0.0215\n",
            "     44        0.3895  0.0216\n",
            "     45        0.3895  0.0230\n",
            "     46        0.3895  0.0228\n",
            "     47        0.3895  0.0215\n",
            "     48        0.3895  0.0237\n",
            "     49        0.3895  0.0251\n",
            "     50        0.3895  0.0255\n",
            "     51        0.3895  0.0226\n",
            "     52        0.3895  0.0223\n",
            "     53        0.3895  0.0238\n",
            "     54        0.3895  0.0266\n",
            "     55        0.3895  0.0236\n",
            "     56        0.3895  0.0243\n",
            "     57        0.3895  0.0232\n",
            "     58        0.3895  0.0249\n",
            "     59        0.3930  0.0254\n",
            "     60        \u001b[36m0.3860\u001b[0m  0.0312\n",
            "     61        \u001b[36m0.3789\u001b[0m  0.0293\n",
            "     62        0.3789  0.0221\n",
            "     63        0.3789  0.0214\n",
            "     64        0.3789  0.0255\n",
            "     65        0.3789  0.0231\n",
            "     66        0.3789  0.0224\n",
            "     67        0.3789  0.0212\n",
            "     68        0.3789  0.0211\n",
            "     69        0.3789  0.0211\n",
            "     70        0.3789  0.0212\n",
            "     71        0.3789  0.0208\n",
            "     72        0.3789  0.0214\n",
            "     73        0.3789  0.0214\n",
            "     74        0.3789  0.0220\n",
            "     75        \u001b[36m0.3754\u001b[0m  0.0211\n",
            "     76        0.3754  0.0207\n",
            "     77        0.3754  0.0209\n",
            "     78        0.3754  0.0206\n",
            "     79        0.3754  0.0257\n",
            "     80        0.3754  0.0224\n",
            "     81        \u001b[36m0.3754\u001b[0m  0.0218\n",
            "     82        \u001b[36m0.3754\u001b[0m  0.0209\n",
            "     83        \u001b[36m0.3752\u001b[0m  0.0232\n",
            "     84        \u001b[36m0.3736\u001b[0m  0.0212\n",
            "     85        \u001b[36m0.3736\u001b[0m  0.0223\n",
            "     86        \u001b[36m0.3735\u001b[0m  0.0270\n",
            "     87        \u001b[36m0.3735\u001b[0m  0.0249\n",
            "     88        0.3736  0.0235\n",
            "     89        \u001b[36m0.3735\u001b[0m  0.0257\n",
            "     90        \u001b[36m0.3734\u001b[0m  0.0260\n",
            "     91        \u001b[36m0.3734\u001b[0m  0.0244\n",
            "     92        \u001b[36m0.3734\u001b[0m  0.0242\n",
            "     93        \u001b[36m0.3734\u001b[0m  0.0271\n",
            "     94        \u001b[36m0.3734\u001b[0m  0.0257\n",
            "     95        \u001b[36m0.3733\u001b[0m  0.0257\n",
            "     96        \u001b[36m0.3733\u001b[0m  0.0265\n",
            "     97        \u001b[36m0.3733\u001b[0m  0.0254\n",
            "     98        \u001b[36m0.3733\u001b[0m  0.0275\n",
            "     99        \u001b[36m0.3733\u001b[0m  0.0253\n",
            "    100        \u001b[36m0.3733\u001b[0m  0.0216\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4296\u001b[0m  0.0170\n",
            "      2        0.4296  0.0189\n",
            "      3        0.4296  0.0172\n",
            "      4        0.4296  0.0172\n",
            "      5        0.4296  0.0171\n",
            "      6        0.4296  0.0173\n",
            "      7        0.4296  0.0166\n",
            "      8        0.4296  0.0222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        0.4296  0.0172\n",
            "     10        0.4296  0.0186\n",
            "     11        0.4296  0.0188\n",
            "     12        0.4296  0.0212\n",
            "     13        0.4296  0.0186\n",
            "     14        0.4296  0.0200\n",
            "     15        0.4296  0.0207\n",
            "     16        0.4296  0.0222\n",
            "     17        0.4296  0.0180\n",
            "     18        0.4296  0.0177\n",
            "     19        0.4296  0.0172\n",
            "     20        0.4296  0.0170\n",
            "     21        0.4296  0.0174\n",
            "     22        0.4296  0.0178\n",
            "     23        0.4296  0.0190\n",
            "     24        0.4296  0.0168\n",
            "     25        0.4296  0.0170\n",
            "     26        0.4296  0.0169\n",
            "     27        0.4296  0.0174\n",
            "     28        0.4296  0.0206\n",
            "     29        0.4296  0.0186\n",
            "     30        0.4296  0.0193\n",
            "     31        0.4296  0.0192\n",
            "     32        0.4296  0.0196\n",
            "     33        0.4296  0.0194\n",
            "     34        0.4296  0.0207\n",
            "     35        0.4296  0.0191\n",
            "     36        0.4296  0.0191\n",
            "     37        0.4296  0.0199\n",
            "     38        0.4296  0.0187\n",
            "     39        0.4296  0.0198\n",
            "     40        0.4296  0.0191\n",
            "     41        0.4296  0.0199\n",
            "     42        0.4296  0.0292\n",
            "     43        0.4296  0.0213\n",
            "     44        0.4296  0.0178\n",
            "     45        0.4296  0.0178\n",
            "     46        0.4296  0.0262\n",
            "     47        0.4296  0.0184\n",
            "     48        0.4296  0.0242\n",
            "     49        0.4296  0.0218\n",
            "     50        0.4296  0.0209\n",
            "     51        0.4296  0.0173\n",
            "     52        0.4296  0.0172\n",
            "     53        0.4296  0.0184\n",
            "     54        0.4296  0.0199\n",
            "     55        0.4296  0.0197\n",
            "     56        0.4296  0.0186\n",
            "     57        0.4296  0.0176\n",
            "     58        0.4296  0.0233\n",
            "     59        0.4296  0.0173\n",
            "     60        0.4296  0.0162\n",
            "     61        0.4296  0.0166\n",
            "     62        0.4296  0.0169\n",
            "     63        0.4296  0.0229\n",
            "     64        0.4296  0.0187\n",
            "     65        0.4296  0.0183\n",
            "     66        0.4296  0.0182\n",
            "     67        0.4296  0.0212\n",
            "     68        0.4296  0.0203\n",
            "     69        0.4296  0.0186\n",
            "     70        0.4296  0.0182\n",
            "     71        0.4296  0.0204\n",
            "     72        0.4296  0.0218\n",
            "     73        0.4296  0.0186\n",
            "     74        0.4296  0.0191\n",
            "     75        0.4296  0.0207\n",
            "     76        0.4296  0.0176\n",
            "     77        0.4296  0.0185\n",
            "     78        0.4296  0.0181\n",
            "     79        0.4296  0.0197\n",
            "     80        0.4296  0.0203\n",
            "     81        0.4296  0.0199\n",
            "     82        0.4296  0.0182\n",
            "     83        0.4296  0.0242\n",
            "     84        0.4296  0.0179\n",
            "     85        0.4296  0.0165\n",
            "     86        0.4296  0.0166\n",
            "     87        0.4296  0.0168\n",
            "     88        0.4296  0.0173\n",
            "     89        0.4296  0.0183\n",
            "     90        0.4296  0.0197\n",
            "     91        0.4296  0.0188\n",
            "     92        0.4296  0.0205\n",
            "     93        0.4296  0.0209\n",
            "     94        0.4296  0.0173\n",
            "     95        0.4296  0.0169\n",
            "     96        0.4296  0.0159\n",
            "     97        0.4296  0.0161\n",
            "     98        0.4296  0.0178\n",
            "     99        0.4296  0.0181\n",
            "    100        0.4296  0.0208\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.3895\u001b[0m  0.0173\n",
            "      2        0.3895  0.0166\n",
            "      3        0.3895  0.0160\n",
            "      4        0.3895  0.0169\n",
            "      5        0.3895  0.0180\n",
            "      6        0.3895  0.0185\n",
            "      7        0.3895  0.0178\n",
            "      8        0.3895  0.0176\n",
            "      9        0.3895  0.0176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        0.3895  0.0172\n",
            "     11        0.3895  0.0175\n",
            "     12        0.3895  0.0185\n",
            "     13        0.3895  0.0193\n",
            "     14        0.3895  0.0193\n",
            "     15        0.3895  0.0203\n",
            "     16        0.3895  0.0210\n",
            "     17        0.3895  0.0202\n",
            "     18        0.3895  0.0198\n",
            "     19        0.3895  0.0225\n",
            "     20        0.3895  0.0193\n",
            "     21        0.3895  0.0195\n",
            "     22        0.3895  0.0168\n",
            "     23        0.3895  0.0168\n",
            "     24        0.3895  0.0161\n",
            "     25        0.3895  0.0165\n",
            "     26        0.3895  0.0163\n",
            "     27        0.3895  0.0162\n",
            "     28        0.3895  0.0162\n",
            "     29        0.3895  0.0181\n",
            "     30        0.3895  0.0168\n",
            "     31        0.3895  0.0248\n",
            "     32        0.3895  0.0171\n",
            "     33        0.3895  0.0169\n",
            "     34        0.3895  0.0165\n",
            "     35        0.3895  0.0229\n",
            "     36        0.3895  0.0167\n",
            "     37        0.3895  0.0165\n",
            "     38        0.3895  0.0169\n",
            "     39        0.3895  0.0160\n",
            "     40        0.3895  0.0160\n",
            "     41        0.3895  0.0169\n",
            "     42        0.3895  0.0161\n",
            "     43        0.3895  0.0166\n",
            "     44        0.3895  0.0158\n",
            "     45        0.3895  0.0205\n",
            "     46        0.3895  0.0169\n",
            "     47        0.3895  0.0163\n",
            "     48        0.3895  0.0163\n",
            "     49        0.3895  0.0167\n",
            "     50        0.3895  0.0167\n",
            "     51        0.3895  0.0169\n",
            "     52        0.3895  0.0174\n",
            "     53        0.3895  0.0167\n",
            "     54        0.3895  0.0175\n",
            "     55        0.3895  0.0181\n",
            "     56        0.3895  0.0181\n",
            "     57        0.3895  0.0216\n",
            "     58        0.3895  0.0198\n",
            "     59        0.3895  0.0185\n",
            "     60        0.3895  0.0199\n",
            "     61        0.3895  0.0201\n",
            "     62        0.3895  0.0193\n",
            "     63        0.3895  0.0195\n",
            "     64        0.3895  0.0185\n",
            "     65        0.3895  0.0186\n",
            "     66        0.3895  0.0176\n",
            "     67        0.3895  0.0171\n",
            "     68        0.3895  0.0164\n",
            "     69        0.3895  0.0162\n",
            "     70        0.3895  0.0163\n",
            "     71        0.3895  0.0164\n",
            "     72        0.3895  0.0157\n",
            "     73        0.3895  0.0161\n",
            "     74        0.3895  0.0165\n",
            "     75        0.3895  0.0163\n",
            "     76        0.3895  0.0167\n",
            "     77        0.3895  0.0244\n",
            "     78        0.3895  0.0203\n",
            "     79        0.3895  0.0226\n",
            "     80        0.3895  0.0187\n",
            "     81        0.3895  0.0166\n",
            "     82        0.3895  0.0164\n",
            "     83        0.3895  0.0157\n",
            "     84        0.3895  0.0164\n",
            "     85        0.3895  0.0161\n",
            "     86        0.3895  0.0164\n",
            "     87        0.3895  0.0158\n",
            "     88        0.3895  0.0162\n",
            "     89        0.3895  0.0166\n",
            "     90        0.3895  0.0159\n",
            "     91        0.3895  0.0168\n",
            "     92        0.3895  0.0175\n",
            "     93        0.3895  0.0162\n",
            "     94        0.3895  0.0172\n",
            "     95        0.3895  0.0175\n",
            "     96        0.3895  0.0192\n",
            "     97        0.3895  0.0192\n",
            "     98        0.3895  0.0189\n",
            "     99        0.3895  0.0181\n",
            "    100        0.3895  0.0197\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.3732\u001b[0m  0.0251\n",
            "      2        0.3732  0.0267\n",
            "      3        0.3768  0.0270\n",
            "      4        0.3768  0.0246\n",
            "      5        0.3768  0.0247\n",
            "      6        0.3768  0.0239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        0.3768  0.0264\n",
            "      8        0.3768  0.0227\n",
            "      9        0.3768  0.0232\n",
            "     10        0.3768  0.0212\n",
            "     11        0.3768  0.0202\n",
            "     12        0.3768  0.0206\n",
            "     13        0.3768  0.0208\n",
            "     14        0.3768  0.0237\n",
            "     15        0.3768  0.0203\n",
            "     16        0.3768  0.0215\n",
            "     17        0.3768  0.0233\n",
            "     18        0.3768  0.0209\n",
            "     19        0.3768  0.0212\n",
            "     20        0.3768  0.0255\n",
            "     21        0.3768  0.0270\n",
            "     22        0.3768  0.0298\n",
            "     23        0.3768  0.0258\n",
            "     24        0.3768  0.0243\n",
            "     25        0.3768  0.0250\n",
            "     26        0.3768  0.0230\n",
            "     27        0.3768  0.0246\n",
            "     28        0.3768  0.0210\n",
            "     29        0.3768  0.0223\n",
            "     30        0.3768  0.0225\n",
            "     31        0.3768  0.0235\n",
            "     32        0.3768  0.0218\n",
            "     33        0.3768  0.0242\n",
            "     34        0.3768  0.0226\n",
            "     35        0.3768  0.0240\n",
            "     36        0.3768  0.0236\n",
            "     37        0.3768  0.0235\n",
            "     38        0.3768  0.0238\n",
            "     39        0.3768  0.0250\n",
            "     40        0.3768  0.0228\n",
            "     41        0.3768  0.0255\n",
            "     42        0.3768  0.0241\n",
            "     43        0.3768  0.0223\n",
            "     44        0.3768  0.0235\n",
            "     45        0.3768  0.0214\n",
            "     46        0.3768  0.0216\n",
            "     47        0.3768  0.0202\n",
            "     48        0.3768  0.0224\n",
            "     49        0.3768  0.0219\n",
            "     50        0.3768  0.0218\n",
            "     51        0.3768  0.0217\n",
            "     52        0.3768  0.0231\n",
            "     53        0.3732  0.0225\n",
            "     54        0.3732  0.0217\n",
            "     55        0.3732  0.0211\n",
            "     56        0.3732  0.0205\n",
            "     57        0.3732  0.0213\n",
            "     58        0.3732  0.0207\n",
            "     59        0.3732  0.0221\n",
            "     60        0.3732  0.0228\n",
            "     61        0.3732  0.0212\n",
            "     62        0.3732  0.0199\n",
            "     63        0.3732  0.0224\n",
            "     64        0.3732  0.0245\n",
            "     65        0.3732  0.0209\n",
            "     66        0.3732  0.0230\n",
            "     67        0.3732  0.0219\n",
            "     68        0.3732  0.0218\n",
            "     69        0.3733  0.0214\n",
            "     70        0.3732  0.0252\n",
            "     71        0.3732  0.0262\n",
            "     72        0.3732  0.0240\n",
            "     73        0.3732  0.0266\n",
            "     74        0.3732  0.0266\n",
            "     75        0.3732  0.0244\n",
            "     76        0.3732  0.0272\n",
            "     77        0.3732  0.0246\n",
            "     78        0.3732  0.0276\n",
            "     79        0.3732  0.0256\n",
            "     80        0.3732  0.0242\n",
            "     81        0.3732  0.0250\n",
            "     82        0.3732  0.0240\n",
            "     83        0.3732  0.0251\n",
            "     84        0.3732  0.0208\n",
            "     85        0.3732  0.0228\n",
            "     86        0.3732  0.0221\n",
            "     87        0.3732  0.0219\n",
            "     88        0.3732  0.0225\n",
            "     89        0.3732  0.0231\n",
            "     90        0.3732  0.0317\n",
            "     91        0.3732  0.0215\n",
            "     92        0.3732  0.0220\n",
            "     93        0.3732  0.0219\n",
            "     94        0.3732  0.0224\n",
            "     95        0.3732  0.0206\n",
            "     96        0.3732  0.0212\n",
            "     97        0.3732  0.0230\n",
            "     98        0.3732  0.0283\n",
            "     99        0.3732  0.0245\n",
            "    100        0.3732  0.0281\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4561\u001b[0m  0.0231\n",
            "      2        \u001b[36m0.4456\u001b[0m  0.0234\n",
            "      3        0.4456  0.0314\n",
            "      4        0.4456  0.0240\n",
            "      5        0.4456  0.0240\n",
            "      6        \u001b[36m0.4421\u001b[0m  0.0232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        0.4421  0.0287\n",
            "      8        \u001b[36m0.4386\u001b[0m  0.0283\n",
            "      9        \u001b[36m0.4351\u001b[0m  0.0289\n",
            "     10        0.4351  0.0275\n",
            "     11        \u001b[36m0.4281\u001b[0m  0.0290\n",
            "     12        0.4281  0.0280\n",
            "     13        0.4281  0.0254\n",
            "     14        \u001b[36m0.4246\u001b[0m  0.0246\n",
            "     15        0.4246  0.0255\n",
            "     16        0.4246  0.0213\n",
            "     17        0.4246  0.0229\n",
            "     18        0.4246  0.0282\n",
            "     19        \u001b[36m0.4211\u001b[0m  0.0294\n",
            "     20        0.4211  0.0320\n",
            "     21        0.4211  0.0279\n",
            "     22        \u001b[36m0.4140\u001b[0m  0.0227\n",
            "     23        \u001b[36m0.3965\u001b[0m  0.0265\n",
            "     24        \u001b[36m0.3860\u001b[0m  0.0234\n",
            "     25        0.3860  0.0223\n",
            "     26        0.3860  0.0211\n",
            "     27        0.3860  0.0225\n",
            "     28        0.3860  0.0283\n",
            "     29        0.3860  0.0344\n",
            "     30        0.3860  0.0300\n",
            "     31        0.3860  0.0269\n",
            "     32        0.3860  0.0240\n",
            "     33        0.3860  0.0227\n",
            "     34        0.3860  0.0268\n",
            "     35        0.3860  0.0288\n",
            "     36        0.3860  0.0294\n",
            "     37        0.3860  0.0281\n",
            "     38        0.3860  0.0257\n",
            "     39        \u001b[36m0.3827\u001b[0m  0.0257\n",
            "     40        \u001b[36m0.3825\u001b[0m  0.0270\n",
            "     41        0.3825  0.0281\n",
            "     42        0.3825  0.0255\n",
            "     43        0.3825  0.0254\n",
            "     44        0.3825  0.0246\n",
            "     45        0.3825  0.0256\n",
            "     46        0.3825  0.0250\n",
            "     47        0.3825  0.0271\n",
            "     48        0.3825  0.0240\n",
            "     49        0.3825  0.0239\n",
            "     50        0.3825  0.0219\n",
            "     51        0.3825  0.0222\n",
            "     52        0.3825  0.0222\n",
            "     53        0.3825  0.0246\n",
            "     54        0.3825  0.0230\n",
            "     55        0.3825  0.0243\n",
            "     56        0.3825  0.0237\n",
            "     57        0.3825  0.0225\n",
            "     58        0.3825  0.0236\n",
            "     59        0.3825  0.0222\n",
            "     60        \u001b[36m0.3789\u001b[0m  0.0230\n",
            "     61        0.3789  0.0249\n",
            "     62        0.3789  0.0234\n",
            "     63        0.3789  0.0266\n",
            "     64        0.3789  0.0298\n",
            "     65        0.3790  0.0267\n",
            "     66        0.3789  0.0286\n",
            "     67        0.3789  0.0268\n",
            "     68        0.3789  0.0273\n",
            "     69        0.3789  0.0283\n",
            "     70        0.3789  0.0275\n",
            "     71        0.3789  0.0275\n",
            "     72        0.3789  0.0263\n",
            "     73        0.3789  0.0272\n",
            "     74        0.3789  0.0263\n",
            "     75        0.3789  0.0282\n",
            "     76        0.3789  0.0309\n",
            "     77        0.3789  0.0280\n",
            "     78        0.3789  0.0255\n",
            "     79        0.3789  0.0293\n",
            "     80        0.3789  0.0255\n",
            "     81        0.3789  0.0242\n",
            "     82        0.3789  0.0226\n",
            "     83        0.3789  0.0249\n",
            "     84        0.3789  0.0236\n",
            "     85        0.3789  0.0226\n",
            "     86        0.3789  0.0222\n",
            "     87        0.3789  0.0230\n",
            "     88        0.3789  0.0313\n",
            "     89        0.3789  0.0227\n",
            "     90        0.3789  0.0240\n",
            "     91        0.3789  0.0225\n",
            "     92        0.3789  0.0225\n",
            "     93        0.3789  0.0235\n",
            "     94        0.3789  0.0242\n",
            "     95        0.3789  0.0228\n",
            "     96        0.3789  0.0224\n",
            "     97        0.3789  0.0276\n",
            "     98        0.3789  0.0223\n",
            "     99        0.3789  0.0283\n",
            "    100        0.3789  0.0249\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.4120\u001b[0m  0.0184\n",
            "      2        0.4120  0.0176\n",
            "      3        0.4120  0.0198\n",
            "      4        0.4120  0.0195\n",
            "      5        0.4120  0.0211\n",
            "      6        0.4120  0.0213\n",
            "      7        0.4120  0.0203\n",
            "      8        0.4120  0.0213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        0.4120  0.0219\n",
            "     10        0.4120  0.0218\n",
            "     11        0.4120  0.0233\n",
            "     12        0.4120  0.0220\n",
            "     13        0.4120  0.0225\n",
            "     14        0.4120  0.0198\n",
            "     15        0.4120  0.0172\n",
            "     16        0.4120  0.0180\n",
            "     17        0.4120  0.0171\n",
            "     18        0.4120  0.0183\n",
            "     19        0.4120  0.0191\n",
            "     20        0.4120  0.0175\n",
            "     21        0.4120  0.0173\n",
            "     22        0.4120  0.0189\n",
            "     23        0.4120  0.0177\n",
            "     24        0.4120  0.0170\n",
            "     25        0.4120  0.0171\n",
            "     26        0.4120  0.0168\n",
            "     27        0.4120  0.0200\n",
            "     28        0.4120  0.0173\n",
            "     29        0.4120  0.0187\n",
            "     30        0.4120  0.0172\n",
            "     31        0.4120  0.0173\n",
            "     32        0.4120  0.0169\n",
            "     33        0.4120  0.0176\n",
            "     34        0.4120  0.0175\n",
            "     35        0.4120  0.0189\n",
            "     36        0.4120  0.0182\n",
            "     37        0.4120  0.0185\n",
            "     38        0.4120  0.0225\n",
            "     39        0.4120  0.0201\n",
            "     40        0.4120  0.0234\n",
            "     41        0.4120  0.0214\n",
            "     42        0.4120  0.0203\n",
            "     43        0.4120  0.0221\n",
            "     44        0.4120  0.0223\n",
            "     45        0.4120  0.0209\n",
            "     46        0.4120  0.0198\n",
            "     47        0.4120  0.0224\n",
            "     48        0.4120  0.0237\n",
            "     49        0.4120  0.0208\n",
            "     50        0.4120  0.0222\n",
            "     51        0.4120  0.0230\n",
            "     52        0.4120  0.0204\n",
            "     53        0.4120  0.0211\n",
            "     54        0.4120  0.0213\n",
            "     55        0.4120  0.0211\n",
            "     56        0.4120  0.0205\n",
            "     57        0.4120  0.0185\n",
            "     58        0.4120  0.0211\n",
            "     59        0.4120  0.0215\n",
            "     60        0.4120  0.0195\n",
            "     61        0.4120  0.0207\n",
            "     62        0.4120  0.0213\n",
            "     63        0.4120  0.0200\n",
            "     64        0.4120  0.0188\n",
            "     65        0.4120  0.0186\n",
            "     66        0.4120  0.0173\n",
            "     67        0.4120  0.0176\n",
            "     68        0.4120  0.0173\n",
            "     69        0.4120  0.0183\n",
            "     70        0.4120  0.0196\n",
            "     71        0.4120  0.0225\n",
            "     72        0.4120  0.0190\n",
            "     73        0.4120  0.0185\n",
            "     74        0.4120  0.0177\n",
            "     75        0.4120  0.0181\n",
            "     76        0.4120  0.0192\n",
            "     77        0.4120  0.0178\n",
            "     78        0.4120  0.0225\n",
            "     79        0.4120  0.0172\n",
            "     80        0.4120  0.0203\n",
            "     81        0.4120  0.0184\n",
            "     82        0.4120  0.0182\n",
            "     83        0.4120  0.0177\n",
            "     84        0.4120  0.0193\n",
            "     85        0.4120  0.0209\n",
            "     86        0.4120  0.0215\n",
            "     87        0.4120  0.0195\n",
            "     88        0.4120  0.0200\n",
            "     89        0.4120  0.0237\n",
            "     90        0.4120  0.0266\n",
            "     91        0.4120  0.0192\n",
            "     92        0.4120  0.0331\n",
            "     93        0.4120  0.0209\n",
            "     94        0.4120  0.0223\n",
            "     95        0.4120  0.0192\n",
            "     96        0.4120  0.0198\n",
            "     97        0.4120  0.0179\n",
            "     98        0.4120  0.0181\n",
            "     99        0.4120  0.0181\n",
            "    100        0.4120  0.0197\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9860\u001b[0m  0.0158\n",
            "      2        0.9860  0.0167\n",
            "      3        0.9860  0.0173\n",
            "      4        0.9860  0.0172\n",
            "      5        0.9860  0.0171\n",
            "      6        0.9860  0.0191\n",
            "      7        0.9860  0.0177\n",
            "      8        0.9860  0.0172\n",
            "      9        0.9860  0.0233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        0.9860  0.0239\n",
            "     11        0.9860  0.0178\n",
            "     12        0.9860  0.0181\n",
            "     13        0.9860  0.0162\n",
            "     14        0.9860  0.0175\n",
            "     15        0.9860  0.0170\n",
            "     16        0.9860  0.0164\n",
            "     17        0.9860  0.0174\n",
            "     18        0.9860  0.0170\n",
            "     19        0.9860  0.0166\n",
            "     20        0.9860  0.0196\n",
            "     21        0.9860  0.0205\n",
            "     22        0.9860  0.0162\n",
            "     23        0.9860  0.0184\n",
            "     24        0.9860  0.0199\n",
            "     25        0.9860  0.0196\n",
            "     26        0.9860  0.0178\n",
            "     27        0.9860  0.0180\n",
            "     28        0.9860  0.0184\n",
            "     29        0.9860  0.0252\n",
            "     30        0.9860  0.0268\n",
            "     31        0.9860  0.0210\n",
            "     32        0.9860  0.0209\n",
            "     33        0.9860  0.0193\n",
            "     34        0.9860  0.0184\n",
            "     35        0.9860  0.0187\n",
            "     36        0.9860  0.0172\n",
            "     37        0.9860  0.0168\n",
            "     38        0.9860  0.0173\n",
            "     39        0.9860  0.0155\n",
            "     40        0.9860  0.0166\n",
            "     41        0.9860  0.0171\n",
            "     42        0.9860  0.0164\n",
            "     43        0.9860  0.0170\n",
            "     44        0.9860  0.0174\n",
            "     45        0.9860  0.0159\n",
            "     46        0.9860  0.0201\n",
            "     47        0.9860  0.0180\n",
            "     48        0.9860  0.0179\n",
            "     49        0.9860  0.0174\n",
            "     50        0.9860  0.0168\n",
            "     51        0.9860  0.0167\n",
            "     52        0.9860  0.0161\n",
            "     53        0.9860  0.0167\n",
            "     54        0.9860  0.0165\n",
            "     55        0.9860  0.0171\n",
            "     56        0.9860  0.0159\n",
            "     57        0.9860  0.0175\n",
            "     58        0.9860  0.0194\n",
            "     59        0.9860  0.0175\n",
            "     60        0.9860  0.0193\n",
            "     61        0.9860  0.0172\n",
            "     62        0.9860  0.0164\n",
            "     63        0.9860  0.0167\n",
            "     64        0.9860  0.0188\n",
            "     65        0.9860  0.0203\n",
            "     66        0.9860  0.0192\n",
            "     67        0.9860  0.0179\n",
            "     68        0.9860  0.0195\n",
            "     69        0.9860  0.0176\n",
            "     70        0.9860  0.0185\n",
            "     71        0.9860  0.0266\n",
            "     72        0.9860  0.0258\n",
            "     73        0.9860  0.0201\n",
            "     74        0.9860  0.0205\n",
            "     75        0.9860  0.0313\n",
            "     76        0.9860  0.0256\n",
            "     77        0.9860  0.0229\n",
            "     78        0.9860  0.0246\n",
            "     79        0.9860  0.0226\n",
            "     80        0.9860  0.0187\n",
            "     81        0.9860  0.0214\n",
            "     82        0.9860  0.0164\n",
            "     83        0.9860  0.0196\n",
            "     84        0.9860  0.0238\n",
            "     85        0.9860  0.0173\n",
            "     86        0.9860  0.0175\n",
            "     87        0.9860  0.0186\n",
            "     88        0.9860  0.0186\n",
            "     89        0.9860  0.0174\n",
            "     90        0.9860  0.0170\n",
            "     91        0.9860  0.0203\n",
            "     92        0.9860  0.0178\n",
            "     93        0.9860  0.0167\n",
            "     94        0.9860  0.0173\n",
            "     95        0.9860  0.0168\n",
            "     96        0.9860  0.0174\n",
            "     97        0.9860  0.0169\n",
            "     98        0.9860  0.0170\n",
            "     99        0.9860  0.0278\n",
            "    100        0.9860  0.0167\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9708\u001b[0m  0.0221\n",
            "      2        \u001b[36m0.9685\u001b[0m  0.0215\n",
            "      3        \u001b[36m0.9661\u001b[0m  0.0234\n",
            "      4        \u001b[36m0.9634\u001b[0m  0.0274\n",
            "      5        \u001b[36m0.9605\u001b[0m  0.0235\n",
            "      6        \u001b[36m0.9572\u001b[0m  0.0256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.9535\u001b[0m  0.0244\n",
            "      8        \u001b[36m0.9493\u001b[0m  0.0263\n",
            "      9        \u001b[36m0.9446\u001b[0m  0.0240\n",
            "     10        \u001b[36m0.9392\u001b[0m  0.0240\n",
            "     11        \u001b[36m0.9330\u001b[0m  0.0329\n",
            "     12        \u001b[36m0.9258\u001b[0m  0.0358\n",
            "     13        \u001b[36m0.9173\u001b[0m  0.0224\n",
            "     14        \u001b[36m0.9070\u001b[0m  0.0206\n",
            "     15        \u001b[36m0.8944\u001b[0m  0.0212\n",
            "     16        \u001b[36m0.8786\u001b[0m  0.0252\n",
            "     17        \u001b[36m0.8584\u001b[0m  0.0219\n",
            "     18        \u001b[36m0.8323\u001b[0m  0.0209\n",
            "     19        \u001b[36m0.7987\u001b[0m  0.0209\n",
            "     20        \u001b[36m0.7567\u001b[0m  0.0208\n",
            "     21        \u001b[36m0.7075\u001b[0m  0.0219\n",
            "     22        \u001b[36m0.6556\u001b[0m  0.0220\n",
            "     23        \u001b[36m0.6066\u001b[0m  0.0216\n",
            "     24        \u001b[36m0.5651\u001b[0m  0.0225\n",
            "     25        \u001b[36m0.5321\u001b[0m  0.0220\n",
            "     26        \u001b[36m0.5067\u001b[0m  0.0212\n",
            "     27        \u001b[36m0.4871\u001b[0m  0.0283\n",
            "     28        \u001b[36m0.4718\u001b[0m  0.0213\n",
            "     29        \u001b[36m0.4599\u001b[0m  0.0213\n",
            "     30        \u001b[36m0.4505\u001b[0m  0.0216\n",
            "     31        \u001b[36m0.4430\u001b[0m  0.0216\n",
            "     32        \u001b[36m0.4370\u001b[0m  0.0210\n",
            "     33        \u001b[36m0.4320\u001b[0m  0.0212\n",
            "     34        \u001b[36m0.4279\u001b[0m  0.0207\n",
            "     35        \u001b[36m0.4244\u001b[0m  0.0242\n",
            "     36        \u001b[36m0.4214\u001b[0m  0.0279\n",
            "     37        \u001b[36m0.4187\u001b[0m  0.0303\n",
            "     38        \u001b[36m0.4164\u001b[0m  0.0283\n",
            "     39        \u001b[36m0.4143\u001b[0m  0.0256\n",
            "     40        \u001b[36m0.4124\u001b[0m  0.0270\n",
            "     41        \u001b[36m0.4106\u001b[0m  0.0239\n",
            "     42        \u001b[36m0.4090\u001b[0m  0.0267\n",
            "     43        \u001b[36m0.4076\u001b[0m  0.0282\n",
            "     44        \u001b[36m0.4062\u001b[0m  0.0249\n",
            "     45        \u001b[36m0.4050\u001b[0m  0.0310\n",
            "     46        \u001b[36m0.4038\u001b[0m  0.0303\n",
            "     47        \u001b[36m0.4027\u001b[0m  0.0347\n",
            "     48        \u001b[36m0.4017\u001b[0m  0.0315\n",
            "     49        \u001b[36m0.4007\u001b[0m  0.0314\n",
            "     50        \u001b[36m0.3998\u001b[0m  0.0251\n",
            "     51        \u001b[36m0.3990\u001b[0m  0.0230\n",
            "     52        \u001b[36m0.3982\u001b[0m  0.0220\n",
            "     53        \u001b[36m0.3974\u001b[0m  0.0286\n",
            "     54        \u001b[36m0.3967\u001b[0m  0.0247\n",
            "     55        \u001b[36m0.3960\u001b[0m  0.0239\n",
            "     56        \u001b[36m0.3954\u001b[0m  0.0258\n",
            "     57        \u001b[36m0.3947\u001b[0m  0.0240\n",
            "     58        \u001b[36m0.3942\u001b[0m  0.0224\n",
            "     59        \u001b[36m0.3936\u001b[0m  0.0216\n",
            "     60        \u001b[36m0.3931\u001b[0m  0.0203\n",
            "     61        \u001b[36m0.3925\u001b[0m  0.0213\n",
            "     62        \u001b[36m0.3921\u001b[0m  0.0214\n",
            "     63        \u001b[36m0.3916\u001b[0m  0.0210\n",
            "     64        \u001b[36m0.3911\u001b[0m  0.0209\n",
            "     65        \u001b[36m0.3907\u001b[0m  0.0206\n",
            "     66        \u001b[36m0.3903\u001b[0m  0.0215\n",
            "     67        \u001b[36m0.3899\u001b[0m  0.0213\n",
            "     68        \u001b[36m0.3895\u001b[0m  0.0230\n",
            "     69        \u001b[36m0.3891\u001b[0m  0.0235\n",
            "     70        \u001b[36m0.3888\u001b[0m  0.0239\n",
            "     71        \u001b[36m0.3884\u001b[0m  0.0230\n",
            "     72        \u001b[36m0.3881\u001b[0m  0.0231\n",
            "     73        \u001b[36m0.3878\u001b[0m  0.0223\n",
            "     74        \u001b[36m0.3875\u001b[0m  0.0223\n",
            "     75        \u001b[36m0.3872\u001b[0m  0.0249\n",
            "     76        \u001b[36m0.3869\u001b[0m  0.0268\n",
            "     77        \u001b[36m0.3866\u001b[0m  0.0251\n",
            "     78        \u001b[36m0.3863\u001b[0m  0.0259\n",
            "     79        \u001b[36m0.3861\u001b[0m  0.0259\n",
            "     80        \u001b[36m0.3858\u001b[0m  0.0265\n",
            "     81        \u001b[36m0.3856\u001b[0m  0.0330\n",
            "     82        \u001b[36m0.3853\u001b[0m  0.0330\n",
            "     83        \u001b[36m0.3851\u001b[0m  0.0338\n",
            "     84        \u001b[36m0.3849\u001b[0m  0.0239\n",
            "     85        \u001b[36m0.3847\u001b[0m  0.0227\n",
            "     86        \u001b[36m0.3845\u001b[0m  0.0209\n",
            "     87        \u001b[36m0.3843\u001b[0m  0.0223\n",
            "     88        \u001b[36m0.3841\u001b[0m  0.0278\n",
            "     89        \u001b[36m0.3839\u001b[0m  0.0214\n",
            "     90        \u001b[36m0.3837\u001b[0m  0.0213\n",
            "     91        \u001b[36m0.3835\u001b[0m  0.0209\n",
            "     92        \u001b[36m0.3833\u001b[0m  0.0267\n",
            "     93        \u001b[36m0.3831\u001b[0m  0.0238\n",
            "     94        \u001b[36m0.3830\u001b[0m  0.0217\n",
            "     95        \u001b[36m0.3828\u001b[0m  0.0227\n",
            "     96        \u001b[36m0.3827\u001b[0m  0.0217\n",
            "     97        \u001b[36m0.3825\u001b[0m  0.0220\n",
            "     98        \u001b[36m0.3824\u001b[0m  0.0207\n",
            "     99        \u001b[36m0.3822\u001b[0m  0.0204\n",
            "    100        \u001b[36m0.3821\u001b[0m  0.0235\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9793\u001b[0m  0.0268\n",
            "      2        \u001b[36m0.9777\u001b[0m  0.0221\n",
            "      3        \u001b[36m0.9759\u001b[0m  0.0280\n",
            "      4        \u001b[36m0.9739\u001b[0m  0.0232\n",
            "      5        \u001b[36m0.9718\u001b[0m  0.0264\n",
            "      6        \u001b[36m0.9693\u001b[0m  0.0263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.9666\u001b[0m  0.0297\n",
            "      8        \u001b[36m0.9634\u001b[0m  0.0271\n",
            "      9        \u001b[36m0.9598\u001b[0m  0.0288\n",
            "     10        \u001b[36m0.9557\u001b[0m  0.0269\n",
            "     11        \u001b[36m0.9508\u001b[0m  0.0250\n",
            "     12        \u001b[36m0.9451\u001b[0m  0.0257\n",
            "     13        \u001b[36m0.9381\u001b[0m  0.0240\n",
            "     14        \u001b[36m0.9295\u001b[0m  0.0254\n",
            "     15        \u001b[36m0.9186\u001b[0m  0.0248\n",
            "     16        \u001b[36m0.9044\u001b[0m  0.0256\n",
            "     17        \u001b[36m0.8856\u001b[0m  0.0332\n",
            "     18        \u001b[36m0.8605\u001b[0m  0.0359\n",
            "     19        \u001b[36m0.8271\u001b[0m  0.0234\n",
            "     20        \u001b[36m0.7842\u001b[0m  0.0243\n",
            "     21        \u001b[36m0.7330\u001b[0m  0.0223\n",
            "     22        \u001b[36m0.6773\u001b[0m  0.0221\n",
            "     23        \u001b[36m0.6229\u001b[0m  0.0214\n",
            "     24        \u001b[36m0.5749\u001b[0m  0.0219\n",
            "     25        \u001b[36m0.5359\u001b[0m  0.0226\n",
            "     26        \u001b[36m0.5058\u001b[0m  0.0221\n",
            "     27        \u001b[36m0.4832\u001b[0m  0.0212\n",
            "     28        \u001b[36m0.4663\u001b[0m  0.0215\n",
            "     29        \u001b[36m0.4533\u001b[0m  0.0228\n",
            "     30        \u001b[36m0.4431\u001b[0m  0.0212\n",
            "     31        \u001b[36m0.4350\u001b[0m  0.0216\n",
            "     32        \u001b[36m0.4284\u001b[0m  0.0217\n",
            "     33        \u001b[36m0.4231\u001b[0m  0.0215\n",
            "     34        \u001b[36m0.4187\u001b[0m  0.0208\n",
            "     35        \u001b[36m0.4152\u001b[0m  0.0232\n",
            "     36        \u001b[36m0.4123\u001b[0m  0.0274\n",
            "     37        \u001b[36m0.4098\u001b[0m  0.0261\n",
            "     38        \u001b[36m0.4077\u001b[0m  0.0229\n",
            "     39        \u001b[36m0.4058\u001b[0m  0.0258\n",
            "     40        \u001b[36m0.4042\u001b[0m  0.0247\n",
            "     41        \u001b[36m0.4027\u001b[0m  0.0239\n",
            "     42        \u001b[36m0.4014\u001b[0m  0.0252\n",
            "     43        \u001b[36m0.4002\u001b[0m  0.0250\n",
            "     44        \u001b[36m0.3991\u001b[0m  0.0240\n",
            "     45        \u001b[36m0.3980\u001b[0m  0.0249\n",
            "     46        \u001b[36m0.3971\u001b[0m  0.0276\n",
            "     47        \u001b[36m0.3962\u001b[0m  0.0327\n",
            "     48        \u001b[36m0.3953\u001b[0m  0.0249\n",
            "     49        \u001b[36m0.3945\u001b[0m  0.0282\n",
            "     50        \u001b[36m0.3938\u001b[0m  0.0258\n",
            "     51        \u001b[36m0.3931\u001b[0m  0.0254\n",
            "     52        \u001b[36m0.3925\u001b[0m  0.0240\n",
            "     53        \u001b[36m0.3918\u001b[0m  0.0322\n",
            "     54        \u001b[36m0.3912\u001b[0m  0.0239\n",
            "     55        \u001b[36m0.3907\u001b[0m  0.0251\n",
            "     56        \u001b[36m0.3902\u001b[0m  0.0216\n",
            "     57        \u001b[36m0.3896\u001b[0m  0.0217\n",
            "     58        \u001b[36m0.3892\u001b[0m  0.0289\n",
            "     59        \u001b[36m0.3887\u001b[0m  0.0226\n",
            "     60        \u001b[36m0.3883\u001b[0m  0.0220\n",
            "     61        \u001b[36m0.3878\u001b[0m  0.0224\n",
            "     62        \u001b[36m0.3874\u001b[0m  0.0225\n",
            "     63        \u001b[36m0.3871\u001b[0m  0.0253\n",
            "     64        \u001b[36m0.3867\u001b[0m  0.0237\n",
            "     65        \u001b[36m0.3863\u001b[0m  0.0219\n",
            "     66        \u001b[36m0.3860\u001b[0m  0.0234\n",
            "     67        \u001b[36m0.3857\u001b[0m  0.0229\n",
            "     68        \u001b[36m0.3853\u001b[0m  0.0261\n",
            "     69        \u001b[36m0.3850\u001b[0m  0.0441\n",
            "     70        \u001b[36m0.3848\u001b[0m  0.0282\n",
            "     71        \u001b[36m0.3845\u001b[0m  0.0276\n",
            "     72        \u001b[36m0.3842\u001b[0m  0.0269\n",
            "     73        \u001b[36m0.3839\u001b[0m  0.0249\n",
            "     74        \u001b[36m0.3837\u001b[0m  0.0288\n",
            "     75        \u001b[36m0.3834\u001b[0m  0.0268\n",
            "     76        \u001b[36m0.3832\u001b[0m  0.0275\n",
            "     77        \u001b[36m0.3830\u001b[0m  0.0292\n",
            "     78        \u001b[36m0.3828\u001b[0m  0.0309\n",
            "     79        \u001b[36m0.3825\u001b[0m  0.0265\n",
            "     80        \u001b[36m0.3823\u001b[0m  0.0261\n",
            "     81        \u001b[36m0.3821\u001b[0m  0.0260\n",
            "     82        \u001b[36m0.3819\u001b[0m  0.0234\n",
            "     83        \u001b[36m0.3818\u001b[0m  0.0235\n",
            "     84        \u001b[36m0.3816\u001b[0m  0.0231\n",
            "     85        \u001b[36m0.3814\u001b[0m  0.0226\n",
            "     86        \u001b[36m0.3812\u001b[0m  0.0226\n",
            "     87        \u001b[36m0.3811\u001b[0m  0.0334\n",
            "     88        \u001b[36m0.3809\u001b[0m  0.0416\n",
            "     89        \u001b[36m0.3807\u001b[0m  0.0262\n",
            "     90        \u001b[36m0.3806\u001b[0m  0.0250\n",
            "     91        \u001b[36m0.3804\u001b[0m  0.0250\n",
            "     92        \u001b[36m0.3803\u001b[0m  0.0275\n",
            "     93        \u001b[36m0.3801\u001b[0m  0.0288\n",
            "     94        \u001b[36m0.3800\u001b[0m  0.0282\n",
            "     95        \u001b[36m0.3799\u001b[0m  0.0233\n",
            "     96        \u001b[36m0.3797\u001b[0m  0.0243\n",
            "     97        \u001b[36m0.3796\u001b[0m  0.0276\n",
            "     98        \u001b[36m0.3795\u001b[0m  0.0243\n",
            "     99        \u001b[36m0.3794\u001b[0m  0.0285\n",
            "    100        \u001b[36m0.3793\u001b[0m  0.0305\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9399\u001b[0m  0.0265\n",
            "      2        \u001b[36m0.9396\u001b[0m  0.0265\n",
            "      3        \u001b[36m0.9394\u001b[0m  0.0220\n",
            "      4        \u001b[36m0.9391\u001b[0m  0.0219\n",
            "      5        \u001b[36m0.9389\u001b[0m  0.0213\n",
            "      6        \u001b[36m0.9387\u001b[0m  0.0281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.9384\u001b[0m  0.0292\n",
            "      8        \u001b[36m0.9382\u001b[0m  0.0257\n",
            "      9        \u001b[36m0.9379\u001b[0m  0.0240\n",
            "     10        \u001b[36m0.9376\u001b[0m  0.0212\n",
            "     11        \u001b[36m0.9374\u001b[0m  0.0240\n",
            "     12        \u001b[36m0.9371\u001b[0m  0.0224\n",
            "     13        \u001b[36m0.9369\u001b[0m  0.0192\n",
            "     14        \u001b[36m0.9366\u001b[0m  0.0213\n",
            "     15        \u001b[36m0.9363\u001b[0m  0.0190\n",
            "     16        \u001b[36m0.9361\u001b[0m  0.0210\n",
            "     17        \u001b[36m0.9358\u001b[0m  0.0170\n",
            "     18        \u001b[36m0.9355\u001b[0m  0.0193\n",
            "     19        \u001b[36m0.9353\u001b[0m  0.0193\n",
            "     20        \u001b[36m0.9350\u001b[0m  0.0191\n",
            "     21        \u001b[36m0.9347\u001b[0m  0.0178\n",
            "     22        \u001b[36m0.9344\u001b[0m  0.0196\n",
            "     23        \u001b[36m0.9342\u001b[0m  0.0222\n",
            "     24        \u001b[36m0.9339\u001b[0m  0.0248\n",
            "     25        \u001b[36m0.9336\u001b[0m  0.0215\n",
            "     26        \u001b[36m0.9333\u001b[0m  0.0187\n",
            "     27        \u001b[36m0.9330\u001b[0m  0.0202\n",
            "     28        \u001b[36m0.9327\u001b[0m  0.0209\n",
            "     29        \u001b[36m0.9324\u001b[0m  0.0188\n",
            "     30        \u001b[36m0.9321\u001b[0m  0.0196\n",
            "     31        \u001b[36m0.9318\u001b[0m  0.0183\n",
            "     32        \u001b[36m0.9315\u001b[0m  0.0183\n",
            "     33        \u001b[36m0.9312\u001b[0m  0.0188\n",
            "     34        \u001b[36m0.9309\u001b[0m  0.0188\n",
            "     35        \u001b[36m0.9306\u001b[0m  0.0223\n",
            "     36        \u001b[36m0.9303\u001b[0m  0.0174\n",
            "     37        \u001b[36m0.9300\u001b[0m  0.0213\n",
            "     38        \u001b[36m0.9297\u001b[0m  0.0198\n",
            "     39        \u001b[36m0.9294\u001b[0m  0.0181\n",
            "     40        \u001b[36m0.9290\u001b[0m  0.0184\n",
            "     41        \u001b[36m0.9287\u001b[0m  0.0242\n",
            "     42        \u001b[36m0.9284\u001b[0m  0.0238\n",
            "     43        \u001b[36m0.9281\u001b[0m  0.0211\n",
            "     44        \u001b[36m0.9277\u001b[0m  0.0222\n",
            "     45        \u001b[36m0.9274\u001b[0m  0.0233\n",
            "     46        \u001b[36m0.9271\u001b[0m  0.0264\n",
            "     47        \u001b[36m0.9267\u001b[0m  0.0195\n",
            "     48        \u001b[36m0.9264\u001b[0m  0.0239\n",
            "     49        \u001b[36m0.9260\u001b[0m  0.0201\n",
            "     50        \u001b[36m0.9257\u001b[0m  0.0200\n",
            "     51        \u001b[36m0.9253\u001b[0m  0.0182\n",
            "     52        \u001b[36m0.9250\u001b[0m  0.0186\n",
            "     53        \u001b[36m0.9246\u001b[0m  0.0197\n",
            "     54        \u001b[36m0.9243\u001b[0m  0.0191\n",
            "     55        \u001b[36m0.9239\u001b[0m  0.0193\n",
            "     56        \u001b[36m0.9235\u001b[0m  0.0213\n",
            "     57        \u001b[36m0.9232\u001b[0m  0.0192\n",
            "     58        \u001b[36m0.9228\u001b[0m  0.0216\n",
            "     59        \u001b[36m0.9224\u001b[0m  0.0205\n",
            "     60        \u001b[36m0.9221\u001b[0m  0.0192\n",
            "     61        \u001b[36m0.9217\u001b[0m  0.0193\n",
            "     62        \u001b[36m0.9213\u001b[0m  0.0289\n",
            "     63        \u001b[36m0.9209\u001b[0m  0.0254\n",
            "     64        \u001b[36m0.9205\u001b[0m  0.0269\n",
            "     65        \u001b[36m0.9201\u001b[0m  0.0188\n",
            "     66        \u001b[36m0.9197\u001b[0m  0.0184\n",
            "     67        \u001b[36m0.9193\u001b[0m  0.0175\n",
            "     68        \u001b[36m0.9189\u001b[0m  0.0178\n",
            "     69        \u001b[36m0.9185\u001b[0m  0.0165\n",
            "     70        \u001b[36m0.9181\u001b[0m  0.0193\n",
            "     71        \u001b[36m0.9177\u001b[0m  0.0258\n",
            "     72        \u001b[36m0.9173\u001b[0m  0.0200\n",
            "     73        \u001b[36m0.9169\u001b[0m  0.0199\n",
            "     74        \u001b[36m0.9164\u001b[0m  0.0212\n",
            "     75        \u001b[36m0.9160\u001b[0m  0.0277\n",
            "     76        \u001b[36m0.9156\u001b[0m  0.0229\n",
            "     77        \u001b[36m0.9151\u001b[0m  0.0222\n",
            "     78        \u001b[36m0.9147\u001b[0m  0.0244\n",
            "     79        \u001b[36m0.9143\u001b[0m  0.0254\n",
            "     80        \u001b[36m0.9138\u001b[0m  0.0232\n",
            "     81        \u001b[36m0.9134\u001b[0m  0.0244\n",
            "     82        \u001b[36m0.9129\u001b[0m  0.0263\n",
            "     83        \u001b[36m0.9124\u001b[0m  0.0259\n",
            "     84        \u001b[36m0.9120\u001b[0m  0.0234\n",
            "     85        \u001b[36m0.9115\u001b[0m  0.0227\n",
            "     86        \u001b[36m0.9110\u001b[0m  0.0245\n",
            "     87        \u001b[36m0.9106\u001b[0m  0.0216\n",
            "     88        \u001b[36m0.9101\u001b[0m  0.0236\n",
            "     89        \u001b[36m0.9096\u001b[0m  0.0222\n",
            "     90        \u001b[36m0.9091\u001b[0m  0.0207\n",
            "     91        \u001b[36m0.9086\u001b[0m  0.0215\n",
            "     92        \u001b[36m0.9081\u001b[0m  0.0196\n",
            "     93        \u001b[36m0.9076\u001b[0m  0.0289\n",
            "     94        \u001b[36m0.9071\u001b[0m  0.0193\n",
            "     95        \u001b[36m0.9066\u001b[0m  0.0192\n",
            "     96        \u001b[36m0.9061\u001b[0m  0.0181\n",
            "     97        \u001b[36m0.9056\u001b[0m  0.0180\n",
            "     98        \u001b[36m0.9051\u001b[0m  0.0196\n",
            "     99        \u001b[36m0.9045\u001b[0m  0.0181\n",
            "    100        \u001b[36m0.9040\u001b[0m  0.0182\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9784\u001b[0m  0.0236\n",
            "      2        \u001b[36m0.9784\u001b[0m  0.0219\n",
            "      3        \u001b[36m0.9783\u001b[0m  0.0178\n",
            "      4        \u001b[36m0.9783\u001b[0m  0.0171\n",
            "      5        \u001b[36m0.9783\u001b[0m  0.0184\n",
            "      6        \u001b[36m0.9782\u001b[0m  0.0179\n",
            "      7        \u001b[36m0.9782\u001b[0m  0.0172\n",
            "      8        \u001b[36m0.9781\u001b[0m  0.0212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9        \u001b[36m0.9781\u001b[0m  0.0201\n",
            "     10        \u001b[36m0.9781\u001b[0m  0.0180\n",
            "     11        \u001b[36m0.9780\u001b[0m  0.0183\n",
            "     12        \u001b[36m0.9780\u001b[0m  0.0182\n",
            "     13        \u001b[36m0.9780\u001b[0m  0.0181\n",
            "     14        \u001b[36m0.9779\u001b[0m  0.0184\n",
            "     15        \u001b[36m0.9779\u001b[0m  0.0233\n",
            "     16        \u001b[36m0.9779\u001b[0m  0.0231\n",
            "     17        \u001b[36m0.9778\u001b[0m  0.0217\n",
            "     18        \u001b[36m0.9778\u001b[0m  0.0246\n",
            "     19        \u001b[36m0.9777\u001b[0m  0.0215\n",
            "     20        \u001b[36m0.9777\u001b[0m  0.0217\n",
            "     21        \u001b[36m0.9777\u001b[0m  0.0213\n",
            "     22        \u001b[36m0.9776\u001b[0m  0.0205\n",
            "     23        \u001b[36m0.9776\u001b[0m  0.0201\n",
            "     24        \u001b[36m0.9776\u001b[0m  0.0216\n",
            "     25        \u001b[36m0.9775\u001b[0m  0.0201\n",
            "     26        \u001b[36m0.9775\u001b[0m  0.0205\n",
            "     27        \u001b[36m0.9774\u001b[0m  0.0204\n",
            "     28        \u001b[36m0.9774\u001b[0m  0.0190\n",
            "     29        \u001b[36m0.9774\u001b[0m  0.0245\n",
            "     30        \u001b[36m0.9773\u001b[0m  0.0171\n",
            "     31        \u001b[36m0.9773\u001b[0m  0.0232\n",
            "     32        \u001b[36m0.9772\u001b[0m  0.0180\n",
            "     33        \u001b[36m0.9772\u001b[0m  0.0191\n",
            "     34        \u001b[36m0.9772\u001b[0m  0.0188\n",
            "     35        \u001b[36m0.9771\u001b[0m  0.0204\n",
            "     36        \u001b[36m0.9771\u001b[0m  0.0192\n",
            "     37        \u001b[36m0.9770\u001b[0m  0.0196\n",
            "     38        \u001b[36m0.9770\u001b[0m  0.0215\n",
            "     39        \u001b[36m0.9770\u001b[0m  0.0211\n",
            "     40        \u001b[36m0.9769\u001b[0m  0.0221\n",
            "     41        \u001b[36m0.9769\u001b[0m  0.0216\n",
            "     42        \u001b[36m0.9768\u001b[0m  0.0222\n",
            "     43        \u001b[36m0.9768\u001b[0m  0.0247\n",
            "     44        \u001b[36m0.9768\u001b[0m  0.0263\n",
            "     45        \u001b[36m0.9767\u001b[0m  0.0291\n",
            "     46        \u001b[36m0.9767\u001b[0m  0.0191\n",
            "     47        \u001b[36m0.9766\u001b[0m  0.0210\n",
            "     48        \u001b[36m0.9766\u001b[0m  0.0177\n",
            "     49        \u001b[36m0.9766\u001b[0m  0.0232\n",
            "     50        \u001b[36m0.9765\u001b[0m  0.0192\n",
            "     51        \u001b[36m0.9765\u001b[0m  0.0189\n",
            "     52        \u001b[36m0.9764\u001b[0m  0.0194\n",
            "     53        \u001b[36m0.9764\u001b[0m  0.0188\n",
            "     54        \u001b[36m0.9764\u001b[0m  0.0185\n",
            "     55        \u001b[36m0.9763\u001b[0m  0.0200\n",
            "     56        \u001b[36m0.9763\u001b[0m  0.0210\n",
            "     57        \u001b[36m0.9762\u001b[0m  0.0215\n",
            "     58        \u001b[36m0.9762\u001b[0m  0.0211\n",
            "     59        \u001b[36m0.9761\u001b[0m  0.0201\n",
            "     60        \u001b[36m0.9761\u001b[0m  0.0227\n",
            "     61        \u001b[36m0.9761\u001b[0m  0.0216\n",
            "     62        \u001b[36m0.9760\u001b[0m  0.0218\n",
            "     63        \u001b[36m0.9760\u001b[0m  0.0236\n",
            "     64        \u001b[36m0.9759\u001b[0m  0.0202\n",
            "     65        \u001b[36m0.9759\u001b[0m  0.0179\n",
            "     66        \u001b[36m0.9758\u001b[0m  0.0174\n",
            "     67        \u001b[36m0.9758\u001b[0m  0.0185\n",
            "     68        \u001b[36m0.9757\u001b[0m  0.0189\n",
            "     69        \u001b[36m0.9757\u001b[0m  0.0185\n",
            "     70        \u001b[36m0.9757\u001b[0m  0.0177\n",
            "     71        \u001b[36m0.9756\u001b[0m  0.0212\n",
            "     72        \u001b[36m0.9756\u001b[0m  0.0202\n",
            "     73        \u001b[36m0.9755\u001b[0m  0.0199\n",
            "     74        \u001b[36m0.9755\u001b[0m  0.0193\n",
            "     75        \u001b[36m0.9754\u001b[0m  0.0180\n",
            "     76        \u001b[36m0.9754\u001b[0m  0.0208\n",
            "     77        \u001b[36m0.9753\u001b[0m  0.0324\n",
            "     78        \u001b[36m0.9753\u001b[0m  0.0212\n",
            "     79        \u001b[36m0.9753\u001b[0m  0.0193\n",
            "     80        \u001b[36m0.9752\u001b[0m  0.0197\n",
            "     81        \u001b[36m0.9752\u001b[0m  0.0189\n",
            "     82        \u001b[36m0.9751\u001b[0m  0.0190\n",
            "     83        \u001b[36m0.9751\u001b[0m  0.0175\n",
            "     84        \u001b[36m0.9750\u001b[0m  0.0192\n",
            "     85        \u001b[36m0.9750\u001b[0m  0.0206\n",
            "     86        \u001b[36m0.9749\u001b[0m  0.0225\n",
            "     87        \u001b[36m0.9749\u001b[0m  0.0261\n",
            "     88        \u001b[36m0.9748\u001b[0m  0.0177\n",
            "     89        \u001b[36m0.9748\u001b[0m  0.0196\n",
            "     90        \u001b[36m0.9747\u001b[0m  0.0181\n",
            "     91        \u001b[36m0.9747\u001b[0m  0.0202\n",
            "     92        \u001b[36m0.9746\u001b[0m  0.0194\n",
            "     93        \u001b[36m0.9746\u001b[0m  0.0188\n",
            "     94        \u001b[36m0.9745\u001b[0m  0.0249\n",
            "     95        \u001b[36m0.9745\u001b[0m  0.0234\n",
            "     96        \u001b[36m0.9744\u001b[0m  0.0208\n",
            "     97        \u001b[36m0.9744\u001b[0m  0.0206\n",
            "     98        \u001b[36m0.9743\u001b[0m  0.0192\n",
            "     99        \u001b[36m0.9743\u001b[0m  0.0205\n",
            "    100        \u001b[36m0.9743\u001b[0m  0.0206\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9981\u001b[0m  0.0254\n",
            "      2        \u001b[36m0.9978\u001b[0m  0.0250\n",
            "      3        \u001b[36m0.9975\u001b[0m  0.0242\n",
            "      4        \u001b[36m0.9970\u001b[0m  0.0225\n",
            "      5        \u001b[36m0.9965\u001b[0m  0.0222\n",
            "      6        \u001b[36m0.9959\u001b[0m  0.0228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.9950\u001b[0m  0.0247\n",
            "      8        \u001b[36m0.9940\u001b[0m  0.0238\n",
            "      9        \u001b[36m0.9926\u001b[0m  0.0235\n",
            "     10        \u001b[36m0.9909\u001b[0m  0.0246\n",
            "     11        \u001b[36m0.9885\u001b[0m  0.0301\n",
            "     12        \u001b[36m0.9854\u001b[0m  0.0230\n",
            "     13        \u001b[36m0.9813\u001b[0m  0.0209\n",
            "     14        \u001b[36m0.9756\u001b[0m  0.0224\n",
            "     15        \u001b[36m0.9680\u001b[0m  0.0221\n",
            "     16        \u001b[36m0.9576\u001b[0m  0.0219\n",
            "     17        \u001b[36m0.9437\u001b[0m  0.0219\n",
            "     18        \u001b[36m0.9250\u001b[0m  0.0230\n",
            "     19        \u001b[36m0.9005\u001b[0m  0.0231\n",
            "     20        \u001b[36m0.8692\u001b[0m  0.0231\n",
            "     21        \u001b[36m0.8308\u001b[0m  0.0214\n",
            "     22        \u001b[36m0.7858\u001b[0m  0.0230\n",
            "     23        \u001b[36m0.7363\u001b[0m  0.0244\n",
            "     24        \u001b[36m0.6852\u001b[0m  0.0270\n",
            "     25        \u001b[36m0.6363\u001b[0m  0.0319\n",
            "     26        \u001b[36m0.5931\u001b[0m  0.0280\n",
            "     27        \u001b[36m0.5571\u001b[0m  0.0264\n",
            "     28        \u001b[36m0.5275\u001b[0m  0.0275\n",
            "     29        \u001b[36m0.5023\u001b[0m  0.0283\n",
            "     30        \u001b[36m0.4796\u001b[0m  0.0267\n",
            "     31        \u001b[36m0.4578\u001b[0m  0.0274\n",
            "     32        \u001b[36m0.4376\u001b[0m  0.0273\n",
            "     33        \u001b[36m0.4223\u001b[0m  0.0254\n",
            "     34        \u001b[36m0.4132\u001b[0m  0.0321\n",
            "     35        \u001b[36m0.4082\u001b[0m  0.0301\n",
            "     36        \u001b[36m0.4050\u001b[0m  0.0255\n",
            "     37        \u001b[36m0.4027\u001b[0m  0.0257\n",
            "     38        \u001b[36m0.4008\u001b[0m  0.0249\n",
            "     39        \u001b[36m0.3990\u001b[0m  0.0210\n",
            "     40        \u001b[36m0.3970\u001b[0m  0.0224\n",
            "     41        \u001b[36m0.3944\u001b[0m  0.0220\n",
            "     42        \u001b[36m0.3915\u001b[0m  0.0228\n",
            "     43        \u001b[36m0.3888\u001b[0m  0.0226\n",
            "     44        \u001b[36m0.3868\u001b[0m  0.0240\n",
            "     45        \u001b[36m0.3856\u001b[0m  0.0237\n",
            "     46        \u001b[36m0.3849\u001b[0m  0.0248\n",
            "     47        \u001b[36m0.3845\u001b[0m  0.0246\n",
            "     48        \u001b[36m0.3841\u001b[0m  0.0254\n",
            "     49        \u001b[36m0.3838\u001b[0m  0.0231\n",
            "     50        \u001b[36m0.3836\u001b[0m  0.0232\n",
            "     51        \u001b[36m0.3833\u001b[0m  0.0222\n",
            "     52        \u001b[36m0.3830\u001b[0m  0.0226\n",
            "     53        \u001b[36m0.3827\u001b[0m  0.0227\n",
            "     54        \u001b[36m0.3822\u001b[0m  0.0216\n",
            "     55        \u001b[36m0.3817\u001b[0m  0.0221\n",
            "     56        \u001b[36m0.3810\u001b[0m  0.0271\n",
            "     57        \u001b[36m0.3801\u001b[0m  0.0290\n",
            "     58        \u001b[36m0.3793\u001b[0m  0.0244\n",
            "     59        \u001b[36m0.3788\u001b[0m  0.0215\n",
            "     60        \u001b[36m0.3785\u001b[0m  0.0269\n",
            "     61        \u001b[36m0.3784\u001b[0m  0.0260\n",
            "     62        \u001b[36m0.3783\u001b[0m  0.0237\n",
            "     63        \u001b[36m0.3782\u001b[0m  0.0211\n",
            "     64        \u001b[36m0.3782\u001b[0m  0.0262\n",
            "     65        \u001b[36m0.3781\u001b[0m  0.0239\n",
            "     66        \u001b[36m0.3780\u001b[0m  0.0256\n",
            "     67        \u001b[36m0.3780\u001b[0m  0.0257\n",
            "     68        \u001b[36m0.3779\u001b[0m  0.0272\n",
            "     69        \u001b[36m0.3779\u001b[0m  0.0278\n",
            "     70        \u001b[36m0.3778\u001b[0m  0.0262\n",
            "     71        \u001b[36m0.3777\u001b[0m  0.0260\n",
            "     72        \u001b[36m0.3777\u001b[0m  0.0260\n",
            "     73        \u001b[36m0.3776\u001b[0m  0.0223\n",
            "     74        \u001b[36m0.3776\u001b[0m  0.0221\n",
            "     75        \u001b[36m0.3775\u001b[0m  0.0221\n",
            "     76        \u001b[36m0.3775\u001b[0m  0.0225\n",
            "     77        \u001b[36m0.3774\u001b[0m  0.0214\n",
            "     78        \u001b[36m0.3774\u001b[0m  0.0219\n",
            "     79        \u001b[36m0.3773\u001b[0m  0.0235\n",
            "     80        \u001b[36m0.3773\u001b[0m  0.0235\n",
            "     81        \u001b[36m0.3772\u001b[0m  0.0232\n",
            "     82        \u001b[36m0.3772\u001b[0m  0.0238\n",
            "     83        \u001b[36m0.3771\u001b[0m  0.0221\n",
            "     84        \u001b[36m0.3771\u001b[0m  0.0287\n",
            "     85        \u001b[36m0.3770\u001b[0m  0.0209\n",
            "     86        \u001b[36m0.3770\u001b[0m  0.0233\n",
            "     87        \u001b[36m0.3770\u001b[0m  0.0217\n",
            "     88        \u001b[36m0.3769\u001b[0m  0.0219\n",
            "     89        \u001b[36m0.3769\u001b[0m  0.0217\n",
            "     90        \u001b[36m0.3768\u001b[0m  0.0219\n",
            "     91        \u001b[36m0.3768\u001b[0m  0.0210\n",
            "     92        \u001b[36m0.3768\u001b[0m  0.0239\n",
            "     93        \u001b[36m0.3767\u001b[0m  0.0221\n",
            "     94        \u001b[36m0.3767\u001b[0m  0.0229\n",
            "     95        \u001b[36m0.3766\u001b[0m  0.0220\n",
            "     96        \u001b[36m0.3766\u001b[0m  0.0225\n",
            "     97        \u001b[36m0.3766\u001b[0m  0.0249\n",
            "     98        \u001b[36m0.3765\u001b[0m  0.0274\n",
            "     99        \u001b[36m0.3765\u001b[0m  0.0327\n",
            "    100        \u001b[36m0.3765\u001b[0m  0.0262\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9997\u001b[0m  0.0264\n",
            "      2        \u001b[36m0.9996\u001b[0m  0.0273\n",
            "      3        \u001b[36m0.9995\u001b[0m  0.0281\n",
            "      4        \u001b[36m0.9994\u001b[0m  0.0274\n",
            "      5        \u001b[36m0.9994\u001b[0m  0.0269\n",
            "      6        \u001b[36m0.9992\u001b[0m  0.0252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.9991\u001b[0m  0.0250\n",
            "      8        \u001b[36m0.9989\u001b[0m  0.0258\n",
            "      9        \u001b[36m0.9986\u001b[0m  0.0256\n",
            "     10        \u001b[36m0.9983\u001b[0m  0.0243\n",
            "     11        \u001b[36m0.9979\u001b[0m  0.0244\n",
            "     12        \u001b[36m0.9973\u001b[0m  0.0226\n",
            "     13        \u001b[36m0.9965\u001b[0m  0.0224\n",
            "     14        \u001b[36m0.9954\u001b[0m  0.0215\n",
            "     15        \u001b[36m0.9938\u001b[0m  0.0291\n",
            "     16        \u001b[36m0.9917\u001b[0m  0.0226\n",
            "     17        \u001b[36m0.9885\u001b[0m  0.0237\n",
            "     18        \u001b[36m0.9839\u001b[0m  0.0246\n",
            "     19        \u001b[36m0.9766\u001b[0m  0.0233\n",
            "     20        \u001b[36m0.9643\u001b[0m  0.0219\n",
            "     21        \u001b[36m0.9417\u001b[0m  0.0219\n",
            "     22        \u001b[36m0.8988\u001b[0m  0.0235\n",
            "     23        \u001b[36m0.8240\u001b[0m  0.0232\n",
            "     24        \u001b[36m0.7194\u001b[0m  0.0220\n",
            "     25        \u001b[36m0.6147\u001b[0m  0.0231\n",
            "     26        \u001b[36m0.5396\u001b[0m  0.0222\n",
            "     27        \u001b[36m0.4933\u001b[0m  0.0227\n",
            "     28        \u001b[36m0.4639\u001b[0m  0.0231\n",
            "     29        \u001b[36m0.4432\u001b[0m  0.0227\n",
            "     30        \u001b[36m0.4268\u001b[0m  0.0253\n",
            "     31        \u001b[36m0.4138\u001b[0m  0.0249\n",
            "     32        \u001b[36m0.4051\u001b[0m  0.0253\n",
            "     33        \u001b[36m0.3999\u001b[0m  0.0245\n",
            "     34        \u001b[36m0.3967\u001b[0m  0.0291\n",
            "     35        \u001b[36m0.3944\u001b[0m  0.0299\n",
            "     36        \u001b[36m0.3922\u001b[0m  0.0281\n",
            "     37        \u001b[36m0.3901\u001b[0m  0.0273\n",
            "     38        \u001b[36m0.3884\u001b[0m  0.0247\n",
            "     39        \u001b[36m0.3872\u001b[0m  0.0270\n",
            "     40        \u001b[36m0.3864\u001b[0m  0.0266\n",
            "     41        \u001b[36m0.3857\u001b[0m  0.0266\n",
            "     42        \u001b[36m0.3851\u001b[0m  0.0249\n",
            "     43        \u001b[36m0.3846\u001b[0m  0.0230\n",
            "     44        \u001b[36m0.3841\u001b[0m  0.0229\n",
            "     45        \u001b[36m0.3837\u001b[0m  0.0225\n",
            "     46        \u001b[36m0.3833\u001b[0m  0.0259\n",
            "     47        \u001b[36m0.3829\u001b[0m  0.0230\n",
            "     48        \u001b[36m0.3825\u001b[0m  0.0241\n",
            "     49        \u001b[36m0.3822\u001b[0m  0.0242\n",
            "     50        \u001b[36m0.3818\u001b[0m  0.0229\n",
            "     51        \u001b[36m0.3815\u001b[0m  0.0237\n",
            "     52        \u001b[36m0.3812\u001b[0m  0.0258\n",
            "     53        \u001b[36m0.3809\u001b[0m  0.0226\n",
            "     54        \u001b[36m0.3805\u001b[0m  0.0229\n",
            "     55        \u001b[36m0.3799\u001b[0m  0.0229\n",
            "     56        \u001b[36m0.3789\u001b[0m  0.0224\n",
            "     57        \u001b[36m0.3781\u001b[0m  0.0234\n",
            "     58        \u001b[36m0.3777\u001b[0m  0.0229\n",
            "     59        \u001b[36m0.3774\u001b[0m  0.0224\n",
            "     60        \u001b[36m0.3773\u001b[0m  0.0239\n",
            "     61        \u001b[36m0.3771\u001b[0m  0.0279\n",
            "     62        \u001b[36m0.3770\u001b[0m  0.0247\n",
            "     63        \u001b[36m0.3769\u001b[0m  0.0247\n",
            "     64        \u001b[36m0.3768\u001b[0m  0.0215\n",
            "     65        \u001b[36m0.3768\u001b[0m  0.0232\n",
            "     66        \u001b[36m0.3767\u001b[0m  0.0253\n",
            "     67        \u001b[36m0.3766\u001b[0m  0.0282\n",
            "     68        \u001b[36m0.3765\u001b[0m  0.0267\n",
            "     69        \u001b[36m0.3764\u001b[0m  0.0261\n",
            "     70        \u001b[36m0.3763\u001b[0m  0.0307\n",
            "     71        \u001b[36m0.3763\u001b[0m  0.0304\n",
            "     72        \u001b[36m0.3762\u001b[0m  0.0327\n",
            "     73        \u001b[36m0.3761\u001b[0m  0.0259\n",
            "     74        \u001b[36m0.3761\u001b[0m  0.0257\n",
            "     75        \u001b[36m0.3760\u001b[0m  0.0314\n",
            "     76        \u001b[36m0.3759\u001b[0m  0.0230\n",
            "     77        \u001b[36m0.3759\u001b[0m  0.0228\n",
            "     78        \u001b[36m0.3758\u001b[0m  0.0228\n",
            "     79        \u001b[36m0.3757\u001b[0m  0.0221\n",
            "     80        \u001b[36m0.3757\u001b[0m  0.0224\n",
            "     81        \u001b[36m0.3756\u001b[0m  0.0231\n",
            "     82        \u001b[36m0.3756\u001b[0m  0.0253\n",
            "     83        \u001b[36m0.3755\u001b[0m  0.0272\n",
            "     84        \u001b[36m0.3755\u001b[0m  0.0264\n",
            "     85        \u001b[36m0.3754\u001b[0m  0.0243\n",
            "     86        \u001b[36m0.3754\u001b[0m  0.0246\n",
            "     87        \u001b[36m0.3753\u001b[0m  0.0328\n",
            "     88        \u001b[36m0.3753\u001b[0m  0.0223\n",
            "     89        \u001b[36m0.3752\u001b[0m  0.0239\n",
            "     90        \u001b[36m0.3752\u001b[0m  0.0235\n",
            "     91        \u001b[36m0.3751\u001b[0m  0.0216\n",
            "     92        \u001b[36m0.3751\u001b[0m  0.0235\n",
            "     93        \u001b[36m0.3750\u001b[0m  0.0223\n",
            "     94        \u001b[36m0.3750\u001b[0m  0.0222\n",
            "     95        \u001b[36m0.3749\u001b[0m  0.0230\n",
            "     96        \u001b[36m0.3749\u001b[0m  0.0232\n",
            "     97        \u001b[36m0.3749\u001b[0m  0.0230\n",
            "     98        \u001b[36m0.3748\u001b[0m  0.0237\n",
            "     99        \u001b[36m0.3748\u001b[0m  0.0237\n",
            "    100        \u001b[36m0.3747\u001b[0m  0.0259\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9998\u001b[0m  0.0236\n",
            "      2        \u001b[36m0.9998\u001b[0m  0.0205\n",
            "      3        0.9998  0.0210\n",
            "      4        0.9998  0.0191\n",
            "      5        0.9998  0.0207\n",
            "      6        0.9998  0.0227\n",
            "      7        0.9998  0.0219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        0.9998  0.0233\n",
            "      9        0.9998  0.0237\n",
            "     10        0.9998  0.0192\n",
            "     11        0.9998  0.0209\n",
            "     12        0.9998  0.0202\n",
            "     13        \u001b[36m0.9998\u001b[0m  0.0190\n",
            "     14        \u001b[36m0.9998\u001b[0m  0.0188\n",
            "     15        0.9998  0.0177\n",
            "     16        0.9998  0.0177\n",
            "     17        0.9998  0.0176\n",
            "     18        0.9998  0.0159\n",
            "     19        0.9998  0.0208\n",
            "     20        0.9998  0.0207\n",
            "     21        0.9998  0.0180\n",
            "     22        0.9998  0.0230\n",
            "     23        0.9998  0.0219\n",
            "     24        0.9998  0.0237\n",
            "     25        0.9998  0.0200\n",
            "     26        0.9998  0.0250\n",
            "     27        0.9998  0.0193\n",
            "     28        0.9998  0.0215\n",
            "     29        0.9998  0.0238\n",
            "     30        0.9998  0.0196\n",
            "     31        0.9998  0.0299\n",
            "     32        0.9998  0.0198\n",
            "     33        0.9998  0.0180\n",
            "     34        0.9998  0.0174\n",
            "     35        0.9998  0.0194\n",
            "     36        \u001b[36m0.9998\u001b[0m  0.0192\n",
            "     37        \u001b[36m0.9998\u001b[0m  0.0201\n",
            "     38        0.9998  0.0191\n",
            "     39        0.9998  0.0187\n",
            "     40        0.9998  0.0194\n",
            "     41        0.9998  0.0203\n",
            "     42        0.9998  0.0208\n",
            "     43        0.9998  0.0220\n",
            "     44        0.9998  0.0278\n",
            "     45        0.9998  0.0227\n",
            "     46        0.9998  0.0218\n",
            "     47        0.9998  0.0264\n",
            "     48        \u001b[36m0.9998\u001b[0m  0.0214\n",
            "     49        0.9998  0.0202\n",
            "     50        0.9998  0.0208\n",
            "     51        0.9998  0.0175\n",
            "     52        0.9998  0.0209\n",
            "     53        0.9998  0.0200\n",
            "     54        0.9998  0.0209\n",
            "     55        0.9998  0.0183\n",
            "     56        0.9998  0.0185\n",
            "     57        0.9998  0.0169\n",
            "     58        0.9998  0.0182\n",
            "     59        \u001b[36m0.9998\u001b[0m  0.0182\n",
            "     60        \u001b[36m0.9998\u001b[0m  0.0186\n",
            "     61        0.9998  0.0188\n",
            "     62        0.9998  0.0190\n",
            "     63        0.9998  0.0204\n",
            "     64        0.9998  0.0199\n",
            "     65        0.9998  0.0187\n",
            "     66        0.9998  0.0173\n",
            "     67        0.9998  0.0174\n",
            "     68        0.9998  0.0185\n",
            "     69        0.9998  0.0184\n",
            "     70        0.9998  0.0179\n",
            "     71        \u001b[36m0.9998\u001b[0m  0.0177\n",
            "     72        0.9998  0.0177\n",
            "     73        0.9998  0.0185\n",
            "     74        0.9998  0.0194\n",
            "     75        0.9998  0.0184\n",
            "     76        0.9998  0.0215\n",
            "     77        0.9998  0.0199\n",
            "     78        0.9998  0.0207\n",
            "     79        0.9998  0.0198\n",
            "     80        0.9998  0.0236\n",
            "     81        0.9998  0.0195\n",
            "     82        \u001b[36m0.9998\u001b[0m  0.0208\n",
            "     83        \u001b[36m0.9998\u001b[0m  0.0198\n",
            "     84        0.9998  0.0239\n",
            "     85        0.9998  0.0262\n",
            "     86        0.9998  0.0238\n",
            "     87        0.9998  0.0193\n",
            "     88        0.9998  0.0186\n",
            "     89        0.9998  0.0179\n",
            "     90        0.9998  0.0185\n",
            "     91        0.9998  0.0172\n",
            "     92        0.9998  0.0187\n",
            "     93        0.9998  0.0191\n",
            "     94        \u001b[36m0.9998\u001b[0m  0.0185\n",
            "     95        0.9998  0.0192\n",
            "     96        0.9998  0.0186\n",
            "     97        0.9998  0.0208\n",
            "     98        0.9998  0.0273\n",
            "     99        0.9998  0.0171\n",
            "    100        0.9998  0.0171\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.9991\u001b[0m  0.0172\n",
            "      2        \u001b[36m0.9991\u001b[0m  0.0188\n",
            "      3        \u001b[36m0.9991\u001b[0m  0.0197\n",
            "      4        \u001b[36m0.9991\u001b[0m  0.0158\n",
            "      5        \u001b[36m0.9991\u001b[0m  0.0232\n",
            "      6        \u001b[36m0.9991\u001b[0m  0.0201\n",
            "      7        \u001b[36m0.9991\u001b[0m  0.0215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.9991\u001b[0m  0.0303\n",
            "      9        \u001b[36m0.9991\u001b[0m  0.0199\n",
            "     10        \u001b[36m0.9991\u001b[0m  0.0243\n",
            "     11        \u001b[36m0.9991\u001b[0m  0.0212\n",
            "     12        \u001b[36m0.9991\u001b[0m  0.0251\n",
            "     13        \u001b[36m0.9991\u001b[0m  0.0201\n",
            "     14        \u001b[36m0.9991\u001b[0m  0.0211\n",
            "     15        \u001b[36m0.9991\u001b[0m  0.0277\n",
            "     16        \u001b[36m0.9991\u001b[0m  0.0237\n",
            "     17        \u001b[36m0.9991\u001b[0m  0.0215\n",
            "     18        \u001b[36m0.9991\u001b[0m  0.0212\n",
            "     19        \u001b[36m0.9991\u001b[0m  0.0227\n",
            "     20        \u001b[36m0.9991\u001b[0m  0.0194\n",
            "     21        \u001b[36m0.9991\u001b[0m  0.0213\n",
            "     22        \u001b[36m0.9991\u001b[0m  0.0180\n",
            "     23        \u001b[36m0.9991\u001b[0m  0.0170\n",
            "     24        \u001b[36m0.9991\u001b[0m  0.0193\n",
            "     25        \u001b[36m0.9991\u001b[0m  0.0243\n",
            "     26        \u001b[36m0.9991\u001b[0m  0.0245\n",
            "     27        \u001b[36m0.9991\u001b[0m  0.0181\n",
            "     28        \u001b[36m0.9991\u001b[0m  0.0171\n",
            "     29        \u001b[36m0.9991\u001b[0m  0.0168\n",
            "     30        \u001b[36m0.9991\u001b[0m  0.0240\n",
            "     31        \u001b[36m0.9991\u001b[0m  0.0195\n",
            "     32        \u001b[36m0.9991\u001b[0m  0.0178\n",
            "     33        \u001b[36m0.9991\u001b[0m  0.0203\n",
            "     34        \u001b[36m0.9991\u001b[0m  0.0168\n",
            "     35        \u001b[36m0.9991\u001b[0m  0.0174\n",
            "     36        \u001b[36m0.9991\u001b[0m  0.0165\n",
            "     37        \u001b[36m0.9991\u001b[0m  0.0167\n",
            "     38        \u001b[36m0.9991\u001b[0m  0.0173\n",
            "     39        \u001b[36m0.9991\u001b[0m  0.0163\n",
            "     40        \u001b[36m0.9991\u001b[0m  0.0171\n",
            "     41        \u001b[36m0.9991\u001b[0m  0.0178\n",
            "     42        \u001b[36m0.9991\u001b[0m  0.0197\n",
            "     43        \u001b[36m0.9991\u001b[0m  0.0181\n",
            "     44        \u001b[36m0.9991\u001b[0m  0.0198\n",
            "     45        \u001b[36m0.9991\u001b[0m  0.0220\n",
            "     46        \u001b[36m0.9991\u001b[0m  0.0200\n",
            "     47        \u001b[36m0.9991\u001b[0m  0.0221\n",
            "     48        \u001b[36m0.9991\u001b[0m  0.0216\n",
            "     49        \u001b[36m0.9991\u001b[0m  0.0190\n",
            "     50        \u001b[36m0.9991\u001b[0m  0.0200\n",
            "     51        \u001b[36m0.9991\u001b[0m  0.0219\n",
            "     52        \u001b[36m0.9991\u001b[0m  0.0205\n",
            "     53        \u001b[36m0.9991\u001b[0m  0.0244\n",
            "     54        \u001b[36m0.9991\u001b[0m  0.0259\n",
            "     55        \u001b[36m0.9991\u001b[0m  0.0210\n",
            "     56        \u001b[36m0.9991\u001b[0m  0.0209\n",
            "     57        \u001b[36m0.9991\u001b[0m  0.0211\n",
            "     58        \u001b[36m0.9991\u001b[0m  0.0216\n",
            "     59        \u001b[36m0.9991\u001b[0m  0.0220\n",
            "     60        \u001b[36m0.9991\u001b[0m  0.0212\n",
            "     61        \u001b[36m0.9991\u001b[0m  0.0202\n",
            "     62        \u001b[36m0.9991\u001b[0m  0.0170\n",
            "     63        \u001b[36m0.9991\u001b[0m  0.0179\n",
            "     64        \u001b[36m0.9991\u001b[0m  0.0186\n",
            "     65        \u001b[36m0.9991\u001b[0m  0.0203\n",
            "     66        \u001b[36m0.9991\u001b[0m  0.0180\n",
            "     67        \u001b[36m0.9991\u001b[0m  0.0212\n",
            "     68        \u001b[36m0.9991\u001b[0m  0.0206\n",
            "     69        \u001b[36m0.9991\u001b[0m  0.0273\n",
            "     70        \u001b[36m0.9991\u001b[0m  0.0206\n",
            "     71        \u001b[36m0.9991\u001b[0m  0.0196\n",
            "     72        \u001b[36m0.9991\u001b[0m  0.0204\n",
            "     73        \u001b[36m0.9991\u001b[0m  0.0201\n",
            "     74        \u001b[36m0.9991\u001b[0m  0.0182\n",
            "     75        \u001b[36m0.9991\u001b[0m  0.0174\n",
            "     76        \u001b[36m0.9991\u001b[0m  0.0172\n",
            "     77        \u001b[36m0.9991\u001b[0m  0.0189\n",
            "     78        \u001b[36m0.9991\u001b[0m  0.0174\n",
            "     79        \u001b[36m0.9991\u001b[0m  0.0175\n",
            "     80        \u001b[36m0.9991\u001b[0m  0.0171\n",
            "     81        \u001b[36m0.9991\u001b[0m  0.0179\n",
            "     82        \u001b[36m0.9991\u001b[0m  0.0197\n",
            "     83        \u001b[36m0.9991\u001b[0m  0.0250\n",
            "     84        \u001b[36m0.9991\u001b[0m  0.0229\n",
            "     85        \u001b[36m0.9991\u001b[0m  0.0205\n",
            "     86        \u001b[36m0.9991\u001b[0m  0.0211\n",
            "     87        \u001b[36m0.9991\u001b[0m  0.0208\n",
            "     88        \u001b[36m0.9991\u001b[0m  0.0190\n",
            "     89        \u001b[36m0.9991\u001b[0m  0.0221\n",
            "     90        \u001b[36m0.9991\u001b[0m  0.0210\n",
            "     91        \u001b[36m0.9991\u001b[0m  0.0230\n",
            "     92        \u001b[36m0.9991\u001b[0m  0.0270\n",
            "     93        \u001b[36m0.9991\u001b[0m  0.0274\n",
            "     94        \u001b[36m0.9991\u001b[0m  0.0349\n",
            "     95        \u001b[36m0.9991\u001b[0m  0.0237\n",
            "     96        \u001b[36m0.9991\u001b[0m  0.0277\n",
            "     97        \u001b[36m0.9991\u001b[0m  0.0224\n",
            "     98        \u001b[36m0.9991\u001b[0m  0.0213\n",
            "     99        \u001b[36m0.9991\u001b[0m  0.0227\n",
            "    100        \u001b[36m0.9991\u001b[0m  0.0218\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.5749\u001b[0m  0.0315\n",
            "      2        \u001b[36m0.5638\u001b[0m  0.0228\n",
            "      3        \u001b[36m0.5517\u001b[0m  0.0225\n",
            "      4        \u001b[36m0.5452\u001b[0m  0.0246\n",
            "      5        \u001b[36m0.5372\u001b[0m  0.0231\n",
            "      6        \u001b[36m0.5295\u001b[0m  0.0237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.5213\u001b[0m  0.0281\n",
            "      8        \u001b[36m0.5151\u001b[0m  0.0290\n",
            "      9        \u001b[36m0.5091\u001b[0m  0.0306\n",
            "     10        \u001b[36m0.5033\u001b[0m  0.0242\n",
            "     11        \u001b[36m0.4981\u001b[0m  0.0233\n",
            "     12        \u001b[36m0.4933\u001b[0m  0.0249\n",
            "     13        \u001b[36m0.4887\u001b[0m  0.0227\n",
            "     14        \u001b[36m0.4842\u001b[0m  0.0228\n",
            "     15        \u001b[36m0.4800\u001b[0m  0.0227\n",
            "     16        \u001b[36m0.4759\u001b[0m  0.0223\n",
            "     17        0.4760  0.0240\n",
            "     18        \u001b[36m0.4722\u001b[0m  0.0268\n",
            "     19        \u001b[36m0.4685\u001b[0m  0.0250\n",
            "     20        \u001b[36m0.4650\u001b[0m  0.0246\n",
            "     21        \u001b[36m0.4615\u001b[0m  0.0252\n",
            "     22        \u001b[36m0.4581\u001b[0m  0.0253\n",
            "     23        \u001b[36m0.4549\u001b[0m  0.0249\n",
            "     24        \u001b[36m0.4518\u001b[0m  0.0263\n",
            "     25        \u001b[36m0.4488\u001b[0m  0.0260\n",
            "     26        \u001b[36m0.4461\u001b[0m  0.0257\n",
            "     27        \u001b[36m0.4403\u001b[0m  0.0261\n",
            "     28        \u001b[36m0.4378\u001b[0m  0.0258\n",
            "     29        \u001b[36m0.4354\u001b[0m  0.0283\n",
            "     30        \u001b[36m0.4332\u001b[0m  0.0271\n",
            "     31        \u001b[36m0.4311\u001b[0m  0.0283\n",
            "     32        \u001b[36m0.4295\u001b[0m  0.0255\n",
            "     33        \u001b[36m0.4284\u001b[0m  0.0293\n",
            "     34        \u001b[36m0.4270\u001b[0m  0.0298\n",
            "     35        \u001b[36m0.4256\u001b[0m  0.0342\n",
            "     36        \u001b[36m0.4240\u001b[0m  0.0254\n",
            "     37        \u001b[36m0.4226\u001b[0m  0.0247\n",
            "     38        \u001b[36m0.4213\u001b[0m  0.0225\n",
            "     39        0.4216  0.0235\n",
            "     40        \u001b[36m0.4188\u001b[0m  0.0234\n",
            "     41        \u001b[36m0.4176\u001b[0m  0.0343\n",
            "     42        \u001b[36m0.4166\u001b[0m  0.0235\n",
            "     43        \u001b[36m0.4156\u001b[0m  0.0351\n",
            "     44        \u001b[36m0.4146\u001b[0m  0.0313\n",
            "     45        \u001b[36m0.4136\u001b[0m  0.0240\n",
            "     46        \u001b[36m0.4127\u001b[0m  0.0214\n",
            "     47        \u001b[36m0.4119\u001b[0m  0.0214\n",
            "     48        \u001b[36m0.4110\u001b[0m  0.0227\n",
            "     49        \u001b[36m0.4102\u001b[0m  0.0233\n",
            "     50        \u001b[36m0.4094\u001b[0m  0.0245\n",
            "     51        \u001b[36m0.4086\u001b[0m  0.0214\n",
            "     52        \u001b[36m0.4079\u001b[0m  0.0236\n",
            "     53        \u001b[36m0.4071\u001b[0m  0.0230\n",
            "     54        \u001b[36m0.4064\u001b[0m  0.0229\n",
            "     55        \u001b[36m0.4057\u001b[0m  0.0241\n",
            "     56        \u001b[36m0.4049\u001b[0m  0.0228\n",
            "     57        \u001b[36m0.4042\u001b[0m  0.0232\n",
            "     58        \u001b[36m0.4035\u001b[0m  0.0260\n",
            "     59        \u001b[36m0.4028\u001b[0m  0.0259\n",
            "     60        \u001b[36m0.4020\u001b[0m  0.0243\n",
            "     61        \u001b[36m0.4013\u001b[0m  0.0284\n",
            "     62        \u001b[36m0.4006\u001b[0m  0.0317\n",
            "     63        \u001b[36m0.3998\u001b[0m  0.0260\n",
            "     64        \u001b[36m0.3991\u001b[0m  0.0261\n",
            "     65        \u001b[36m0.3983\u001b[0m  0.0275\n",
            "     66        \u001b[36m0.3976\u001b[0m  0.0237\n",
            "     67        \u001b[36m0.3968\u001b[0m  0.0234\n",
            "     68        \u001b[36m0.3961\u001b[0m  0.0256\n",
            "     69        \u001b[36m0.3954\u001b[0m  0.0225\n",
            "     70        \u001b[36m0.3947\u001b[0m  0.0228\n",
            "     71        \u001b[36m0.3940\u001b[0m  0.0226\n",
            "     72        \u001b[36m0.3933\u001b[0m  0.0281\n",
            "     73        \u001b[36m0.3927\u001b[0m  0.0219\n",
            "     74        \u001b[36m0.3921\u001b[0m  0.0214\n",
            "     75        \u001b[36m0.3915\u001b[0m  0.0216\n",
            "     76        \u001b[36m0.3908\u001b[0m  0.0240\n",
            "     77        \u001b[36m0.3902\u001b[0m  0.0220\n",
            "     78        \u001b[36m0.3896\u001b[0m  0.0248\n",
            "     79        \u001b[36m0.3891\u001b[0m  0.0224\n",
            "     80        \u001b[36m0.3886\u001b[0m  0.0253\n",
            "     81        \u001b[36m0.3881\u001b[0m  0.0238\n",
            "     82        \u001b[36m0.3877\u001b[0m  0.0256\n",
            "     83        \u001b[36m0.3873\u001b[0m  0.0226\n",
            "     84        \u001b[36m0.3868\u001b[0m  0.0210\n",
            "     85        \u001b[36m0.3864\u001b[0m  0.0205\n",
            "     86        \u001b[36m0.3861\u001b[0m  0.0275\n",
            "     87        \u001b[36m0.3857\u001b[0m  0.0214\n",
            "     88        \u001b[36m0.3853\u001b[0m  0.0224\n",
            "     89        \u001b[36m0.3850\u001b[0m  0.0242\n",
            "     90        \u001b[36m0.3836\u001b[0m  0.0250\n",
            "     91        \u001b[36m0.3818\u001b[0m  0.0266\n",
            "     92        \u001b[36m0.3816\u001b[0m  0.0255\n",
            "     93        \u001b[36m0.3814\u001b[0m  0.0280\n",
            "     94        \u001b[36m0.3812\u001b[0m  0.0267\n",
            "     95        \u001b[36m0.3810\u001b[0m  0.0266\n",
            "     96        \u001b[36m0.3808\u001b[0m  0.0289\n",
            "     97        \u001b[36m0.3806\u001b[0m  0.0282\n",
            "     98        \u001b[36m0.3804\u001b[0m  0.0265\n",
            "     99        \u001b[36m0.3802\u001b[0m  0.0261\n",
            "    100        \u001b[36m0.3801\u001b[0m  0.0271\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7904\u001b[0m  0.0255\n",
            "      2        \u001b[36m0.7805\u001b[0m  0.0256\n",
            "      3        \u001b[36m0.7682\u001b[0m  0.0311\n",
            "      4        \u001b[36m0.7556\u001b[0m  0.0228\n",
            "      5        \u001b[36m0.7443\u001b[0m  0.0227\n",
            "      6        \u001b[36m0.7354\u001b[0m  0.0234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.7276\u001b[0m  0.0254\n",
            "      8        \u001b[36m0.7198\u001b[0m  0.0226\n",
            "      9        \u001b[36m0.7126\u001b[0m  0.0216\n",
            "     10        \u001b[36m0.7049\u001b[0m  0.0227\n",
            "     11        \u001b[36m0.6989\u001b[0m  0.0230\n",
            "     12        \u001b[36m0.6903\u001b[0m  0.0234\n",
            "     13        \u001b[36m0.6810\u001b[0m  0.0237\n",
            "     14        \u001b[36m0.6711\u001b[0m  0.0276\n",
            "     15        \u001b[36m0.6605\u001b[0m  0.0254\n",
            "     16        \u001b[36m0.6495\u001b[0m  0.0249\n",
            "     17        \u001b[36m0.6381\u001b[0m  0.0316\n",
            "     18        \u001b[36m0.6266\u001b[0m  0.0229\n",
            "     19        \u001b[36m0.6148\u001b[0m  0.0267\n",
            "     20        \u001b[36m0.6037\u001b[0m  0.0234\n",
            "     21        \u001b[36m0.5930\u001b[0m  0.0220\n",
            "     22        \u001b[36m0.5828\u001b[0m  0.0236\n",
            "     23        \u001b[36m0.5731\u001b[0m  0.0241\n",
            "     24        \u001b[36m0.5639\u001b[0m  0.0256\n",
            "     25        \u001b[36m0.5553\u001b[0m  0.0272\n",
            "     26        \u001b[36m0.5472\u001b[0m  0.0278\n",
            "     27        \u001b[36m0.5396\u001b[0m  0.0273\n",
            "     28        \u001b[36m0.5321\u001b[0m  0.0264\n",
            "     29        \u001b[36m0.5255\u001b[0m  0.0280\n",
            "     30        \u001b[36m0.5193\u001b[0m  0.0275\n",
            "     31        \u001b[36m0.5135\u001b[0m  0.0284\n",
            "     32        \u001b[36m0.5081\u001b[0m  0.0258\n",
            "     33        \u001b[36m0.5031\u001b[0m  0.0308\n",
            "     34        \u001b[36m0.4981\u001b[0m  0.0260\n",
            "     35        \u001b[36m0.4935\u001b[0m  0.0276\n",
            "     36        \u001b[36m0.4894\u001b[0m  0.0234\n",
            "     37        \u001b[36m0.4855\u001b[0m  0.0227\n",
            "     38        \u001b[36m0.4819\u001b[0m  0.0224\n",
            "     39        \u001b[36m0.4785\u001b[0m  0.0217\n",
            "     40        \u001b[36m0.4750\u001b[0m  0.0223\n",
            "     41        \u001b[36m0.4717\u001b[0m  0.0235\n",
            "     42        \u001b[36m0.4688\u001b[0m  0.0217\n",
            "     43        \u001b[36m0.4661\u001b[0m  0.0221\n",
            "     44        \u001b[36m0.4640\u001b[0m  0.0297\n",
            "     45        \u001b[36m0.4615\u001b[0m  0.0248\n",
            "     46        \u001b[36m0.4592\u001b[0m  0.0229\n",
            "     47        \u001b[36m0.4570\u001b[0m  0.0229\n",
            "     48        \u001b[36m0.4548\u001b[0m  0.0235\n",
            "     49        \u001b[36m0.4528\u001b[0m  0.0236\n",
            "     50        \u001b[36m0.4508\u001b[0m  0.0219\n",
            "     51        \u001b[36m0.4490\u001b[0m  0.0219\n",
            "     52        \u001b[36m0.4472\u001b[0m  0.0307\n",
            "     53        \u001b[36m0.4454\u001b[0m  0.0385\n",
            "     54        \u001b[36m0.4437\u001b[0m  0.0259\n",
            "     55        \u001b[36m0.4421\u001b[0m  0.0244\n",
            "     56        \u001b[36m0.4405\u001b[0m  0.0222\n",
            "     57        \u001b[36m0.4389\u001b[0m  0.0229\n",
            "     58        \u001b[36m0.4375\u001b[0m  0.0249\n",
            "     59        \u001b[36m0.4360\u001b[0m  0.0255\n",
            "     60        \u001b[36m0.4343\u001b[0m  0.0246\n",
            "     61        \u001b[36m0.4329\u001b[0m  0.0264\n",
            "     62        \u001b[36m0.4317\u001b[0m  0.0265\n",
            "     63        \u001b[36m0.4304\u001b[0m  0.0280\n",
            "     64        \u001b[36m0.4292\u001b[0m  0.0286\n",
            "     65        \u001b[36m0.4280\u001b[0m  0.0249\n",
            "     66        \u001b[36m0.4267\u001b[0m  0.0257\n",
            "     67        \u001b[36m0.4256\u001b[0m  0.0243\n",
            "     68        \u001b[36m0.4245\u001b[0m  0.0249\n",
            "     69        \u001b[36m0.4234\u001b[0m  0.0246\n",
            "     70        \u001b[36m0.4224\u001b[0m  0.0237\n",
            "     71        \u001b[36m0.4206\u001b[0m  0.0224\n",
            "     72        \u001b[36m0.4197\u001b[0m  0.0249\n",
            "     73        \u001b[36m0.4184\u001b[0m  0.0223\n",
            "     74        \u001b[36m0.4173\u001b[0m  0.0208\n",
            "     75        \u001b[36m0.4162\u001b[0m  0.0234\n",
            "     76        \u001b[36m0.4152\u001b[0m  0.0251\n",
            "     77        \u001b[36m0.4141\u001b[0m  0.0218\n",
            "     78        \u001b[36m0.4131\u001b[0m  0.0216\n",
            "     79        \u001b[36m0.4120\u001b[0m  0.0213\n",
            "     80        \u001b[36m0.4109\u001b[0m  0.0228\n",
            "     81        \u001b[36m0.4098\u001b[0m  0.0207\n",
            "     82        \u001b[36m0.4087\u001b[0m  0.0220\n",
            "     83        \u001b[36m0.4078\u001b[0m  0.0214\n",
            "     84        \u001b[36m0.4069\u001b[0m  0.0217\n",
            "     85        \u001b[36m0.4060\u001b[0m  0.0210\n",
            "     86        \u001b[36m0.4051\u001b[0m  0.0202\n",
            "     87        \u001b[36m0.4044\u001b[0m  0.0216\n",
            "     88        \u001b[36m0.4036\u001b[0m  0.0248\n",
            "     89        \u001b[36m0.4029\u001b[0m  0.0273\n",
            "     90        \u001b[36m0.4022\u001b[0m  0.0231\n",
            "     91        \u001b[36m0.4015\u001b[0m  0.0234\n",
            "     92        \u001b[36m0.4009\u001b[0m  0.0220\n",
            "     93        \u001b[36m0.3994\u001b[0m  0.0236\n",
            "     94        \u001b[36m0.3989\u001b[0m  0.0255\n",
            "     95        \u001b[36m0.3983\u001b[0m  0.0251\n",
            "     96        \u001b[36m0.3978\u001b[0m  0.0236\n",
            "     97        \u001b[36m0.3973\u001b[0m  0.0251\n",
            "     98        \u001b[36m0.3968\u001b[0m  0.0264\n",
            "     99        \u001b[36m0.3963\u001b[0m  0.0281\n",
            "    100        \u001b[36m0.3958\u001b[0m  0.0240\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7870\u001b[0m  0.0199\n",
            "      2        \u001b[36m0.7856\u001b[0m  0.0213\n",
            "      3        \u001b[36m0.7849\u001b[0m  0.0216\n",
            "      4        \u001b[36m0.7842\u001b[0m  0.0190\n",
            "      5        \u001b[36m0.7835\u001b[0m  0.0258\n",
            "      6        \u001b[36m0.7828\u001b[0m  0.0200\n",
            "      7        \u001b[36m0.7821\u001b[0m  0.0239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.7813\u001b[0m  0.0189\n",
            "      9        \u001b[36m0.7806\u001b[0m  0.0170\n",
            "     10        \u001b[36m0.7799\u001b[0m  0.0164\n",
            "     11        \u001b[36m0.7791\u001b[0m  0.0163\n",
            "     12        \u001b[36m0.7784\u001b[0m  0.0169\n",
            "     13        \u001b[36m0.7776\u001b[0m  0.0174\n",
            "     14        \u001b[36m0.7769\u001b[0m  0.0174\n",
            "     15        \u001b[36m0.7761\u001b[0m  0.0172\n",
            "     16        \u001b[36m0.7754\u001b[0m  0.0169\n",
            "     17        \u001b[36m0.7746\u001b[0m  0.0164\n",
            "     18        \u001b[36m0.7739\u001b[0m  0.0197\n",
            "     19        \u001b[36m0.7731\u001b[0m  0.0178\n",
            "     20        \u001b[36m0.7723\u001b[0m  0.0169\n",
            "     21        \u001b[36m0.7715\u001b[0m  0.0207\n",
            "     22        \u001b[36m0.7708\u001b[0m  0.0159\n",
            "     23        \u001b[36m0.7700\u001b[0m  0.0173\n",
            "     24        \u001b[36m0.7692\u001b[0m  0.0190\n",
            "     25        \u001b[36m0.7684\u001b[0m  0.0179\n",
            "     26        \u001b[36m0.7676\u001b[0m  0.0180\n",
            "     27        \u001b[36m0.7668\u001b[0m  0.0182\n",
            "     28        \u001b[36m0.7660\u001b[0m  0.0215\n",
            "     29        \u001b[36m0.7652\u001b[0m  0.0237\n",
            "     30        \u001b[36m0.7644\u001b[0m  0.0269\n",
            "     31        \u001b[36m0.7636\u001b[0m  0.0199\n",
            "     32        \u001b[36m0.7628\u001b[0m  0.0224\n",
            "     33        \u001b[36m0.7620\u001b[0m  0.0232\n",
            "     34        \u001b[36m0.7611\u001b[0m  0.0244\n",
            "     35        \u001b[36m0.7603\u001b[0m  0.0218\n",
            "     36        \u001b[36m0.7595\u001b[0m  0.0210\n",
            "     37        \u001b[36m0.7587\u001b[0m  0.0216\n",
            "     38        \u001b[36m0.7578\u001b[0m  0.0215\n",
            "     39        \u001b[36m0.7570\u001b[0m  0.0253\n",
            "     40        \u001b[36m0.7561\u001b[0m  0.0205\n",
            "     41        \u001b[36m0.7553\u001b[0m  0.0201\n",
            "     42        \u001b[36m0.7544\u001b[0m  0.0191\n",
            "     43        \u001b[36m0.7536\u001b[0m  0.0208\n",
            "     44        \u001b[36m0.7527\u001b[0m  0.0228\n",
            "     45        \u001b[36m0.7519\u001b[0m  0.0190\n",
            "     46        \u001b[36m0.7510\u001b[0m  0.0190\n",
            "     47        \u001b[36m0.7501\u001b[0m  0.0184\n",
            "     48        \u001b[36m0.7493\u001b[0m  0.0182\n",
            "     49        \u001b[36m0.7484\u001b[0m  0.0185\n",
            "     50        \u001b[36m0.7475\u001b[0m  0.0209\n",
            "     51        \u001b[36m0.7467\u001b[0m  0.0209\n",
            "     52        \u001b[36m0.7458\u001b[0m  0.0205\n",
            "     53        \u001b[36m0.7449\u001b[0m  0.0215\n",
            "     54        \u001b[36m0.7440\u001b[0m  0.0202\n",
            "     55        \u001b[36m0.7431\u001b[0m  0.0197\n",
            "     56        \u001b[36m0.7422\u001b[0m  0.0212\n",
            "     57        \u001b[36m0.7413\u001b[0m  0.0203\n",
            "     58        \u001b[36m0.7404\u001b[0m  0.0181\n",
            "     59        \u001b[36m0.7395\u001b[0m  0.0206\n",
            "     60        \u001b[36m0.7386\u001b[0m  0.0217\n",
            "     61        \u001b[36m0.7377\u001b[0m  0.0184\n",
            "     62        \u001b[36m0.7368\u001b[0m  0.0194\n",
            "     63        \u001b[36m0.7359\u001b[0m  0.0192\n",
            "     64        \u001b[36m0.7350\u001b[0m  0.0245\n",
            "     65        \u001b[36m0.7341\u001b[0m  0.0176\n",
            "     66        \u001b[36m0.7331\u001b[0m  0.0175\n",
            "     67        \u001b[36m0.7322\u001b[0m  0.0201\n",
            "     68        \u001b[36m0.7313\u001b[0m  0.0198\n",
            "     69        \u001b[36m0.7304\u001b[0m  0.0219\n",
            "     70        \u001b[36m0.7294\u001b[0m  0.0301\n",
            "     71        \u001b[36m0.7285\u001b[0m  0.0273\n",
            "     72        \u001b[36m0.7276\u001b[0m  0.0229\n",
            "     73        \u001b[36m0.7266\u001b[0m  0.0221\n",
            "     74        \u001b[36m0.7257\u001b[0m  0.0201\n",
            "     75        \u001b[36m0.7247\u001b[0m  0.0224\n",
            "     76        \u001b[36m0.7238\u001b[0m  0.0250\n",
            "     77        \u001b[36m0.7229\u001b[0m  0.0230\n",
            "     78        \u001b[36m0.7219\u001b[0m  0.0214\n",
            "     79        \u001b[36m0.7210\u001b[0m  0.0241\n",
            "     80        \u001b[36m0.7200\u001b[0m  0.0239\n",
            "     81        \u001b[36m0.7191\u001b[0m  0.0164\n",
            "     82        \u001b[36m0.7181\u001b[0m  0.0234\n",
            "     83        \u001b[36m0.7171\u001b[0m  0.0168\n",
            "     84        \u001b[36m0.7162\u001b[0m  0.0166\n",
            "     85        \u001b[36m0.7152\u001b[0m  0.0173\n",
            "     86        \u001b[36m0.7143\u001b[0m  0.0190\n",
            "     87        \u001b[36m0.7133\u001b[0m  0.0169\n",
            "     88        \u001b[36m0.7123\u001b[0m  0.0252\n",
            "     89        \u001b[36m0.7114\u001b[0m  0.0193\n",
            "     90        \u001b[36m0.7104\u001b[0m  0.0176\n",
            "     91        \u001b[36m0.7094\u001b[0m  0.0177\n",
            "     92        \u001b[36m0.7085\u001b[0m  0.0206\n",
            "     93        \u001b[36m0.7075\u001b[0m  0.0183\n",
            "     94        \u001b[36m0.7065\u001b[0m  0.0184\n",
            "     95        \u001b[36m0.7055\u001b[0m  0.0195\n",
            "     96        \u001b[36m0.7046\u001b[0m  0.0184\n",
            "     97        \u001b[36m0.7036\u001b[0m  0.0192\n",
            "     98        \u001b[36m0.7026\u001b[0m  0.0178\n",
            "     99        \u001b[36m0.7016\u001b[0m  0.0187\n",
            "    100        \u001b[36m0.7007\u001b[0m  0.0167\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.7910\u001b[0m  0.0183\n",
            "      2        \u001b[36m0.7903\u001b[0m  0.0189\n",
            "      3        \u001b[36m0.7898\u001b[0m  0.0189\n",
            "      4        \u001b[36m0.7892\u001b[0m  0.0194\n",
            "      5        \u001b[36m0.7887\u001b[0m  0.0190\n",
            "      6        \u001b[36m0.7881\u001b[0m  0.0222\n",
            "      7        \u001b[36m0.7876\u001b[0m  0.0194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.7870\u001b[0m  0.0289\n",
            "      9        \u001b[36m0.7865\u001b[0m  0.0222\n",
            "     10        \u001b[36m0.7859\u001b[0m  0.0223\n",
            "     11        \u001b[36m0.7853\u001b[0m  0.0225\n",
            "     12        \u001b[36m0.7847\u001b[0m  0.0190\n",
            "     13        \u001b[36m0.7842\u001b[0m  0.0209\n",
            "     14        \u001b[36m0.7836\u001b[0m  0.0193\n",
            "     15        \u001b[36m0.7830\u001b[0m  0.0207\n",
            "     16        \u001b[36m0.7824\u001b[0m  0.0244\n",
            "     17        \u001b[36m0.7818\u001b[0m  0.0251\n",
            "     18        \u001b[36m0.7812\u001b[0m  0.0192\n",
            "     19        \u001b[36m0.7807\u001b[0m  0.0193\n",
            "     20        \u001b[36m0.7801\u001b[0m  0.0182\n",
            "     21        \u001b[36m0.7795\u001b[0m  0.0177\n",
            "     22        \u001b[36m0.7789\u001b[0m  0.0201\n",
            "     23        \u001b[36m0.7783\u001b[0m  0.0175\n",
            "     24        \u001b[36m0.7776\u001b[0m  0.0175\n",
            "     25        \u001b[36m0.7770\u001b[0m  0.0189\n",
            "     26        \u001b[36m0.7764\u001b[0m  0.0173\n",
            "     27        \u001b[36m0.7758\u001b[0m  0.0169\n",
            "     28        \u001b[36m0.7752\u001b[0m  0.0173\n",
            "     29        \u001b[36m0.7746\u001b[0m  0.0186\n",
            "     30        \u001b[36m0.7739\u001b[0m  0.0171\n",
            "     31        \u001b[36m0.7733\u001b[0m  0.0198\n",
            "     32        \u001b[36m0.7727\u001b[0m  0.0175\n",
            "     33        \u001b[36m0.7718\u001b[0m  0.0170\n",
            "     34        \u001b[36m0.7711\u001b[0m  0.0171\n",
            "     35        \u001b[36m0.7700\u001b[0m  0.0183\n",
            "     36        \u001b[36m0.7693\u001b[0m  0.0223\n",
            "     37        \u001b[36m0.7687\u001b[0m  0.0166\n",
            "     38        \u001b[36m0.7680\u001b[0m  0.0166\n",
            "     39        \u001b[36m0.7674\u001b[0m  0.0175\n",
            "     40        \u001b[36m0.7667\u001b[0m  0.0208\n",
            "     41        \u001b[36m0.7660\u001b[0m  0.0167\n",
            "     42        \u001b[36m0.7654\u001b[0m  0.0201\n",
            "     43        \u001b[36m0.7647\u001b[0m  0.0171\n",
            "     44        \u001b[36m0.7640\u001b[0m  0.0182\n",
            "     45        \u001b[36m0.7634\u001b[0m  0.0171\n",
            "     46        \u001b[36m0.7627\u001b[0m  0.0178\n",
            "     47        \u001b[36m0.7620\u001b[0m  0.0192\n",
            "     48        \u001b[36m0.7613\u001b[0m  0.0297\n",
            "     49        \u001b[36m0.7606\u001b[0m  0.0213\n",
            "     50        \u001b[36m0.7600\u001b[0m  0.0241\n",
            "     51        \u001b[36m0.7593\u001b[0m  0.0347\n",
            "     52        \u001b[36m0.7586\u001b[0m  0.0237\n",
            "     53        \u001b[36m0.7579\u001b[0m  0.0221\n",
            "     54        \u001b[36m0.7572\u001b[0m  0.0211\n",
            "     55        \u001b[36m0.7565\u001b[0m  0.0220\n",
            "     56        \u001b[36m0.7558\u001b[0m  0.0239\n",
            "     57        \u001b[36m0.7551\u001b[0m  0.0218\n",
            "     58        \u001b[36m0.7543\u001b[0m  0.0236\n",
            "     59        \u001b[36m0.7536\u001b[0m  0.0254\n",
            "     60        \u001b[36m0.7529\u001b[0m  0.0217\n",
            "     61        \u001b[36m0.7522\u001b[0m  0.0256\n",
            "     62        \u001b[36m0.7515\u001b[0m  0.0205\n",
            "     63        \u001b[36m0.7507\u001b[0m  0.0231\n",
            "     64        \u001b[36m0.7500\u001b[0m  0.0176\n",
            "     65        \u001b[36m0.7493\u001b[0m  0.0178\n",
            "     66        \u001b[36m0.7486\u001b[0m  0.0177\n",
            "     67        \u001b[36m0.7478\u001b[0m  0.0213\n",
            "     68        \u001b[36m0.7471\u001b[0m  0.0191\n",
            "     69        \u001b[36m0.7463\u001b[0m  0.0233\n",
            "     70        \u001b[36m0.7456\u001b[0m  0.0198\n",
            "     71        \u001b[36m0.7448\u001b[0m  0.0229\n",
            "     72        \u001b[36m0.7441\u001b[0m  0.0191\n",
            "     73        \u001b[36m0.7433\u001b[0m  0.0195\n",
            "     74        \u001b[36m0.7426\u001b[0m  0.0210\n",
            "     75        \u001b[36m0.7418\u001b[0m  0.0198\n",
            "     76        \u001b[36m0.7411\u001b[0m  0.0186\n",
            "     77        \u001b[36m0.7403\u001b[0m  0.0215\n",
            "     78        \u001b[36m0.7395\u001b[0m  0.0183\n",
            "     79        \u001b[36m0.7387\u001b[0m  0.0210\n",
            "     80        \u001b[36m0.7380\u001b[0m  0.0194\n",
            "     81        \u001b[36m0.7372\u001b[0m  0.0272\n",
            "     82        \u001b[36m0.7364\u001b[0m  0.0206\n",
            "     83        \u001b[36m0.7356\u001b[0m  0.0192\n",
            "     84        \u001b[36m0.7349\u001b[0m  0.0188\n",
            "     85        \u001b[36m0.7341\u001b[0m  0.0178\n",
            "     86        \u001b[36m0.7333\u001b[0m  0.0194\n",
            "     87        \u001b[36m0.7325\u001b[0m  0.0210\n",
            "     88        \u001b[36m0.7317\u001b[0m  0.0224\n",
            "     89        \u001b[36m0.7309\u001b[0m  0.0205\n",
            "     90        \u001b[36m0.7301\u001b[0m  0.0274\n",
            "     91        \u001b[36m0.7293\u001b[0m  0.0228\n",
            "     92        \u001b[36m0.7285\u001b[0m  0.0266\n",
            "     93        \u001b[36m0.7277\u001b[0m  0.0202\n",
            "     94        \u001b[36m0.7269\u001b[0m  0.0200\n",
            "     95        \u001b[36m0.7261\u001b[0m  0.0202\n",
            "     96        \u001b[36m0.7253\u001b[0m  0.0208\n",
            "     97        \u001b[36m0.7244\u001b[0m  0.0213\n",
            "     98        \u001b[36m0.7236\u001b[0m  0.0188\n",
            "     99        \u001b[36m0.7228\u001b[0m  0.0181\n",
            "    100        \u001b[36m0.7220\u001b[0m  0.0169\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8222\u001b[0m  0.0243\n",
            "      2        \u001b[36m0.8038\u001b[0m  0.0229\n",
            "      3        \u001b[36m0.7736\u001b[0m  0.0316\n",
            "      4        \u001b[36m0.7366\u001b[0m  0.0297\n",
            "      5        \u001b[36m0.7006\u001b[0m  0.0231\n",
            "      6        \u001b[36m0.6642\u001b[0m  0.0252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.6263\u001b[0m  0.0258\n",
            "      8        \u001b[36m0.5901\u001b[0m  0.0224\n",
            "      9        \u001b[36m0.5592\u001b[0m  0.0230\n",
            "     10        \u001b[36m0.5340\u001b[0m  0.0255\n",
            "     11        \u001b[36m0.5153\u001b[0m  0.0244\n",
            "     12        \u001b[36m0.5005\u001b[0m  0.0233\n",
            "     13        \u001b[36m0.4885\u001b[0m  0.0244\n",
            "     14        \u001b[36m0.4767\u001b[0m  0.0274\n",
            "     15        \u001b[36m0.4683\u001b[0m  0.0247\n",
            "     16        \u001b[36m0.4586\u001b[0m  0.0281\n",
            "     17        \u001b[36m0.4518\u001b[0m  0.0263\n",
            "     18        \u001b[36m0.4456\u001b[0m  0.0277\n",
            "     19        \u001b[36m0.4397\u001b[0m  0.0288\n",
            "     20        \u001b[36m0.4340\u001b[0m  0.0291\n",
            "     21        \u001b[36m0.4286\u001b[0m  0.0269\n",
            "     22        \u001b[36m0.4244\u001b[0m  0.0290\n",
            "     23        \u001b[36m0.4208\u001b[0m  0.0280\n",
            "     24        \u001b[36m0.4178\u001b[0m  0.0286\n",
            "     25        \u001b[36m0.4157\u001b[0m  0.0312\n",
            "     26        \u001b[36m0.4132\u001b[0m  0.0327\n",
            "     27        \u001b[36m0.4128\u001b[0m  0.0271\n",
            "     28        \u001b[36m0.4097\u001b[0m  0.0285\n",
            "     29        \u001b[36m0.4082\u001b[0m  0.0242\n",
            "     30        \u001b[36m0.4069\u001b[0m  0.0265\n",
            "     31        \u001b[36m0.4057\u001b[0m  0.0324\n",
            "     32        \u001b[36m0.4047\u001b[0m  0.0248\n",
            "     33        \u001b[36m0.4037\u001b[0m  0.0244\n",
            "     34        \u001b[36m0.4028\u001b[0m  0.0334\n",
            "     35        \u001b[36m0.4019\u001b[0m  0.0288\n",
            "     36        \u001b[36m0.4011\u001b[0m  0.0239\n",
            "     37        \u001b[36m0.4004\u001b[0m  0.0259\n",
            "     38        \u001b[36m0.3997\u001b[0m  0.0248\n",
            "     39        \u001b[36m0.3990\u001b[0m  0.0234\n",
            "     40        \u001b[36m0.3985\u001b[0m  0.0230\n",
            "     41        \u001b[36m0.3980\u001b[0m  0.0277\n",
            "     42        \u001b[36m0.3974\u001b[0m  0.0262\n",
            "     43        \u001b[36m0.3969\u001b[0m  0.0350\n",
            "     44        \u001b[36m0.3964\u001b[0m  0.0248\n",
            "     45        \u001b[36m0.3959\u001b[0m  0.0291\n",
            "     46        \u001b[36m0.3954\u001b[0m  0.0292\n",
            "     47        \u001b[36m0.3950\u001b[0m  0.0247\n",
            "     48        \u001b[36m0.3945\u001b[0m  0.0252\n",
            "     49        \u001b[36m0.3939\u001b[0m  0.0275\n",
            "     50        \u001b[36m0.3934\u001b[0m  0.0271\n",
            "     51        \u001b[36m0.3931\u001b[0m  0.0281\n",
            "     52        \u001b[36m0.3927\u001b[0m  0.0268\n",
            "     53        \u001b[36m0.3924\u001b[0m  0.0322\n",
            "     54        \u001b[36m0.3921\u001b[0m  0.0292\n",
            "     55        \u001b[36m0.3918\u001b[0m  0.0265\n",
            "     56        \u001b[36m0.3915\u001b[0m  0.0311\n",
            "     57        \u001b[36m0.3913\u001b[0m  0.0421\n",
            "     58        \u001b[36m0.3909\u001b[0m  0.0380\n",
            "     59        0.3921  0.0296\n",
            "     60        0.3918  0.0255\n",
            "     61        0.3915  0.0334\n",
            "     62        0.3911  0.0312\n",
            "     63        \u001b[36m0.3908\u001b[0m  0.0254\n",
            "     64        \u001b[36m0.3893\u001b[0m  0.0306\n",
            "     65        \u001b[36m0.3889\u001b[0m  0.0244\n",
            "     66        \u001b[36m0.3881\u001b[0m  0.0335\n",
            "     67        \u001b[36m0.3876\u001b[0m  0.0294\n",
            "     68        \u001b[36m0.3870\u001b[0m  0.0248\n",
            "     69        \u001b[36m0.3865\u001b[0m  0.0255\n",
            "     70        \u001b[36m0.3858\u001b[0m  0.0260\n",
            "     71        \u001b[36m0.3849\u001b[0m  0.0254\n",
            "     72        \u001b[36m0.3840\u001b[0m  0.0234\n",
            "     73        \u001b[36m0.3830\u001b[0m  0.0300\n",
            "     74        \u001b[36m0.3821\u001b[0m  0.0259\n",
            "     75        \u001b[36m0.3813\u001b[0m  0.0255\n",
            "     76        \u001b[36m0.3806\u001b[0m  0.0297\n",
            "     77        \u001b[36m0.3800\u001b[0m  0.0252\n",
            "     78        \u001b[36m0.3795\u001b[0m  0.0232\n",
            "     79        \u001b[36m0.3791\u001b[0m  0.0245\n",
            "     80        \u001b[36m0.3788\u001b[0m  0.0281\n",
            "     81        \u001b[36m0.3784\u001b[0m  0.0330\n",
            "     82        \u001b[36m0.3782\u001b[0m  0.0285\n",
            "     83        \u001b[36m0.3780\u001b[0m  0.0320\n",
            "     84        \u001b[36m0.3778\u001b[0m  0.0356\n",
            "     85        \u001b[36m0.3777\u001b[0m  0.0285\n",
            "     86        \u001b[36m0.3775\u001b[0m  0.0285\n",
            "     87        \u001b[36m0.3774\u001b[0m  0.0314\n",
            "     88        \u001b[36m0.3773\u001b[0m  0.0298\n",
            "     89        \u001b[36m0.3772\u001b[0m  0.0366\n",
            "     90        \u001b[36m0.3771\u001b[0m  0.0307\n",
            "     91        \u001b[36m0.3770\u001b[0m  0.0230\n",
            "     92        \u001b[36m0.3769\u001b[0m  0.0242\n",
            "     93        \u001b[36m0.3768\u001b[0m  0.0261\n",
            "     94        \u001b[36m0.3767\u001b[0m  0.0304\n",
            "     95        \u001b[36m0.3766\u001b[0m  0.0267\n",
            "     96        \u001b[36m0.3766\u001b[0m  0.0248\n",
            "     97        \u001b[36m0.3765\u001b[0m  0.0268\n",
            "     98        \u001b[36m0.3764\u001b[0m  0.0264\n",
            "     99        \u001b[36m0.3764\u001b[0m  0.0230\n",
            "    100        \u001b[36m0.3763\u001b[0m  0.0233\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.8552\u001b[0m  0.0235\n",
            "      2        \u001b[36m0.8382\u001b[0m  0.0226\n",
            "      3        \u001b[36m0.8240\u001b[0m  0.0226\n",
            "      4        \u001b[36m0.8091\u001b[0m  0.0294\n",
            "      5        \u001b[36m0.7935\u001b[0m  0.0265\n",
            "      6        \u001b[36m0.7817\u001b[0m  0.0215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      7        \u001b[36m0.7693\u001b[0m  0.0233\n",
            "      8        \u001b[36m0.7557\u001b[0m  0.0264\n",
            "      9        \u001b[36m0.7444\u001b[0m  0.0320\n",
            "     10        \u001b[36m0.7308\u001b[0m  0.0275\n",
            "     11        \u001b[36m0.7165\u001b[0m  0.0313\n",
            "     12        \u001b[36m0.7005\u001b[0m  0.0275\n",
            "     13        \u001b[36m0.6867\u001b[0m  0.0293\n",
            "     14        \u001b[36m0.6699\u001b[0m  0.0262\n",
            "     15        \u001b[36m0.6423\u001b[0m  0.0277\n",
            "     16        \u001b[36m0.6061\u001b[0m  0.0335\n",
            "     17        \u001b[36m0.5554\u001b[0m  0.0271\n",
            "     18        \u001b[36m0.5052\u001b[0m  0.0279\n",
            "     19        \u001b[36m0.4730\u001b[0m  0.0307\n",
            "     20        \u001b[36m0.4550\u001b[0m  0.0255\n",
            "     21        \u001b[36m0.4435\u001b[0m  0.0304\n",
            "     22        \u001b[36m0.4350\u001b[0m  0.0366\n",
            "     23        \u001b[36m0.4280\u001b[0m  0.0292\n",
            "     24        \u001b[36m0.4225\u001b[0m  0.0273\n",
            "     25        \u001b[36m0.4180\u001b[0m  0.0230\n",
            "     26        \u001b[36m0.4142\u001b[0m  0.0238\n",
            "     27        \u001b[36m0.4111\u001b[0m  0.0238\n",
            "     28        \u001b[36m0.4085\u001b[0m  0.0264\n",
            "     29        \u001b[36m0.4061\u001b[0m  0.0249\n",
            "     30        \u001b[36m0.4042\u001b[0m  0.0324\n",
            "     31        \u001b[36m0.4023\u001b[0m  0.0255\n",
            "     32        \u001b[36m0.3998\u001b[0m  0.0233\n",
            "     33        \u001b[36m0.3985\u001b[0m  0.0223\n",
            "     34        \u001b[36m0.3973\u001b[0m  0.0267\n",
            "     35        \u001b[36m0.3961\u001b[0m  0.0232\n",
            "     36        \u001b[36m0.3950\u001b[0m  0.0246\n",
            "     37        \u001b[36m0.3940\u001b[0m  0.0249\n",
            "     38        \u001b[36m0.3935\u001b[0m  0.0270\n",
            "     39        \u001b[36m0.3924\u001b[0m  0.0271\n",
            "     40        \u001b[36m0.3915\u001b[0m  0.0284\n",
            "     41        \u001b[36m0.3906\u001b[0m  0.0330\n",
            "     42        \u001b[36m0.3898\u001b[0m  0.0256\n",
            "     43        \u001b[36m0.3891\u001b[0m  0.0314\n",
            "     44        \u001b[36m0.3888\u001b[0m  0.0304\n",
            "     45        \u001b[36m0.3881\u001b[0m  0.0300\n",
            "     46        \u001b[36m0.3874\u001b[0m  0.0256\n",
            "     47        \u001b[36m0.3868\u001b[0m  0.0265\n",
            "     48        \u001b[36m0.3862\u001b[0m  0.0265\n",
            "     49        \u001b[36m0.3857\u001b[0m  0.0375\n",
            "     50        \u001b[36m0.3847\u001b[0m  0.0284\n",
            "     51        \u001b[36m0.3841\u001b[0m  0.0220\n",
            "     52        \u001b[36m0.3825\u001b[0m  0.0241\n",
            "     53        \u001b[36m0.3822\u001b[0m  0.0265\n",
            "     54        \u001b[36m0.3819\u001b[0m  0.0217\n",
            "     55        \u001b[36m0.3815\u001b[0m  0.0225\n",
            "     56        \u001b[36m0.3812\u001b[0m  0.0235\n",
            "     57        \u001b[36m0.3809\u001b[0m  0.0229\n",
            "     58        \u001b[36m0.3805\u001b[0m  0.0231\n",
            "     59        \u001b[36m0.3802\u001b[0m  0.0237\n",
            "     60        \u001b[36m0.3799\u001b[0m  0.0245\n",
            "     61        \u001b[36m0.3794\u001b[0m  0.0254\n",
            "     62        \u001b[36m0.3792\u001b[0m  0.0259\n",
            "     63        \u001b[36m0.3789\u001b[0m  0.0245\n",
            "     64        \u001b[36m0.3787\u001b[0m  0.0217\n",
            "     65        \u001b[36m0.3785\u001b[0m  0.0289\n",
            "     66        \u001b[36m0.3783\u001b[0m  0.0240\n",
            "     67        0.3800  0.0237\n",
            "     68        0.3798  0.0337\n",
            "     69        0.3796  0.0305\n",
            "     70        0.3794  0.0292\n",
            "     71        0.3792  0.0253\n",
            "     72        0.3790  0.0299\n",
            "     73        0.3788  0.0290\n",
            "     74        \u001b[36m0.3770\u001b[0m  0.0258\n",
            "     75        \u001b[36m0.3769\u001b[0m  0.0250\n",
            "     76        \u001b[36m0.3766\u001b[0m  0.0213\n",
            "     77        \u001b[36m0.3764\u001b[0m  0.0230\n",
            "     78        \u001b[36m0.3763\u001b[0m  0.0225\n",
            "     79        \u001b[36m0.3762\u001b[0m  0.0241\n",
            "     80        \u001b[36m0.3759\u001b[0m  0.0219\n",
            "     81        \u001b[36m0.3758\u001b[0m  0.0220\n",
            "     82        \u001b[36m0.3757\u001b[0m  0.0218\n",
            "     83        \u001b[36m0.3757\u001b[0m  0.0222\n",
            "     84        \u001b[36m0.3756\u001b[0m  0.0224\n",
            "     85        \u001b[36m0.3755\u001b[0m  0.0222\n",
            "     86        \u001b[36m0.3755\u001b[0m  0.0234\n",
            "     87        \u001b[36m0.3754\u001b[0m  0.0283\n",
            "     88        \u001b[36m0.3753\u001b[0m  0.0287\n",
            "     89        \u001b[36m0.3752\u001b[0m  0.0232\n",
            "     90        \u001b[36m0.3752\u001b[0m  0.0274\n",
            "     91        \u001b[36m0.3751\u001b[0m  0.0209\n",
            "     92        \u001b[36m0.3750\u001b[0m  0.0256\n",
            "     93        \u001b[36m0.3750\u001b[0m  0.0240\n",
            "     94        \u001b[36m0.3749\u001b[0m  0.0237\n",
            "     95        \u001b[36m0.3749\u001b[0m  0.0211\n",
            "     96        \u001b[36m0.3748\u001b[0m  0.0221\n",
            "     97        \u001b[36m0.3748\u001b[0m  0.0218\n",
            "     98        \u001b[36m0.3748\u001b[0m  0.0224\n",
            "     99        \u001b[36m0.3747\u001b[0m  0.0285\n",
            "    100        \u001b[36m0.3746\u001b[0m  0.0279\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.5940\u001b[0m  0.0234\n",
            "      2        \u001b[36m0.5932\u001b[0m  0.0237\n",
            "      3        \u001b[36m0.5925\u001b[0m  0.0219\n",
            "      4        \u001b[36m0.5918\u001b[0m  0.0197\n",
            "      5        \u001b[36m0.5912\u001b[0m  0.0239\n",
            "      6        \u001b[36m0.5905\u001b[0m  0.0222\n",
            "      7        \u001b[36m0.5898\u001b[0m  0.0225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8        \u001b[36m0.5891\u001b[0m  0.0247\n",
            "      9        \u001b[36m0.5884\u001b[0m  0.0210\n",
            "     10        \u001b[36m0.5878\u001b[0m  0.0203\n",
            "     11        \u001b[36m0.5871\u001b[0m  0.0197\n",
            "     12        \u001b[36m0.5864\u001b[0m  0.0208\n",
            "     13        \u001b[36m0.5857\u001b[0m  0.0210\n",
            "     14        \u001b[36m0.5850\u001b[0m  0.0200\n",
            "     15        \u001b[36m0.5843\u001b[0m  0.0212\n",
            "     16        \u001b[36m0.5836\u001b[0m  0.0212\n",
            "     17        \u001b[36m0.5829\u001b[0m  0.0184\n",
            "     18        \u001b[36m0.5822\u001b[0m  0.0183\n",
            "     19        \u001b[36m0.5815\u001b[0m  0.0181\n",
            "     20        \u001b[36m0.5807\u001b[0m  0.0168\n",
            "     21        \u001b[36m0.5800\u001b[0m  0.0185\n",
            "     22        \u001b[36m0.5793\u001b[0m  0.0263\n",
            "     23        \u001b[36m0.5786\u001b[0m  0.0217\n",
            "     24        \u001b[36m0.5778\u001b[0m  0.0178\n",
            "     25        \u001b[36m0.5771\u001b[0m  0.0188\n",
            "     26        \u001b[36m0.5764\u001b[0m  0.0193\n",
            "     27        \u001b[36m0.5756\u001b[0m  0.0215\n",
            "     28        \u001b[36m0.5749\u001b[0m  0.0178\n",
            "     29        \u001b[36m0.5741\u001b[0m  0.0185\n",
            "     30        \u001b[36m0.5734\u001b[0m  0.0206\n",
            "     31        \u001b[36m0.5726\u001b[0m  0.0181\n",
            "     32        \u001b[36m0.5719\u001b[0m  0.0183\n",
            "     33        \u001b[36m0.5711\u001b[0m  0.0176\n",
            "     34        \u001b[36m0.5704\u001b[0m  0.0166\n",
            "     35        \u001b[36m0.5696\u001b[0m  0.0177\n",
            "     36        \u001b[36m0.5688\u001b[0m  0.0168\n",
            "     37        \u001b[36m0.5680\u001b[0m  0.0166\n",
            "     38        \u001b[36m0.5673\u001b[0m  0.0205\n",
            "     39        \u001b[36m0.5665\u001b[0m  0.0171\n",
            "     40        \u001b[36m0.5657\u001b[0m  0.0184\n",
            "     41        \u001b[36m0.5649\u001b[0m  0.0171\n",
            "     42        \u001b[36m0.5641\u001b[0m  0.0170\n",
            "     43        \u001b[36m0.5633\u001b[0m  0.0177\n",
            "     44        \u001b[36m0.5625\u001b[0m  0.0200\n",
            "     45        \u001b[36m0.5618\u001b[0m  0.0192\n",
            "     46        \u001b[36m0.5610\u001b[0m  0.0181\n",
            "     47        \u001b[36m0.5602\u001b[0m  0.0197\n",
            "     48        \u001b[36m0.5594\u001b[0m  0.0206\n",
            "     49        \u001b[36m0.5585\u001b[0m  0.0247\n",
            "     50        \u001b[36m0.5577\u001b[0m  0.0323\n",
            "     51        \u001b[36m0.5569\u001b[0m  0.0216\n",
            "     52        \u001b[36m0.5561\u001b[0m  0.0200\n",
            "     53        \u001b[36m0.5553\u001b[0m  0.0205\n",
            "     54        \u001b[36m0.5545\u001b[0m  0.0172\n",
            "     55        \u001b[36m0.5537\u001b[0m  0.0172\n",
            "     56        \u001b[36m0.5529\u001b[0m  0.0194\n",
            "     57        \u001b[36m0.5521\u001b[0m  0.0197\n",
            "     58        \u001b[36m0.5513\u001b[0m  0.0214\n",
            "     59        \u001b[36m0.5505\u001b[0m  0.0198\n",
            "     60        \u001b[36m0.5496\u001b[0m  0.0261\n",
            "     61        \u001b[36m0.5488\u001b[0m  0.0179\n",
            "     62        \u001b[36m0.5480\u001b[0m  0.0174\n",
            "     63        \u001b[36m0.5472\u001b[0m  0.0172\n",
            "     64        \u001b[36m0.5464\u001b[0m  0.0175\n",
            "     65        \u001b[36m0.5456\u001b[0m  0.0173\n",
            "     66        \u001b[36m0.5448\u001b[0m  0.0181\n",
            "     67        \u001b[36m0.5440\u001b[0m  0.0167\n",
            "     68        \u001b[36m0.5432\u001b[0m  0.0177\n",
            "     69        \u001b[36m0.5424\u001b[0m  0.0185\n",
            "     70        \u001b[36m0.5417\u001b[0m  0.0180\n",
            "     71        \u001b[36m0.5409\u001b[0m  0.0183\n",
            "     72        \u001b[36m0.5401\u001b[0m  0.0278\n",
            "     73        \u001b[36m0.5393\u001b[0m  0.0214\n",
            "     74        \u001b[36m0.5385\u001b[0m  0.0196\n",
            "     75        \u001b[36m0.5378\u001b[0m  0.0170\n",
            "     76        \u001b[36m0.5370\u001b[0m  0.0168\n",
            "     77        \u001b[36m0.5363\u001b[0m  0.0169\n",
            "     78        \u001b[36m0.5355\u001b[0m  0.0165\n",
            "     79        \u001b[36m0.5348\u001b[0m  0.0209\n",
            "     80        \u001b[36m0.5340\u001b[0m  0.0209\n",
            "     81        \u001b[36m0.5333\u001b[0m  0.0178\n",
            "     82        \u001b[36m0.5326\u001b[0m  0.0189\n",
            "     83        \u001b[36m0.5318\u001b[0m  0.0186\n",
            "     84        \u001b[36m0.5311\u001b[0m  0.0192\n",
            "     85        \u001b[36m0.5304\u001b[0m  0.0198\n",
            "     86        \u001b[36m0.5297\u001b[0m  0.0200\n",
            "     87        \u001b[36m0.5290\u001b[0m  0.0180\n",
            "     88        \u001b[36m0.5283\u001b[0m  0.0178\n",
            "     89        \u001b[36m0.5276\u001b[0m  0.0180\n",
            "     90        \u001b[36m0.5269\u001b[0m  0.0174\n",
            "     91        \u001b[36m0.5263\u001b[0m  0.0215\n",
            "     92        \u001b[36m0.5256\u001b[0m  0.0198\n",
            "     93        \u001b[36m0.5249\u001b[0m  0.0194\n",
            "     94        \u001b[36m0.5243\u001b[0m  0.0194\n",
            "     95        \u001b[36m0.5237\u001b[0m  0.0196\n",
            "     96        \u001b[36m0.5230\u001b[0m  0.0195\n",
            "     97        \u001b[36m0.5224\u001b[0m  0.0175\n",
            "     98        \u001b[36m0.5218\u001b[0m  0.0185\n",
            "     99        \u001b[36m0.5211\u001b[0m  0.0187\n",
            "    100        \u001b[36m0.5205\u001b[0m  0.0175\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.6834\u001b[0m  0.0171\n",
            "      2        \u001b[36m0.6819\u001b[0m  0.0199\n",
            "      3        \u001b[36m0.6805\u001b[0m  0.0182\n",
            "      4        \u001b[36m0.6791\u001b[0m  0.0170\n",
            "      5        \u001b[36m0.6777\u001b[0m  0.0170\n",
            "      6        \u001b[36m0.6763\u001b[0m  0.0171\n",
            "      7        \u001b[36m0.6749\u001b[0m  0.0167\n",
            "      8        \u001b[36m0.6735\u001b[0m  0.0166\n",
            "      9        \u001b[36m0.6722\u001b[0m  0.0167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     10        \u001b[36m0.6708\u001b[0m  0.0190\n",
            "     11        \u001b[36m0.6694\u001b[0m  0.0181\n",
            "     12        \u001b[36m0.6680\u001b[0m  0.0261\n",
            "     13        \u001b[36m0.6667\u001b[0m  0.0175\n",
            "     14        \u001b[36m0.6653\u001b[0m  0.0168\n",
            "     15        \u001b[36m0.6639\u001b[0m  0.0163\n",
            "     16        \u001b[36m0.6626\u001b[0m  0.0212\n",
            "     17        \u001b[36m0.6612\u001b[0m  0.0265\n",
            "     18        \u001b[36m0.6599\u001b[0m  0.0187\n",
            "     19        \u001b[36m0.6585\u001b[0m  0.0179\n",
            "     20        \u001b[36m0.6571\u001b[0m  0.0180\n",
            "     21        \u001b[36m0.6558\u001b[0m  0.0183\n",
            "     22        \u001b[36m0.6545\u001b[0m  0.0166\n",
            "     23        \u001b[36m0.6531\u001b[0m  0.0220\n",
            "     24        \u001b[36m0.6518\u001b[0m  0.0196\n",
            "     25        \u001b[36m0.6504\u001b[0m  0.0204\n",
            "     26        \u001b[36m0.6491\u001b[0m  0.0204\n",
            "     27        \u001b[36m0.6478\u001b[0m  0.0205\n",
            "     28        \u001b[36m0.6464\u001b[0m  0.0213\n",
            "     29        \u001b[36m0.6451\u001b[0m  0.0205\n",
            "     30        \u001b[36m0.6438\u001b[0m  0.0231\n",
            "     31        \u001b[36m0.6425\u001b[0m  0.0198\n",
            "     32        \u001b[36m0.6412\u001b[0m  0.0280\n",
            "     33        \u001b[36m0.6398\u001b[0m  0.0180\n",
            "     34        \u001b[36m0.6385\u001b[0m  0.0219\n",
            "     35        \u001b[36m0.6372\u001b[0m  0.0192\n",
            "     36        \u001b[36m0.6359\u001b[0m  0.0187\n",
            "     37        \u001b[36m0.6346\u001b[0m  0.0204\n",
            "     38        \u001b[36m0.6333\u001b[0m  0.0228\n",
            "     39        \u001b[36m0.6320\u001b[0m  0.0195\n",
            "     40        \u001b[36m0.6307\u001b[0m  0.0200\n",
            "     41        \u001b[36m0.6294\u001b[0m  0.0199\n",
            "     42        \u001b[36m0.6281\u001b[0m  0.0186\n",
            "     43        \u001b[36m0.6268\u001b[0m  0.0183\n",
            "     44        \u001b[36m0.6256\u001b[0m  0.0182\n",
            "     45        \u001b[36m0.6243\u001b[0m  0.0189\n",
            "     46        \u001b[36m0.6230\u001b[0m  0.0183\n",
            "     47        \u001b[36m0.6217\u001b[0m  0.0175\n",
            "     48        \u001b[36m0.6204\u001b[0m  0.0177\n",
            "     49        \u001b[36m0.6192\u001b[0m  0.0205\n",
            "     50        \u001b[36m0.6179\u001b[0m  0.0179\n",
            "     51        \u001b[36m0.6166\u001b[0m  0.0235\n",
            "     52        \u001b[36m0.6154\u001b[0m  0.0169\n",
            "     53        \u001b[36m0.6141\u001b[0m  0.0176\n",
            "     54        \u001b[36m0.6128\u001b[0m  0.0186\n",
            "     55        \u001b[36m0.6116\u001b[0m  0.0184\n",
            "     56        \u001b[36m0.6103\u001b[0m  0.0183\n",
            "     57        \u001b[36m0.6091\u001b[0m  0.0190\n",
            "     58        \u001b[36m0.6078\u001b[0m  0.0183\n",
            "     59        \u001b[36m0.6066\u001b[0m  0.0179\n",
            "     60        \u001b[36m0.6053\u001b[0m  0.0196\n",
            "     61        \u001b[36m0.6041\u001b[0m  0.0200\n",
            "     62        \u001b[36m0.6029\u001b[0m  0.0176\n",
            "     63        \u001b[36m0.6016\u001b[0m  0.0173\n",
            "     64        \u001b[36m0.6004\u001b[0m  0.0198\n",
            "     65        \u001b[36m0.5992\u001b[0m  0.0169\n",
            "     66        \u001b[36m0.5979\u001b[0m  0.0192\n",
            "     67        \u001b[36m0.5967\u001b[0m  0.0190\n",
            "     68        \u001b[36m0.5955\u001b[0m  0.0184\n",
            "     69        \u001b[36m0.5943\u001b[0m  0.0179\n",
            "     70        \u001b[36m0.5931\u001b[0m  0.0183\n",
            "     71        \u001b[36m0.5919\u001b[0m  0.0182\n",
            "     72        \u001b[36m0.5907\u001b[0m  0.0169\n",
            "     73        \u001b[36m0.5895\u001b[0m  0.0184\n",
            "     74        \u001b[36m0.5883\u001b[0m  0.0173\n",
            "     75        \u001b[36m0.5871\u001b[0m  0.0185\n",
            "     76        \u001b[36m0.5859\u001b[0m  0.0199\n",
            "     77        \u001b[36m0.5847\u001b[0m  0.0222\n",
            "     78        \u001b[36m0.5836\u001b[0m  0.0205\n",
            "     79        \u001b[36m0.5824\u001b[0m  0.0226\n",
            "     80        \u001b[36m0.5812\u001b[0m  0.0225\n",
            "     81        \u001b[36m0.5801\u001b[0m  0.0209\n",
            "     82        \u001b[36m0.5789\u001b[0m  0.0203\n",
            "     83        \u001b[36m0.5777\u001b[0m  0.0217\n",
            "     84        \u001b[36m0.5766\u001b[0m  0.0191\n",
            "     85        \u001b[36m0.5754\u001b[0m  0.0181\n",
            "     86        \u001b[36m0.5743\u001b[0m  0.0175\n",
            "     87        \u001b[36m0.5732\u001b[0m  0.0170\n",
            "     88        \u001b[36m0.5720\u001b[0m  0.0186\n",
            "     89        \u001b[36m0.5709\u001b[0m  0.0185\n",
            "     90        \u001b[36m0.5698\u001b[0m  0.0191\n",
            "     91        \u001b[36m0.5687\u001b[0m  0.0191\n",
            "     92        \u001b[36m0.5675\u001b[0m  0.0182\n",
            "     93        \u001b[36m0.5664\u001b[0m  0.0188\n",
            "     94        \u001b[36m0.5653\u001b[0m  0.0194\n",
            "     95        \u001b[36m0.5642\u001b[0m  0.0189\n",
            "     96        \u001b[36m0.5631\u001b[0m  0.0189\n",
            "     97        \u001b[36m0.5621\u001b[0m  0.0189\n",
            "     98        \u001b[36m0.5610\u001b[0m  0.0182\n",
            "     99        \u001b[36m0.5599\u001b[0m  0.0174\n",
            "    100        \u001b[36m0.5588\u001b[0m  0.0199\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.2583\u001b[0m  0.1229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 258, in _score\n",
            "    y_pred = method_caller(estimator, \"predict\", X)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 68, in _cached_call\n",
            "    return getattr(estimator, method)(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/skorch/classifier.py\", line 357, in predict\n",
            "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
            "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.66086607 0.62741537 0.63797875 0.62741537 0.57107734 0.40942674\n",
            " 0.4851742  0.49934519 0.62741537 0.62741537 0.62741537 0.62741537\n",
            " 0.69583642 0.60632567 0.73993081 0.67137386 0.75417593 0.62741537\n",
            " 0.74889424 0.62741537 0.68032493 0.49714603 0.62214603 0.50281072\n",
            " 0.62039783 0.62741537 0.65201384 0.62741537 0.61861873 0.56061898\n",
            " 0.70479367 0.62388189        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan 0.62741537 0.62741537\n",
            " 0.62741537 0.62741537 0.5010934  0.60109958 0.48319125 0.50805535\n",
            " 0.62741537 0.62741537 0.62741537 0.62741537 0.62911416 0.4655918\n",
            " 0.69773907 0.57102174 0.62741537 0.62741537 0.62741537 0.62741537\n",
            " 0.65901285 0.61688905 0.61508525 0.60461453 0.62741537 0.62741537\n",
            " 0.62741537 0.62741537 0.66091549 0.56588213 0.68893625 0.602681\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      2       37.2583  0.1143\n",
            "      3       37.2583  0.1386\n",
            "      4       37.2583  0.1336\n",
            "      5       37.2583  0.1309\n",
            "      6       37.2583  0.1147\n",
            "      7       37.2583  0.1219\n",
            "      8       37.2583  0.1106\n",
            "      9       37.2583  0.1122\n",
            "     10       37.2583  0.1219\n",
            "     11       37.2583  0.1248\n",
            "     12       37.2583  0.1334\n",
            "     13       37.2583  0.1174\n",
            "     14       37.2583  0.1096\n",
            "     15       37.2583  0.1140\n",
            "     16       37.2583  0.1101\n",
            "     17       37.2583  0.1206\n",
            "     18       37.2583  0.1389\n",
            "     19       37.2583  0.1298\n",
            "     20       \u001b[36m35.3676\u001b[0m  0.1309\n",
            "     21        \u001b[36m1.3889\u001b[0m  0.1094\n",
            "     22        \u001b[36m0.6281\u001b[0m  0.1161\n",
            "     23        \u001b[36m0.5738\u001b[0m  0.1148\n",
            "     24        \u001b[36m0.5458\u001b[0m  0.1160\n",
            "     25        \u001b[36m0.5088\u001b[0m  0.1115\n",
            "     26        \u001b[36m0.4929\u001b[0m  0.1393\n",
            "     27        \u001b[36m0.4848\u001b[0m  0.1467\n",
            "     28        \u001b[36m0.4786\u001b[0m  0.1224\n",
            "     29        \u001b[36m0.4728\u001b[0m  0.1143\n",
            "     30        \u001b[36m0.4645\u001b[0m  0.1108\n",
            "     31        \u001b[36m0.4578\u001b[0m  0.1122\n",
            "     32        \u001b[36m0.4508\u001b[0m  0.1096\n",
            "     33        \u001b[36m0.4446\u001b[0m  0.1307\n",
            "     34        \u001b[36m0.4412\u001b[0m  0.1325\n",
            "     35        \u001b[36m0.4329\u001b[0m  0.1349\n",
            "     36        \u001b[36m0.4280\u001b[0m  0.1296\n",
            "     37        \u001b[36m0.4209\u001b[0m  0.1115\n",
            "     38        \u001b[36m0.4134\u001b[0m  0.1210\n",
            "     39        \u001b[36m0.4085\u001b[0m  0.1187\n",
            "     40        \u001b[36m0.4014\u001b[0m  0.1141\n",
            "     41        \u001b[36m0.3897\u001b[0m  0.1229\n",
            "     42        \u001b[36m0.3822\u001b[0m  0.1404\n",
            "     43        \u001b[36m0.3738\u001b[0m  0.1298\n",
            "     44        \u001b[36m0.3681\u001b[0m  0.1367\n",
            "     45        \u001b[36m0.3602\u001b[0m  0.1184\n",
            "     46        \u001b[36m0.3480\u001b[0m  0.1101\n",
            "     47        \u001b[36m0.3384\u001b[0m  0.1138\n",
            "     48        \u001b[36m0.3330\u001b[0m  0.1202\n",
            "     49        \u001b[36m0.3264\u001b[0m  0.1105\n",
            "     50        \u001b[36m0.3123\u001b[0m  0.1419\n",
            "     51        0.3162  0.1438\n",
            "     52        \u001b[36m0.2931\u001b[0m  0.1503\n",
            "     53        0.3077  0.1203\n",
            "     54        \u001b[36m0.2793\u001b[0m  0.1247\n",
            "     55        \u001b[36m0.2785\u001b[0m  0.1135\n",
            "     56        \u001b[36m0.2747\u001b[0m  0.1224\n",
            "     57        \u001b[36m0.2699\u001b[0m  0.1313\n",
            "     58        \u001b[36m0.2627\u001b[0m  0.1471\n",
            "     59        \u001b[36m0.2528\u001b[0m  0.1143\n",
            "     60        \u001b[36m0.2500\u001b[0m  0.1059\n",
            "     61        \u001b[36m0.2460\u001b[0m  0.1129\n",
            "     62        \u001b[36m0.2395\u001b[0m  0.1173\n",
            "     63        \u001b[36m0.2363\u001b[0m  0.1184\n",
            "     64        \u001b[36m0.2323\u001b[0m  0.1140\n",
            "     65        \u001b[36m0.2273\u001b[0m  0.1260\n",
            "     66        \u001b[36m0.2271\u001b[0m  0.1031\n",
            "     67        \u001b[36m0.2267\u001b[0m  0.1166\n",
            "     68        \u001b[36m0.2146\u001b[0m  0.0951\n",
            "     69        \u001b[36m0.2082\u001b[0m  0.0931\n",
            "     70        \u001b[36m0.2074\u001b[0m  0.0872\n",
            "     71        \u001b[36m0.2029\u001b[0m  0.0879\n",
            "     72        0.2032  0.0864\n",
            "     73        \u001b[36m0.1970\u001b[0m  0.0906\n",
            "     74        0.1998  0.0866\n",
            "     75        \u001b[36m0.1967\u001b[0m  0.0997\n",
            "     76        \u001b[36m0.1810\u001b[0m  0.0968\n",
            "     77        \u001b[36m0.1772\u001b[0m  0.0951\n",
            "     78        0.1794  0.0992\n",
            "     79        0.1800  0.0867\n",
            "     80        \u001b[36m0.1741\u001b[0m  0.0884\n",
            "     81        \u001b[36m0.1729\u001b[0m  0.0987\n",
            "     82        \u001b[36m0.1716\u001b[0m  0.0904\n",
            "     83        0.1875  0.0892\n",
            "     84        0.1794  0.0914\n",
            "     85        0.1739  0.0872\n",
            "     86        \u001b[36m0.1682\u001b[0m  0.0898\n",
            "     87        0.1709  0.0934\n",
            "     88        \u001b[36m0.1659\u001b[0m  0.0975\n",
            "     89        \u001b[36m0.1657\u001b[0m  0.1085\n",
            "     90        0.1665  0.0899\n",
            "     91        \u001b[36m0.1638\u001b[0m  0.0908\n",
            "     92        \u001b[36m0.1603\u001b[0m  0.0928\n",
            "     93        \u001b[36m0.1567\u001b[0m  0.0941\n",
            "     94        0.1595  0.0918\n",
            "     95        0.1579  0.0916\n",
            "     96        \u001b[36m0.1551\u001b[0m  0.0931\n",
            "     97        0.1565  0.1023\n",
            "     98        \u001b[36m0.1541\u001b[0m  0.0939\n",
            "     99        0.1564  0.1087\n",
            "    100        \u001b[36m0.1511\u001b[0m  0.0933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_precisao = grid_search.best_score_\n",
        "melhores_parametros, melhor_precisao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdety7VaelOL",
        "outputId": "96a7a4b2-f73e-4c1e-bfa5-ac2a423c265c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'batch_size': 10,\n",
              "  'criterion': torch.nn.modules.loss.BCELoss,\n",
              "  'max_epochs': 100,\n",
              "  'module__activation': <function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor>,\n",
              "  'module__initializer': <function torch.nn.init.uniform_(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0) -> torch.Tensor>,\n",
              "  'module__neurons': 8,\n",
              "  'optimizer': torch.optim.adam.Adam},\n",
              " 0.7541759327897208)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}